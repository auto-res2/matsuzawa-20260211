{
  "research_topic": "Proposing an improved Chain-of-Thought based on human thinking methods, evaluated purely through prompt tuning without fine-tuning or time-intensive experiments",
  "queries": [
    "prompt tuning CoT",
    "human reasoning prompting",
    "cognitive strategies CoT"
  ],
  "research_study_list": [
    {
      "title": "Co-training Improves Prompt-based Learning for Large Language Models",
      "full_text": "A Computational Approach for   Mapping Electrochemical Activity of Multi-Principal Element Alloys    Jodie A. Yuwono1,2, Xinyu Li3, Tyler D. DoleÅ¾al4, Adib J. Samin4,   Javen Qinfeng Shi3, Zhipeng Li2 and Nick Birbilis2,5    1School of Chemical Engineering, The University of Adelaide, Adelaide, SA 5005, Australia  2College of Engineering, Computing and Cybernetics, Australian National University, Canberra, ACT 2601, Australia  3Australian Institute for Machine Learning, The University of Adelaide, Adelaide, SA 5000, Australia  4Department of Engineering Physics, Air Force Institute of Technology, 2950 Hobson Way, Wright-Patterson Air Force  Base, OH 45433, USA  5Faculty of Science, Engineering and Built Environment, Deakin University, Waurn Ponds, VIC 3216, Australia      ABSTRACT  Multi principal element alloys (MPEAs) comprise a unique class of metal alloys. MPEAs have  been demonstrated to possess several exceptional properties, including, as most relevant to the  present study a high corrosion resistance. In the context of MPEA design, the vast number of  potential alloying elements and the staggering number of elemental combinations favours a  computational alloy design approach . In order to computationally assess the prospective  corrosion performance of MPEA, an approach was developed in this study. A density  functional theory (DFT) â€“ based Monte Carlo method was used for the development of MPEA  â€˜structureâ€™; with the AlCrTiV alloy used as a model. High-throughput DFT calculations were  performed to create training datasets for surface activity/selectivity towards different adsorbate  species: O2-, Cl - and H +. Machine -learning (ML) with combined representation was  then  utilised to predict the  adsorption and vacancy energies as descriptors for surface  activity/selectivity. The capability of the combined computational methods of MC, DFT and  ML, as a virtual electrochemical performance simulator for MPEAs was established and may  be useful in exploring other MPEAs.   1. INTRODUCTION    In conventional alloy design  to date , typically a principal metal lic element is explored,  accompanied by other elements in relatively minor concentrations. Such alloy design â€“ whilst  often empirical â€“ has also been aided by thermodynamic calculations; the development and use  of phase diagrams ; and more recently by kinetic assessment to factor in thermally activated  solid-state transformations. Whilst such ap proaches have developed to significant levels of  sophistication, the exploration of more complicated alloy systems, of which no prior data  exists, can make desktop alloy design complex1 Modern alloy exploration in the form of the  so-called multi-principal element alloy (MPEA s) requires new tools and approaches to aid in  their design. As the name suggests, MPEAs contain at least two (but often more, including >5),  principal alloying elements . One impetus for the rapid development and research regarding  MPEAs is owing to their excellent physical properties2 and more recently, reports of their  aqueous corrosion resistance â€“ amongst other unique properties. Furthermore, other  applications in which MPEAs are being explored include catalysts and battery electrodes , in  addition to structural materials 3. Due to the large number  of possible a lloying elements  available for potential MPEA production, and the myriad of possible elemental combinations,  the realm of MPEAs still remains heavily unexplored4. If MPEA development is to continue at  an appropriate pace , conventional trial -and-error method s for alloy development must be  supplemented by objective alloy design5. In addition to the need for rational alloy design based  on tools that can assist in prediction of structure and perhaps key properties, the desktop   computational prediction of corrosion performance remains its own unique challenge. To date,  there are very few mechanistic or deterministic tools available that can generate computed  information in a manner that can aid in prediction of electrochemical performance. In this work,  a computational methodology (which is actually a computational workflow) is p resented, in  order to estimate the electrochemical performance of an MPEA, without the need for any  empirical testing.    Integrated approaches permit computational materials discovery approaches for new (objective  oriented) alloy design. To date, the calculation of phase diagrams (CALPHAD), which is based  on equilibrium thermodynamics to provide information on  probable equilibrium phase s, is  widely used; as is the use of density functional theory (DFT) to provide information on phase  stability. Though CALPHAD and DFT approaches are beneficial, such approaches may be  time-consuming, computationally costly and limited in what can be explored relative to  properties of interest and the breadth of MPEA compositions1,6. Recently, a purely machine  learning (ML) approach has been developed to  accelerate the discovery of MPEAs , with a  specific focus on alloy development of INVAR7. This kind of ML-based method provides the  capability to build a connection between underlying physics and composition -dependent  properties7,8, allowing for the selection, design and verification of new alloys to be done an  order of magnitude faster . However, the lack of understanding regarding how alloy   composition can impact material properties for unexplored composition space, remains a  bottleneck. This level of understanding is critical for predicting which elemental combinations  may yield optimal structural performance in various operational environments.     When a primary property of interest is alloy electrochemistry and/or corrosion behaviour,  detailed information regarding surface features is critical â€“ as electrochemical processes occur  at the alloy surface (i.e. alloy-environment interface). Without a complete understanding of the  composition-dependent surface properties in the context of electrochemistry and/or corrosion,  the verification of newly designed alloys will be tedious. For instance, in alloys that develop  local sites with different electrochemical activities, this may accelerate corrosion due to micro-galvanic coupling or localised pit formation9. Herein, a computational approach is proposed  that combines Monte Carlo (MC) methods, high-throughput DFT, and ML â€“ to enable virtual  electrochemical characterisation of digitally designed (new) alloys. As a proof of concept, this  methodology was explored for an MPEA with the equi -atomic composition AlCrTiV. This  alloy was selected as a representative model system, because its electrochemical activity has  been thoroughly and well-documented in the literature10â€“14. The â€˜electrochemistry simulatorâ€™  is expected to describe the relationship between surface features and surface reactivity .  Furthermore, the intent is to provide a pathway, if not a methodology, for data collection and  verification;, which would enable  a knowledge-guided ML approach for materials discovery  of MPEAs with tailored electrochemical properties.   2. APPROACH    A state-of-the-art hybrid MC and DFT approach was employed in order to develop MPEA  models sampled from equilibrium15.  Herein, the ability to predict electrochemical activity of  an MPEA using computer simulation  was investigated. Three stages were employed in this  workflow (Figure 1).       Figure 1. The overall workflow of virtual electrochemistry simulation performed in this study  and its verification.    The phase and composition at equilibrium were obtained using the  multi-cell Monte Carlo   â€œ(MC)2â€ method. Specifically, the existing MC code , based on a methodology  proposed by  Ghazisaeidi16, was utilised in order to predict stable phases and phase fractions for an MPEA  of choice and a given set of condi tions (temperature and pressure). Using an (MC)2 predicted  structure, we then generated surface slabs, and performed high-throughput DFT calculations to  understand the reactivity of these surfaces in aqueous environment s - ahead of developing  machine-learning (ML) models to predict the DFT -derived electronic energy. This was  followed by a survey of surface electrochemical activity on (100) Al CrTiV surfaces with  different surface features (of both the ratio and the combination of elements).  We carefully  investigated their selectivity and electronic response toward different species in aqueous media,  including H+, O2- and Cl-. We used combined representation of elemental properties (EP) 17,  smooth overlap atom position (SOAP) 18 and grou p and period -based coordination atom  fingerprints (GP-CAF)19 to predict the electrochemical activity: passive film formation and  surface dissolution from a combination of different MPEA surfaces.     2.1 Bulk Calculations (MC)2 Method  The bulk structure was generated using an implementation15 of the (MC)2 algorithm16,20,21. The  simulation was performed at T = 300 K, P = 0 Pa.  Four simulation cells were initiali sed with  64 atoms per cell in the initial configurations of body -centred cubic (bcc), hexagonal close  packed (hcp), and face-centred cubic (fcc), respectively. During the simulation , two types of  moves were considered, a local flip or intra -swap. A local flip is described as randomly  selecting one of the four simulation cells, then randomly selecting one atom within the chosen  simulation cell, and \"flipping\" it from its current species type to one of the other four species  types. The intra -swap consists of randomly selecting one of the four simulation cells, then  randomly selec ting two atoms whose positions are swapped. The algorithm has been  constructed to only perform an intra -swap between two atoms of different types and should a  simulation cell become 100% of one species, the intra-swap move is rejected. DFT calculations  were executed to calculate simulation cell internal energy and volume changes. The acceptance  probabilities, based on the Metropolis criteria22, for the local flip and intra -swap, are given in  the following formulas respectively;  ğ‘ƒğ‘ğ‘ğ‘ğ‘’ğ‘ğ‘¡ ğ‘“ğ‘™ğ‘–ğ‘ = min{1, exp (âˆ’ğ›½âˆ†ğ» + ğ‘âˆ†ğºğ‘š)}                              (Eqn. 1)  where âˆ†ğ» and âˆ†ğºğ‘š are calculated as follows:  âˆ†ğ» = m âˆ‘(ğ‘ˆğ‘– â€² + ğ‘ğ‘‰ğ‘– â€²)ğ‘“ğ‘– â€² âˆ’ âˆ‘(ğ‘ˆğ‘– + ğ‘ğ‘‰ğ‘–)ğ‘“ğ‘– ğ‘š ğ‘–=1 ğ‘š ğ‘–=1                                                                                        (Eqn. 2)  âˆ†ğºğ‘š = m âˆ‘[ğ‘“ğ‘– â€² ln(ğ‘‰ğ‘– â€²) âˆ’ ğ‘“ğ‘– ln(ğ‘‰ğ‘–)] + âˆ‘ ğ‘“â€² ğ‘– âˆ‘ ğ‘‹ğ‘— â€² ğ‘– ln(ğ‘‹ğ‘— â€² ğ‘–) ğ‘š ğ‘–=1 ğ‘š ğ‘–=1 ğ‘š ğ‘–=1 âˆ’ âˆ‘ ğ‘“ğ‘– âˆ‘ ğ‘‹ğ‘— ğ‘– ln(ğ‘‹ğ‘— ğ‘–) ğ‘š ğ‘–=1 ğ‘š ğ‘–=1              (Eqn. 3)    Here, ğ›½ = 1/ğ‘˜ğµğ‘‡, where ğ‘˜ğµ is the Boltzmann constant, N is the sum of all the particles across  all simulation cells, m is the total number of simulation cells, ğ‘ˆğ‘– is the energy of simulation  cell i, ğ‘‰ğ‘– is the volume of simulation cell i, p is the pressure  (set to 0 Pa ) and ğ‘“ğ‘– is the molar  fraction of simulation cell i. Lastly, where ğ‘›ğ‘— ğ‘– is the number of species i in simulation cell j,  ğ‘‹ğ‘— ğ‘– =  ğ‘›ğ‘— ğ‘–/ âˆ‘ ğ‘›ğ‘— ğ‘˜5 ğ‘˜=1 , and represents the atomic concentrat ion of species i in simulation cell j.  The primed coordinates indicate post -flipped values, while un-primed are pre-flipped values.  The updated phase fractions were obtained by using the Lever rule to enforce mass  conservation.    DFT calculations were performed using the Projector Augmented Wave (PAW) method as  implemented by the Vienna Ab initio Simulation Package (VASP)23,24. The calculations were  completed with a plane-wave cut-off energy of 450 eV and a 2Ã—2Ã—2 Monkhorst -Pack k-point  mesh25. DFT calculations performed on the simulation cells allowed for changes in the volume  and atomic positions (through setting ISIF = 3). The electronic self-consistent calculation was  converged to 1Ã—10-6 eV and ionic relaxation steps were performed using the conjugate-gradient  method (IBRION = 2) and continued until the total force on each atom dropped below a  tolerance of 1Ã—10 -2 eV/Ã…. The generali sed gradient approximation (GGA) was used for the  exchange correlation functionals as parameterized by Perdew -Burke and Ernzerhof (PBE) 26.  The PAW pseudopotentials were used with the valence electron configurations 3s23p1, 3d34s1,  3d54s1, and 3p63d44s1 for Al, Ti, Cr and V, respectively.    2.2 Surface DFT Calculation Method    All surface DFT calculations were  performed using the VASP code23; and core electrons in  calculations treated using both the the PAW method and the GGA -PBE exchange correlation  functionals24,26. A plane-wave cut-off energy of 500 eV and a 3Ã—3Ã—1 Monkhorst-Pack k-point  mesh25. The electronic self -consistent calculation was converged to 1Ã—10 -5 eV and ionic  relaxation steps were performed using the conjugate -gradient method (IBRION = 2) and  continued until the total force on each atom dropped below a tolerance of 1Ã—10-2 eV/Ã…. A slab  method with 20 Ã… vacuum thickness in z-direction was used to model (2Ã—1) (001) surface and  different slab thickness of 6, 7 and 8 layers were considered. During surface relaxation, the  four uppermost layers were allowed to relax, while the others were fixed to their bulk  coordinates, inducing a surface-bulk condition. Figure 2 shows the surface of AlCrTiV MPEA  which is used as the model system explored in this study.      Figure 2. Six different (001) surfaces of AlCrTiV bcc phase as obtained from (MC) 2  optimisation with four identified adsorption sites used for high -throughput DFT calculations  of surface reactivity.    Vacancy calculations were performed by removing one atom in the topmost layer on each slab.  Whereas, for surfa ce adsorption calculations, the adsorbates studied are *H, *O and *Cl, in  their dissociated forms27,28. The adsorption study only considered single species adsorption (no  co-adsorption on the surface) in each DFT calculation, in which each species on the adsorption  sites was fixed in their planar (x and y) direction and only allowed relax in z -direction to find  their lowest energy state. A less strict convergence criteria with total force tolerance of 5Ã—10-2  eV/Ã… was used to perform the scanning of surface reactivity/selectivity.  The adsorption energy is calculated using the following formula:  Î”ğ¸ğ‘ğ‘‘ğ‘  = ğ¸ğ‘ğ‘‘ğ‘ âˆ’ğ‘ ğ‘™ğ‘ğ‘ âˆ’ ğ¸ğ‘ ğ‘™ğ‘ğ‘ âˆ’ ğ¸ğ‘ğ‘‘ğ‘ ğ‘œğ‘Ÿğ‘ğ‘ğ‘¡ğ‘’                                                                               (Eqn. 5)    where ğ¸ğ‘ğ‘‘ğ‘ âˆ’ğ‘ ğ‘™ğ‘ğ‘ is the energy of adsorbate/slab complexes, ğ¸ğ‘ ğ‘™ğ‘ğ‘ is the energy of a pure slab  and ğ¸ğ‘ğ‘‘ğ‘ ğ‘œğ‘Ÿğ‘ğ‘ğ‘¡ğ‘’ is the energy of the adsorbate. Here is ğ¸ğ‘ğ‘‘ğ‘ ğ‘œğ‘Ÿğ‘ğ‘ğ‘¡ğ‘’ of H, O and Cl are defined as  Â½ E(H 2), E(H 2O) â€“ E(H2) and E( HCl) â€“ Â½ E(H2), respectively.  The influence of applied  potential (U vs. SHE) and pH on Î”G can be considered implicitly 27,29, using the followin g  mechanism:  * + H2O + HCl + H+ + e-  (Eqn. 6)  *H + H2O + HCl (Eqn. 7)  *O + HCl + 3H+ + 3e- (Eqn. 8)  *Cl + H2O + 2H+ + 2e- (Eqn. 9)    To identify the contribution of each element to the final adsorption energy, the following  formula was fit to inference each elementâ€™s contribution:  ğ‘šğ‘‡ğ‘–ğ¸ğ‘‡ğ‘– + ğ‘šğ‘‰ ğ¸ğ‘‰ + ğ‘šğ´ğ‘™ğ¸ğ´ğ‘™ + ğ‘šğ¶ğ‘Ÿğ¸ğ¶ğ‘Ÿ = Î”ğ¸                                                                       (Eqn. 10)  where ğ‘šğ‘‡ğ‘–, ğ‘šğ‘£, ğ‘šğ´ğ‘™, ğ‘šğ¶ğ‘Ÿ are the total sum scaling normalised coordination number, which are  calculated according to:  ğ‘šğ¸ğ‘™ğ‘’ = ğ‘›ğ¸ğ‘™ğ‘’ (ğ‘›ğ‘‡ğ‘– + ğ‘›ğ‘‰ + ğ‘›ğ´ğ‘™ + ğ‘›ğ¶ğ‘Ÿ)â„  for ğ¸ğ‘™ğ‘’ in {Ti, V, Al, Cr}                         (Eqn.11)   where ğ‘›ğ‘‡ğ‘–, ğ‘›ğ‘£, ğ‘›ğ´ğ‘™, ğ‘›ğ¶ğ‘Ÿ is the number of coordinated Ti, V, Al, and Cr elements, respectively.  Î”ğ¸ is the DFT calculated adsorption energy of each mono atom adsorbate. ğ¸ğ‘‡ğ‘–, ğ¸ğ‘£, ğ¸ğ´ğ‘™, ğ¸ğ¶ğ‘Ÿ  are the energy values that are fit using the least-squares method.     It is noted that herin, the vacancy energy is calculated by the following formula:  ğ¸ğ‘‰ğ‘ğ‘ğ‘ğ‘›ğ‘ğ‘¦ = ğ¸ğ‘†ğ‘™ğ‘ğ‘ğ‘¤ğ‘‰ âˆ’ ğ¸ğ‘†ğ‘™ğ‘ğ‘ğ‘¤ğ‘œğ‘‰ âˆ’ ğ¸ğ‘‰ğ¸                                                                            (Eqn.12)  where ğ¸ğ‘†ğ‘™ğ‘ğ‘ğ‘¤ğ‘‰ is the energ y of slab with vacancy, ğ¸ğ‘†ğ‘™ğ‘ğ‘ğ‘¤ğ‘œğ‘‰ is the energy of slab without  vacancy and ğ¸ğ‘‰ğ¸  is the reference energy of element.     When comparing the vacancy energies of different elements, the ğ¸ğ‘‰ğ¸  plays a crucial role. In  the present work, we have compared three differently defined approached to ğ¸ğ‘‰ğ¸ , namely: (1)  the energy per atom of different elements in their pure crystal structure at room temperature ;  (2) the energy obtained by fitting the function âˆ‘ ğ‘›ğ‘–ğ¸ğ‘–ğ‘–âˆˆ{ğ‘‡ğ‘–,ğ‘‰,ğ´ğ‘™,ğ¶ğ‘Ÿ} = ğ¸ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™ for all vacancy and  pure slabs in this study; and (3) the energy per atom in the high entropy alloy primary crystal,  which serves as a universal correction value. Figure 3 reveals the vacancy energy using  different vacancy element reference energy. It was determined that V generally has the lowest  vacancy energy using both the pure crystal structure energy and fitting energy, indicating that  V is the element that could most easily form a vacancy. When comparing the magnitude of all  three methods, the crystal structure atomic energy has the vacancy energy ranges in 0.0-2.5 eV,  in comparison to -3.0 â€“ 4.5 eV using fitted atomic energy (2 nd method) and -3.0-2 eV using  HEA average atomic energy (3rd method). However, for the AlCrTiV MPEA, empirical work  by Choudhary12,13 conducted atomic emission spectro-electrochemistry (AESEC)30 dissolution  profiling at the open circuit potential, revealing alloy  dissolution kinetics in the following  order: Al, V, Cr/Ti. In other words, the highest rate of dissolution in a 0.6 M NaCl solution was  observed for Al, and the lowest rate was for the  dissolution of Cr and Ti 13. Therefore, the  calculation using approach (3), is mechanistically the most appropriate in this work . This  method provides a similar trend with Al as the most energetically favourable element to create  a surface vacancy (Figure 3).      Figure 3. The DFT calculated vacancy energy determined using a different vacancy reference  energy: (a) the energy per atom of vacancy element in its pure crystal structure at room  temperature; (b) the energy obtained by fitting the function âˆ‘ ğ‘›ğ‘–ğ¸ğ‘–ğ‘–âˆˆ{ğ‘‡ğ‘–,ğ‘‰,ğ´ğ‘™,ğ¶ğ‘Ÿ} = ğ¸ğ‘¡ğ‘œğ‘¡ğ‘ğ‘™  for all  slabs in this study; and (c) the energy per atom in the MPEA primary crystal, which serves as  a universal correction value.   2.3 ML for Surface Activity/Selectivity Prediction    In this study, we employed Kernel Ridge Regression (KRR) 31, a fast and accurate regression   algorithm17,19,32, as the ML algorithm utilised since it demonstrated high accuracy with a small  number of samples (<1000) . The radial basis function (rbf) kernel was used to transform the  representations of ğ‘–th and ğ‘—th samples into a kernel matrix ğ¾. The element in row ğ‘– and column  ğ‘— of ğ¾ is calculated using the following formula:  ğ¾(ğ‘–, ğ‘—) = ğ‘’ğ‘¥ğ‘ (âˆ’(ğ‘¥ğ‘– âˆ’ ğ‘¥ğ‘—) 2 2ğ›¾2â„ )                                                                                              (Eqn.13)  where ğ‘¥ğ‘– and ğ‘¥ğ‘— represent the representations of the ğ‘–th and ğ‘—th samples, ğ›¾ is a length scale  parameter.     The prediction form of KRR is as follow:  ğ‘¦ğ‘ğ‘Ÿğ‘’ğ‘‘ = ğ¾ğ‘¤                                                                                                                                 (Eqn.14)  where ğ‘¤ is a weight matrix, K is the kernel matrix.     The loss function of KRR is a quadratic function given by the following formula:  ğ‘™ğ‘œğ‘ ğ‘  = â€–ğ‘¦ğ‘¡ğ‘Ÿğ‘¢ğ‘’ âˆ’ ğ¾ğ‘¤â€–2 2 +  ğ›¼ 2â„ ğ‘¤ğ‘‡ğ¾ğ‘¤                                                                                                  (Eqn. 15)  where ğ‘¦ğ‘¡ğ‘Ÿğ‘¢ğ‘’ is the labels of the training set, ğ›¼ is a L2 regularization term. A closed form solution  of the loss function can be derived, as follow:  ğ‘¤ = (ğ¾ + ğ¼ğ›¼)âˆ’1ğ‘¦                                                                                                                                     (Eqn. 16)    To fin d the optimal hyperparameters Î³ and Î±, a grid search technique  was employed. We  searched for the optimal values using a base-2 logarithmic grid from 0.25 to 4096 for the kernel  width and a base-10 logarithmic grid from 10e-7 to 10e-7 for the L2 regularisation term.   3. RESULTS AND DISCUSSION    3.1 First-principles investigation of bulk structure     In this study â€“ as already ascertained from the Approach section -  exploration of the AlCrTiV  MPEA was carried out, owing to the alloy being known to exhibit the formation of only a single  (bcc) phase at room temperature ; along with a reported record regarding the contribution of  each element  to electrochemical reactions . The results of o ur attempt at the prediction of  dissolution characteristics for AlCrTiV, using the workflow proposed herein; follows below,  including correlation with experimental data. The (MC)2 methodology accurately predicted the  formation of a single bcc phase for AlCrTiV with an equi -atomic configuration and a lattice  constant of 3.05 Ã…, with two supercells made are shown in Figures 4(a-b). The final structure  matched experimental data, well, with whereby the empirically determined lattice constant is  3.075 Ã…10. The  empirical and simulated x -ray diffraction patterns  for AlCrTiV crystal  are  shown in Figures 4(e-f).     Figure 4. (a, b) Two final bulk MPEA supercell structures consist of 128 atoms as generated  from (MC)2 calculations along with (c, d) their charge distribution. Green, dark -blue, light- blue and red circles represent Al, Cr, Ti and V atoms, respectively. Turquoise and yellow color  isosurfaces represent the gain and loss of electrons, respectively. (e) X-ray diffraction pattern  from a powdered sample of the as-cast AlCrTiV alloy. Reproduced with permission from Qiu  et al., Acta Materialia 123, 115 -124 (2017)10. Copyright 2017 Elsevier.  (f) Simulated x-ray  diffraction patterns from two final bulk structures predicted using (MC)2 algorithm.    3.2 First-principles investigation of surface electrochemistry    Using the bulk AlCrTiV alloy structure predicted by (MC)2, surface slabs were generated and  used to build a surface reactivity map based upon the chemical interactions between different  species (i.e., H +, Cl -, and O 2-) and the alloy surface. On the basis o f empirical  data being  available for the electrochemical activity of AlCrTiV  (which is from a bulk polycrystal) , we  selected the (001) surface for the adsorption model â€“ as it provided 32 possible adsorption sites  (i.e., atop, bridge -x, bridge-y, and hollow) , along with  eight vacancy sites. Furthermore, we  also employed  an extended supercell  in order to minimi se the lateral interaction between  adsorbed species. Despite the species being charged in the electrolyte, we have only considered  the adsorption of neutral species on the surface  in our DFT calculations (*H, *Cl and *O) ;  noting that the accounting for charge/electrons may be approached using the standard hydrogen  electrode method33. To maximize atomic variation and surface structure, six different surface  slabs were created for the adsorption study and a vacancy was created on the topmost layer to  simulate a dissolution event at the surface. From DFT calculations, we successfully established  the surface activity/selectivity that is represented using the energy map, as shown in Figure 5.     Figure 5. The surface activity/selectivity maps  at U=0 VSHE and pH=0 for six different slabs  studied here (as depicted in Figure 2), showing the interactions of surface atoms following the  adsorptions of Cl-, O2-, H+ and the formation of vacancy.    The results presented in Figure 5 provide information on the early-stage electrochemical  surface activity of AlCrTiV in the presence of different adsorbates . From the calculated  adsorption and vacancy energy, the local electrochemical activity on the surface is observed,  in which the formation of cathodic and anodic sites can be distinguished  by their selective  interaction with adsorbates/vacancy. Red-coloured regions indicate strong interaction between  alloy-environment, whereas blue -coloured regions indicate weak interaction between alloy - environment. Strong H -bonding surface denotes the preference of surface regions to act as  cathodes, w hereas strong O -bonding, Cl -bonding as well as vacancy formation denote the  preference of surface regions to act as anodes. Furthermore, we can understand the dissolution  mechanisms of the alloys, whether it undergoes direct or indirect dissolution via salt-formation  (e.g., by the formation of metal oxide and/or metal chloride). This mapping enables surface  activity/selectivity to be inspected â€˜virtuallyâ€™, in a manner that is usually only obtained using  complex local electrochemical characteri sation techniques such as scanning tunnelling  microscopy (STM) 34,35, whilst alternatively, the scanning vibrati ng electrode technique  (SVET)36 or scanning electrochemical cell microscopy (SECM )37,38 provide excellent spatial  resolution at an order of magnitude lower length scale. It is noted that the length scales of  interrogation from the calculations herein, may also be mechanistically advantageous on the  basis of prospects for nano-engineering alloy structures.     The selection of adsorbates explored was based on their critical role during surface  electrochemistry (namely, alloy corrosion) , such as  participating in the hydrogen evolution  reaction, oxide formation and thickening, surface dissolution , and pitting. From calculated  surface maps, a different reactivity between may distinguish between cathodic and anodic sites  on the surface. While previous studies on conv entional (non MPEA)  alloys have only  highlighted the competitive adsorption of such species on the alloy surface as the coverage  function27,39, in the present there is a deliberate focus on the interaction between those species  with the different elements on the MPEA â€™s surface â€“ allowing identification and analysis of  their roles on the surface electrochemistry.    Figure 6. The least-square fitted ğ¸ğ‘‡ğ‘–, ğ¸ğ‘£, ğ¸ğ´ğ‘™, ğ¸ğ¶ğ‘Ÿ on each slab at U=0 VSHE and pH=0.    Figure 6 shows the  least-square fitted energy values for atop site adsorbates on each slab  (where only the atop site is explored in this study, data shown in Figure S1). It was revealed  that *O and *Cl prefer to stay close to Ti on all surfaces , except on B7 where *Cl also prefer  to stay close to V. M eanwhile *H prefers to stay close to Cr on A6, A8 and B6, to V on A7,  B7 and B8. From the small sampling performed, we can understand  the role of each element  as anodic and cathodic sites during electrochemical reactions. Ti is expected to be oxidised and  formed oxide layers, the anodic site, followed by V, Cr and Al. The same order of elements for  another anodic site  applies for competing oxidation/dissolution via reaction with Cl -. The  elemental order for cathodic site as predicted by hydrogen evolution reaction is varied from the  systems studied here, in which Ti, V and Cr exhibits the capability to facilitate *H adsorption  with negative energies , excluding Al with positive energ y. This suggests  that Al is more  dominant and active towards  any oxidation/anodic reaction , which is consistent with  experiments12,13. Additionally, the role of Ti on the formation of protective oxide layer,  including the enrichment of Ti, can be explained from its strong interaction with O 2- and Cl-.  The adsorption energy difference on different slabs may be used as the preference indicator for  each slab tendency as being more or less anodic/cathodic, as described by the adsorption of *O  and *Cl or *H, respectively.     The inspection of Figure 6 reveals that it is difficult to clearly distinguish the contribution of  each element, due to the complexity of alloy systems (i.e. in addition to chemical complexity,  there is also a variation of adsorption sites, which includes the bridge and hollow sites  that  require the consideration of multiple atoms ) â€“ as well as the limitation of slab samples  investigated in this study. We acknowledge that the electrochemical activity of each element is  unique and not solely defined by its own intrinsic features. Neighbouring atoms and a  coordination network will contribute to electrochemical activity, as understood from the unique  charge distribution of each atomic species in MPEA structure in Figures 4(c-d). Therefore, we  will refrain from quantitatively focusing on feature rank and selection during the present work.  The future objective development of new MPEAs with controlled electrochemical activity will  require the theoretical analysis of many samples,  which can also become an obstacle even in  theoretical-based studies. Instead, we are more interested in the ongoing generation of datasets  for training ML model s to predict the electronic energy of certain surface structures . This  approach will be beneficial for the generation and analysis of large surface datasets, to better  understand the contribution of  individual elements on surface electrochemistry  â€“ which is  particularly important for MPEAs.    From the experiment al work to date studying AlCr TiV, it is understood that â€˜dynamic- passivityâ€™ is a critical factor contributing to corrosion resistance of AlCr TiV; including the  surface film  composition, thickness and electronic properties. Various analysis tools have  confirmed that AlCrTiV exhibits excellent electrochemical stability, which is attributed to the  ability to form passive layer comprising of mixed metallic oxides: Al 2O3, TiO 2, V 2O3 and  Cr2O3. The experimental techniques used to study the passive film characteristics  and  electrochemical activity, include high resolution X-ray photoelectron spectroscopy (XPS) and  atomic emission spectroelectrochemistry. Using AESEC, Choudhary et al.12,13 quantitatively  recorded the dissolution and oxidation of AlCrTiV on a â€˜per elementâ€™ basis. It was revealed  that elemental  Al was readily observed to preferentially dissolve (although the alloy has  stoichiometrically equal atomic proportion of all constitu ent elements), as soon as the sample  was exposed to the electrolyte, as shown in Figure 7(a).      Figure 7. (a) AESEC dissolution profile at OCP for AlCrTiV in quiescent 0.6 M  NaCl.  Reproduced under Creative Commons CC-BY license from Choudhary et al., J. Electrochem.  Soc. 168, 051506 (2021)13. (b) AESEC polarization profile for AlCrTiV in quiescent 0.6 M  NaCl. Reproduced with permission from Choudhary et al., Electrochim . Acta 362 , 137104  (2020)12. Copyright 2020 Elsevier.    It is also revealed in Figure 7(b), that in the presence of an applied potentiodynamic  polarisation, that trans-passive dissolution of V and Cr occurs, accompanied by the enrichment  of Ti and its oxidation to form TiO 2 on the surface 12,13. Despite the high dissolution rate  of  elemental Al , results from detailed XPS analysis revealed that Al 2O3 was the major oxide  present on the surface, followed by TiO 2 and then similar amounts of V 2O3 and Cr2O3 13. The  absence of mixed oxides here is also consistent with  the findings of the computational study  by Samin on the thermodynamics of Niobium-Titanium alloy oxidation40. In that study, it was  determined that TiO2 was found to be the most stable oxide for most temperature -pressure  combinations and mixed oxides were never thermodynamically favourable. Key findings from  such aforementioned studies regarding corrosion resistant alloys, are that dissolution/oxidation  are non -stoichiometric, that mixed -oxides are not present, and that the surface oxides also  include a proportion of unoxidised (M0) metal. The causality between such characteristics and  performance however remains under ongoing investigation.    3.3 Machine-learning prediction of surface electrochemistry    In the workflow outlined for this study, the utilisation of ML was then adopted for predicting  adsorption and vacancy energies. In a previous study, Batchelor et al. employed a coordination- based representation and linear regression to predict adsorption energies of O and OH on high  entropy alloys41, resulting in an RMSE of less than 0.1 eV. Nonetheless, that method was  developed to solely rely on surface coordination and hence disregards the identity of the  adsorbed species; therefore, requiring separate models for each adsorbate.  Li et al. proposed  an alternative approach that integrates both adsorbed species and surface slab information17,  enabling the prediction of â€œcross-adsorbateâ€ and â€œcross -slabâ€ adsorption energies . That  representation, however,  is limited to the most stable site and does not consider specific  adsorption sites or lateral interactions between adsorbates.     The present  study enhances the combined representation by incorporating an additional  representation for the adsorption site, enabling â€œcross-siteâ€ prediction. The improved  representation is comprised of EP to represent the single -atom adsorbate ( *O, *H, or *Cl in  this study), SOAP to represent the high entropy alloy slabs 18, and GP-CAF to represent the  adsorption site19 (representation employed herein shown schematically in Figure 8).       Figure 8. Schematic of the combined representation used in this study . Elemental properties  (EP) is used as adsorbate representation, group and period -based coordination atom  fingerprints (GP-CAF) as adsorption site representation and smooth overlap atom position  (SOAP) as slab representation.    Figures 9(a-c) compare the ML-predicted adsorption energies and DFT calculated values using  5-fold cross validation test, leave-one-slab-out test (LOSO), and LOSO with 20% samples from  the test slab added to the training set. The 5 -fold cross validation test results sho w a strong  correlation between the ML predictions and DFT calculated values, with a mean average error  (MAE) of 0.197 eV, highlighting the effectiveness of our model. In practice, it is desirable for  ML methods to have good \"cross -slab\" prediction capabili ties, as predicting the adsorption  activity on new MPEAs  is a key goal of these calculations. The LOSO test was designed to  evaluate the model's \"cross -slab\" prediction abilities, as seen in Figure 9b. This test resulted  in a higher MAE compared to 5 -fold cross validation test and worse predictions for outliers,  such as the B6 slab, where the ML model systematically underestimate d the adsorption  energies. However, our model accurately distinguishe d high and low adsorption energies on  the test slab, indicating its ability to correctly identify the most stable adsorption  site. The  accuracy of LOSO test can be improved by adding 20% samples from the test slab to the  training set, as shown in Figure 9c, and would likely to be improved with more slabs in the  training set as there are only 6 slabs in this study.    Figure 9. ML predicted adsorption energies against DFT calculated adsorption energies using  (a) 5-fold cross validation (b) leave-one-slab-out test, and (c) leave -one-slab-out with 20%  samples from the test slab added to the training set. ML predicted vacancy energies against  DFT calculated vacancy energies using (d) 5-fold cross validation (e) leave-one-slab-out test,  and (f) leave-one-slab-out with 25% samples from the test slab added to the training set.    Besides testing the performance of the ML model in predicting adsorption energies, we also  evaluated its capability in predicting vacancy energies. Figures 9(d-f) shows a co mparison  between the ML-predicted adsorption energies and DFT calculated values, using 5 -fold cross  validation test, LOSO, and LOSO with 25% samples added from the test slab to the training  set (25% means 2 samples on test slab considering there are 8 vaca ncy sites on each slab) .  Similar to the adsorption energy prediction, the model performed best using 5 -fold cross  validation test, as the training and test set have similar distributions. Also, LOSO test was  significantly worse than 5-fold cross validation testing and could be improved by adding 25%  samples from the test slab to the training set.    3.4. Integrated computational approaches as a surface electrochemistry simulator    We have shown the capability of ML to replace the use of high-throughput DFT for providing  data on the reactivity of the surface models of an alloy . Such results demonstrate the  effectiveness of ML modes in accurately predicting adsorption of different species and vacancy  energies on MPEA. The key findings from the work herein include:   (i) the application of the (MC)2 approach was capable of providing a platform for subsequent  DFT and ML approaches  â€“ that are relevant to the determination of electrochemical  properties â€“ where the prediction of electroc hemical properties has notionally been  historically challenging to model,   (ii) the workflow herein provides a visual, spatial map of electrochemical properties from  calculations entirely in-silico,   (iii) all elements in AlCrTiV exhibit unique properties due to different charge distributions  in  MPEA compared to their pristine crystal struc ture, in which Al  exhibits spontaneous  formation of vacancy in AlCrTiV  alloy that is well validated by independent empirical  data12,13 with the observation of high Al dissolution rate,  (iv) the simulations were also able to ascertain that for AlCr TiV, Ti has the most important  contribution on the formation of protective oxide species such as TiO2,   (v) the use of DFT and ML to model and predict  AlCrTiV surface activity/selectivity by  considering different species, which are critical for reduction oxidation  reactions, enable  the investigations of nano-scale electrochemical reactions on a complex surface.    Whilst we believe the above findings, and the pathway to obtaining such computational results  regarding surface electrochemistry are significant, it is also prudent to identify some of the  physical limitations, and required future work, in order to gain th e most benefit from such  approaches when applied to rationalising MPEA behaviour. Some points that remain open and  will be the focus of future studies include:  â€¢ As also noted in this study, the corrosion performance of MPEAs is closely linked to the  nature of the surface films. The modelling approach herein is  in part, a proxy to studying  surface films on the basis that the modelling approach deals with a â€˜snapshotâ€™ in time for an  alloy surface (whereas surface films develop dynamically). The model is  indeed a major  simplification of the MPEA -electrolyte interface, as it presently stands â€“ however this is  typical for any early model that may evolve in complexity.  â€¢ The numbers of sample for MPEAâ€™s surface, AlCrTiV, studied here using DFT are small.  Large datasets are necessary to make  any conclusive statement regarding elementalâ€™s role  on the electrochemical reactions in-silico.   â€¢ Additional features, such as different surface orientations, explicit modelling of electrolyte  system including cations and anions27,42, explicit treatment of temperature, pH and applied  potential27,29, will be beneficial for a more accurate prediction of MPEAâ€™s electrochemical  activity as well as to understand  multiple competing reactions on the surface.  A detailed  investigation on competing adsorptions in changing pH and applied potential 27,39 either  implicitly (as described using Eqns. 6 â€“ 9) or explicitly, can give insights on the subsequent  pathway (i.e., atomic dissolution, hydrogen evolution reaction, oxide formation).   â€¢ Large datasets are also necessary to improve the performance of our ML prediction   algorithm, which will be the key of such an on-the-fly approach.    The approach outlined in the study herein, whilst applied to only a single empirical benchmark,  has provided a computation workflow that may be utilised as a virtual electrochemical  characterisation tool . Such a tool has the capacity to predict  and therefore estimate the  electrochemical properties of MPEA s in-silico, by considering composition, crystallographic  orientation, and number of samplings. As a result, the computation workflow herein introduces  a cheaper and faster approach to garner an insight into electrochemical properties of new  MPEAs with tuned composition and phase, designed entirely from simulation s; with MPEAs  revealing promising electrochemical performance laboratory verified accordingly.   4. CONCLUSIONS    Herein, a methodology has been presented for generating realistic MPEA structures for  AlCrTiV, using implementation of the (MC)2 algorithm for the investigation of electrochemical  activity and development of activity/selectivity maps. Furthermore, the work herein reports on  the development and utilisation of  an ML method which capable of predicting surface  electrochemical activity  via adsorption and vacancy energies . Independent detailed  experiments were used to correlate and verify the electrochemical properties of the AlCrTiV  MPEA, derived from the simulations herein - in which Ti has the most important contribution  on the formation of protective oxide species such as TiO2.     The combined (MC)2 and DFT/ML approach presented in this work is a potential candidate for  intelligently exploring vast numbers of elemental combinations (MPEA compositions); which  is both critical and necessary in providing an in-silico insight for rationalising structure- electrochemistry relationships. One of the key features is the identification of active sites via  the construction of an electrochemical activity/selectivity map. Such mechanistic   understanding is beneficial for building an improved electrochemistry microkinetic model for  MPEAs, in which reactions are non-uniform, and each reaction is sensitive to local surface  features. The approach developed herein is readily applicable for the design and application to  other MPEAs.    DECLARATION OF COMPETING INTEREST  The authors declare that they have no known competing financial interests or personal  relationships that could have appeared to influence the work reported in this paper.    CODE AND DATA AVAILABILITY  Our implementation of (MC) 2 code for generating the bulk structure is available at:  https://github.com/SaminGroup/Dolezal-MC2.   The DFT calculation was executed using VASP software.   The ML code for energy prediction is available upon request.   Data generated from this study is all published in this study and available upon request.    AUTHOR CONTRIBUTIONS  Jodie A. Yuwono : Conceptualisation (lead); Resources (lead); Research (lead); Writing â€“  original draft (lead); Writing â€“ review & editing (lead). Xinyu Li: Research (equal); Writing â€“  original draft (supporting). Tyler D. DoleÅ¾al: Research (supporting); Writing â€“ original draft  (supporting); Writing â€“ reviewing & editing (supporting) . Adib J. Samin : Resources  (supporting); Writing â€“ review & editing ( supporting). Javen Qinfeng Shi : Resources  (supporting); Zhipeng Li: Writing â€“ review & editing (supporting). Nick Birbilis: Resources  (lead); Writing â€“ original draft (equal); Writing â€“ reviewing & editing (lead).    ACKNOWLEDGMENTS  The authors acknowledge the high -performance computing (HPC) resources from the  Department of Defense through Air Force Research Laboratory (AFRL) HPC Mustang and   from National Computing Infrastructure (NCI) Australia through NCI Gadi. X.L. and J.Q.S.  acknowledge the financial support from the Center for Augmented Reasoning, Australian  Institute for Machine Learning. T.D and A.S. acknowledge the financial support from the Air  Force Office of Scientific Research (AFOSR).  Financial support from the Office of Naval  Research under the contract ONR: N00014 -17-1-2807 with Dr. David Shifler and Dr. Clint  Novotny as program officers is gratefully acknowledged. 5. REFERENCES     1. Senkov, O. N., Miller, J. D., Miracle, D. B. & Woodward, C. Accelerated exploration of multi-principal  element alloys with solid solution phases. Nat. Commun. 6, 6529 (2015).  2. George, E. P., Raabe, D. & Ritchie, R. O. High-entropy alloys. Nat. Rev. Mater. 4, 515â€“534 (2019).  3. Fu, M., Ma, X., Zhao, K., Li, X. & Su, D. High-entropy materials for energy-related applications. iScience  24, 102177 (2021).  4. Birbilis, N., Choudhary, S., Scully, J. R. & Taheri, M. L. A perspective on corrosion of multi-principal  element alloys. Npj Mater. Degrad. 5, 14 (2021).  5. Li, Z. et al. cardiGAN: A generative adversarial network model for design and discovery of multi principal  element alloys. J. Mater. Sci. Technol. 125, 81â€“96 (2022).  6. Sorkin, V. et al. A first-principles-based high fidelity, high throughput approach for the design of high  entropy alloys. Sci. Rep. 12, 11894 (2022).  7. Rao, Z. et al. Machine learningâ€“enabled high-entropy alloy discovery. Science 378, 78â€“85 (2022).  8. Zhang, J., Cai, C., Kim, G., Wang, Y. & Chen, W. Composition design of high-entropy alloys with deep  sets learning. Npj Comput. Mater. 8, 89 (2022).  9. Qiu, Y., Thomas, S., Gibson, M. A., Fraser, H. L. & Birbilis, N. Corrosion of high entropy alloys. Npj  Mater. Degrad. 1, 15 (2017).  10. Qiu, Y. et al. A lightweight single-phase AlTiVCr compositionally complex alloy. Acta Mater. 123, 115â€“ 124 (2017).  11. Qiu, Y. et al. Microstructure and corrosion properties of the low-density single-phase compositionally  complex alloy AlTiVCr. Corros. Sci. 133, 386â€“396 (2018).  12. Choudhary, S., Qiu, Y., Thomas, S. & Birbilis, N. Element-resolved electrochemical analysis of  transpassive dissolution and repassivation behavior of the multi-principal element alloy AlTiVCr.  Electrochimica Acta 362, 137104 (2020).  13. Choudhary, S., Thomas, S., Macdonald, D. D. & Birbilis, N. Growth Kinetics of Multi-Oxide Passive Film  Formed Upon the Multi-Principal Element Alloy AlTiVCr: Effect of Transpassive Dissolution of V and Cr.  J. Electrochem. Soc. 168, 051506 (2021).  14. Cai, Z. et al. Microstructure and corrosion behavior of AlCrTiV-X (X = Cu, Mo, CuMo) high-entropy alloy  films in 3.5 wt.% NaCl solution. Surf. Interfaces 27, 101558 (2021).  15. DoleÅ¾al, T. D. & Samin, A. J. Adsorption of Oxygen to High Entropy Alloy Surfaces for up to 2 ML  Coverage Using Density Functional Theory and Monte Carlo Calculations. Langmuir 38, 3158â€“3169  (2022).  16. Ghazisaeidi, M. Alloy thermodynamics via the Multi-cell Monte Carlo (MC)2 method. Comput. Mater. Sci.  193, 110322 (2021).  17. Li, X., Chiong, R., Hu, Z., Cornforth, D. & Page, A. J. Improved Representations of Heterogeneous Carbon  Reforming Catalysis Using Machine Learning. J. Chem. Theory Comput. 15, 6882â€“6894 (2019).  18. BartÃ³k, A. P., Kondor, R. & CsÃ¡nyi, G. On representing chemical environments. Phys. Rev. B 87, 184115  (2013).  19. Li, X., Chiong, R. & Page, A. J. Group and Period-Based Representations for Improved Machine Learning  Prediction of Heterogeneous Alloy Catalysts. J. Phys. Chem. Lett. 12, 5156â€“5162 (2021).  20. Niu, C., Rao, Y., Windl, W. & Ghazisaeidi, M. Multi-cell Monte Carlo method for phase prediction. Npj  Comput. Mater. 5, 120 (2019).  21. Antillon, E. & Ghazisaeidi, M. Efficient determination of solid-state phase equilibrium with the multicell  Monte Carlo method. Phys. Rev. E 101, 063306 (2020).  22. Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H. & Teller, E. Equation of State  Calculations by Fast Computing Machines. J. Chem. Phys. 21, 1087 (2004).  23. Kresse, G. & FurthmÃ¼ller, J. Efficient iterative schemes for ab initio total-energy calculations using a  plane-wave basis set. Phys. Rev. B 54, 11169â€“11186 (1996).  24. Kresse, G. & Joubert, D. From ultrasoft pseudopotentials to the projector augmented-wave method. Phys.  Rev. B 59, 1758â€“1775 (1999).  25. Monkhorst, H. J. & Pack, J. D. Special points for Brillouin-zone integrations. Phys. Rev. B 13, 5188â€“5192  (1976).  26. Perdew, J. P., Burke, K. & Ernzerhof, M. Generalized Gradient Approximation Made Simple. Phys. Rev.  Lett. 77, 3865â€“3868 (1996).  27. Yuwono, J. A. et al. Aqueous electrochemistry of the magnesium surface: Thermodynamic and kinetic  profiles. Corros. Sci. 147, 53â€“68 (2019).  28. Ke, H. & Taylor, C. D. First-Principles Modeling of the Repassivation of Corrosion Resistant Alloys: Part  I. O and Cl Adsorption Energy. J. Electrochem. Soc. 167, 111502 (2020). 29. Yuwono, J. A., Taylor, C. D., Frankel, G. S., Birbilis, N. & Fajardo, S. Understanding the enhanced rates of  hydrogen evolution on dissolving magnesium. Electrochem. Commun. 104, 106482 (2019).  30. Ogle, K. Atomic Emission Spectroelectrochemistry: Real-Time Rate Measurements of Dissolution,  Corrosion, and Passivation. Corrosion 75, 1398â€“1419 (2019).  31. Murphy, K. P. Machine learning: a probabilistic perspective. (MIT Press, 2012).  32. JÃ¤ger, M. O. J., Morooka, E. V., Federici Canova, F., Himanen, L. & Foster, A. S. Machine learning  hydrogen adsorption on nanoclusters through structural descriptors. Npj Comput. Mater. 4, 37 (2018).  33. Haruyama, J., Ikeshoji, T. & Otani, M. Electrode potential from density functional theory calculations  combined with implicit solvation theory. Phys. Rev. Mater. 2, 095801 (2018).  34. Maurice, V., Strehblow, H.-H. & Marcus, P. In situ STM study of the initial stages of oxidation of Cu(111)  in aqueous solution. Surf. Sci. 458, 185â€“194 (2000).  35. Islam, M. M., Diawara, B., Maurice, V. & Marcus, P. Surface reconstruction modes of Cu2O(001) surface:  A first principles study. Surf. Sci. 604, 1516â€“1523 (2010).  36. Bastos, A. C., Quevedo, M. C., Karavai, O. V. & Ferreira, M. G. S. Reviewâ€”On the Application of the  Scanning Vibrating Electrode Technique (SVET) to Corrosion Research. J. Electrochem. Soc. 164, C973â€“ C990 (2017).  37. Salleh, S. H., Thomas, S., Yuwono, J. A., Venkatesan, K. & Birbilis, N. Enhanced hydrogen evolution on  Mg (OH)2 covered Mg surfaces. Electrochimica Acta 161, 144â€“152 (2015).  38. Thomas, S. et al. On the effect of Fe concentration on Mg dissolution and activation studied using atomic  emission spectroelectrochemistry and scanning electrochemical microscopy. Electrochimica Acta 210,  271â€“284 (2016).  39. Ke, H., Li, T., Lu, P., Frankel, G. S. & Taylor, C. D. First-Principles Modeling of the Repassivation of  Corrosion Resistant Alloys: Part II. Surface Adsorption Isotherms for Alloys and the Chloride  Susceptibility Index. J. Electrochem. Soc. 167, 111501 (2020).  40. Samin, A. J. Oxidation thermodynamics of Nb-Ti alloys studied via first-principles calculations. J. Alloys  Compd. 879, 160455 (2021).  41. Batchelor, T. A. A. et al. High-Entropy Alloys as a Discovery Platform for Electrocatalysis. Joule 3, 834â€“ 845 (2019).  42. Shah, A. H. et al. The role of alkali metal cations and platinum-surface hydroxyl in the alkaline hydrogen  evolution reaction. Nat. Catal. 5, 923â€“933 (2022).      Graphical abstract            Figure S1. The adsorption energies of *O, *H and *Cl on atop sites for each slabs of the  AlCrTiV (001) surface.",
      "references": [
        "Accelerated exploration of multi-principal element alloys with solid solution phases.",
        "High-entropy alloys.",
        "High-entropy materials for energy-related applications.",
        "A perspective on corrosion of multi-principal element alloys.",
        "cardiGAN: A generative adversarial network model for design and discovery of multi principal element alloys.",
        "A first-principles-based high fidelity, high throughput approach for the design of high entropy alloys.",
        "Machine learningâ€“enabled high-entropy alloy discovery.",
        "Composition design of high-entropy alloys with deep sets learning.",
        "Corrosion of high entropy alloys.",
        "A lightweight single-phase AlTiVCr compositionally complex alloy.",
        "Microstructure and corrosion properties of the low-density single-phase compositionally complex alloy AlTiVCr.",
        "Element-resolved electrochemical analysis of transpassive dissolution and repassivation behavior of the multi-principal element alloy AlTiVCr.",
        "Growth Kinetics of Multi-Oxide Passive Film Formed Upon the Multi-Principal Element Alloy AlTiVCr: Effect of Transpassive Dissolution of V and Cr.",
        "Microstructure and corrosion behavior of AlCrTiV-X (X = Cu, Mo, CuMo) high-entropy alloy films in 3.5 wt.% NaCl solution.",
        "Adsorption of Oxygen to High Entropy Alloy Surfaces for up to 2 ML Coverage Using Density Functional Theory and Monte Carlo Calculations.",
        "Alloy thermodynamics via the Multi-cell Monte Carlo (MC)2 method.",
        "Improved Representations of Heterogeneous Carbon Reforming Catalysis Using Machine Learning.",
        "On representing chemical environments.",
        "Group and Period-Based Representations for Improved Machine Learning Prediction of Heterogeneous Alloy Catalysts.",
        "Multi-cell Monte Carlo method for phase prediction.",
        "Efficient determination of solid-state phase equilibrium with the multicell Monte Carlo method.",
        "Equation of State Calculations by Fast Computing Machines.",
        "Efficient iterative schemes for ab initio total-energy calculations using a plane-wave basis set.",
        "From ultrasoft pseudopotentials to the projector augmented-wave method.",
        "Special points for Brillouin-zone integrations.",
        "Generalized Gradient Approximation Made Simple.",
        "Aqueous electrochemistry of the magnesium surface: Thermodynamic and kinetic profiles.",
        "First-Principles Modeling of the Repassivation of Corrosion Resistant Alloys: Part I. O and Cl Adsorption Energy.",
        "Understanding the enhanced rates of hydrogen evolution on dissolving magnesium.",
        "Atomic Emission Spectroelectrochemistry: Real-Time Rate Measurements of Dissolution, Corrosion, and Passivation.",
        "Machine learning: a probabilistic perspective.",
        "Machine learning hydrogen adsorption on nanoclusters through structural descriptors.",
        "Electrode potential from density functional theory calculations combined with implicit solvation theory.",
        "In situ STM study of the initial stages of oxidation of Cu(111) in aqueous solution.",
        "Surface reconstruction modes of Cu2O(001) surface: A first principles study.",
        "Reviewâ€”On the Application of the Scanning Vibrating Electrode Technique (SVET) to Corrosion Research.",
        "Enhanced hydrogen evolution on Mg (OH)2 covered Mg surfaces.",
        "On the effect of Fe concentration on Mg dissolution and activation studied using atomic emission spectroelectrochemistry and scanning electrochemical microscopy.",
        "First-Principles Modeling of the Repassivation of Corrosion Resistant Alloys: Part II. Surface Adsorption Isotherms for Alloys and the Chloride Susceptibility Index.",
        "Oxidation thermodynamics of Nb-Ti alloys studied via first-principles calculations.",
        "High-Entropy Alloys as a Discovery Platform for Electrocatalysis.",
        "The role of alkali metal cations and platinum-surface hydroxyl in the alkaline hydrogen evolution reaction."
      ],
      "meta_data": {
        "arxiv_id": "2305.12059v1",
        "authors": [
          "Jodie A. Yuwono",
          "Xinyu Li",
          "Tyler D. DoleÅ¾al",
          "Adib J. Samin",
          "Javen Qinfeng Shi",
          "Zhipeng Li",
          "Nick Birbilis"
        ],
        "published_date": "2023-05-20T02:07:09Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Develops an end-to-end in-silico workflow to predict and spatially map early-stage electrochemical activity/selectivity of multi-principal element alloy (MPEA) surfaces, demonstrated on equiatomic bcc AlCrTiV. Key contributions: (1) uses (MC)^2 Monte Carlo + DFT to generate equilibrium bulk phases/structures and derive realistic surface slabs; (2) performs high-throughput DFT to compute adsorption energies of corrosion-relevant species (*O, *H, *Cl) and surface vacancy formation energies as descriptors for passivation tendency (oxide formation), cathodic activity (HER-related H adsorption), and anodic dissolution susceptibility (vacancy/Cl/O affinity); (3) constructs surface â€œactivity/selectivity mapsâ€ that localize likely anodic vs cathodic regions; (4) trains a kernel ridge regression model with a combined representation (adsorbate+slab+site) enabling cross-adsorbate/cross-site predictions and partial cross-slab generalization; (5) qualitatively validates predicted dissolution propensity (Al most favorable vacancy/dissolution; Ti strongly binds O/Cl implying protective oxide role) against prior element-resolved AESEC and XPS literature for AlCrTiV.",
        "methodology": "Three-stage workflow: (i) Bulk/phase prediction via multi-cell Monte Carlo (MC)^2 at 300 K, 0 Pa using local-flip and intra-swap moves with Metropolis acceptance; DFT (VASP, PAW, GGA-PBE, 450 eV, 2Ã—2Ã—2 k-mesh, full relaxation ISIF=3) supplies energies/volumes for MC steps, yielding equilibrium phase fractions and bcc lattice constant. (ii) Surface modeling and descriptors from DFT: build (2Ã—1) bcc (001) slabs (6â€“8 layers, 20 Ã… vacuum; top 4 layers relaxed) and compute single-adsorbate adsorption energies for *H/*O/*Cl on atop/bridge/hollow sites and vacancy formation energies for top-layer vacancies (VASP, 500 eV, 3Ã—3Ã—1 k-mesh). Adsorbate references: 1/2 H2; H2Oâ€“H2; HClâ€“1/2 H2. Implicit potential/pH connection referenced via computational hydrogen electrode relations. Elemental contribution to adsorption energy explored with least-squares fit using normalized coordination counts. Vacancy reference energies compared; HEA-average reference chosen to match experimental dissolution trends. (iii) ML surrogate: Kernel Ridge Regression with RBF kernel; hyperparameters via grid search. Combined representation: elemental properties (EP) to encode adsorbate identity, SOAP to encode slab atomic environments, and GP-CAF to encode adsorption site coordination; trained to predict adsorption and vacancy energies.",
        "experimental_setup": "Computational datasets: DFT-generated labels for six distinct AlCrTiV bcc (001) surface slabs derived from (MC)^2 bulk structures; each slab provides 32 adsorption sites (atop, bridge-x, bridge-y, hollow) for each adsorbate (*H, *O, *Cl) under single-adsorbate conditions, plus 8 top-layer vacancy sites. Slab thicknesses of 6â€“8 layers assessed; 20 Ã… vacuum; partial relaxation. Validation/benchmarks: (1) Bulk phase and lattice constant validated against experimental reports (single-phase bcc; simulated XRD compared to literature XRD; lattice constant ~3.05 Ã… vs ~3.075 Ã…). (2) Electrochemical relevance benchmarked qualitatively against independent experimental literature for AlCrTiV (AESEC element-resolved dissolution at OCP and during polarization; XPS passive film composition), used to choose vacancy reference and interpret predicted roles of Al and Ti. ML validation: 5-fold cross-validation (reported MAE ~0.197 eV for adsorption energies), leave-one-slab-out (LOSO) to test cross-slab generalization, and LOSO with small fraction of target-slab samples added (20% for adsorption; 25% for vacancies) to quantify transfer improvement.",
        "limitations": "Modeling simplifications: treats electrochemical interface as static, clean slabs with isolated neutral adsorbates (*H/*O/*Cl) rather than charged species; no explicit solvent, ions, electric double layer, or dynamic passive-film growth. Potential and pH effects are only discussed/handled implicitly (computational hydrogen electrode concept) rather than explicitly sampled. Sampling limitations: only six (001) slabs and limited surface configurations; small DFT dataset restricts statistical confidence in element-by-element role attribution and limits ML cross-slab generalization (LOSO performance degrades, slab-dependent bias observed). Reaction complexity: only single-adsorbate calculations (no co-adsorption/coverage effects), limited treatment of lateral interactions; vacancy formation used as proxy for dissolution without explicit kinetics, diffusion, oxide nucleation/growth, or competing reaction pathways. Vacancy reference energy choice is somewhat ad hoc and tuned to match experimental dissolution ordering. Generality: demonstrated on a single alloy and one surface orientation; transfer to other MPEAs/orientations remains to be proven.",
        "future_research_directions": "Scale up DFT sampling across many more slabs, compositions, and surface orientations to build robust, transferable ML surrogates with stronger cross-alloy/cross-surface generalization. Incorporate explicit electrochemical conditions: solvent and electrolyte (cations/anions), applied potential and pH (implicit CHE plus explicit field/charge or grand-canonical approaches), temperature effects, and adsorption isotherms/coverage and co-adsorption to capture competitive reactions. Extend from early-stage descriptors (adsorption/vacancy) to mechanistic microkinetic or reactive dynamics models for non-uniform corrosion/passivation, including oxide nucleation, film growth, and dissolution kinetics. Improve ML architecture/representations (active learning, uncertainty quantification, message-passing/graph neural nets) and on-the-fly data acquisition to reduce DFT cost. Link predicted spatial activity maps to experimentally measurable local electrochemistry (SECM/SVET/STM) and element-resolved dissolution/passive-film data for rigorous validation and calibration; apply workflow to screen/design new MPEAs with tailored corrosion resistance or catalytic behavior.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "PromptCoT: Align Prompt Distribution via Adapted Chain-of-Thought",
      "full_text": "1    Modulation Mechanism of Ionic Transport through Short  Nanopores by Charged Exterior Surfaces â€   Long Ma,a,b Zhe Liu,a Jia Man,a Jianyong Li,a Zuzanna   S. Siwyc and Yinghua Qiua,b,d*    a. Key Laboratory of High Efficiency and Clean Mechanical Manufacture of Ministry of  Education, National Demonstration Center for Experimental Mechanical Engineering  Education, School of Mechanical Engineering, Shandong University, Jinan, 250061, China   b. Shenzhen Research Institute of Shandong University, Shenzhen, 518000, China   c. Department of Physics and Astronomy, University of California, Irvine, California 92697,  United States  d. Suzhou Research Institute of Shandong University, Suzhou, 215123, China  â€ . Electronic supplementary information (ESI) available . See electronic supplementary  information for simulation details, additional simulation results of ionic current through  nanopores, and ion concentration distributions inside nanopores.              *Corresponding author: yinghua.qiu@sdu.edu.cn     2    Abstract  Short nanopores have various applications in biosensing, desalination, and energy  conversion. Here, the modulation of charged exterior surfaces on ionic transport is  investigated through simulations with sub -200 nm  long nanopores under applied  voltages. Detailed analysis of ionic current, electric field strength, and fluid flow inside  and outside nanopores reveals that charged e xterior surfaces can increase ionic  conductance by increasing both the concentration and migration speed of charge  carriers. The electric double layers near charged exterior surfaces provide an ion  pool and an additional passageway for counterions, which lead to enhanced exterior  surface conductance and ionic  concentrations at pore entrances and inside the  nanopore. We also report that charges on the membrane surfaces increase electric  field strengths inside nanopores. The effective width of a ring with surface charges  placed at pore entrances (Lcs) is considered as well by studying the dependence of  the current on Lcs. We find  a linear relationship between  the effective  Lcs and the  surface charge density and voltage , and an inverse relationship between the  geometrical pore length and salt concentration. Our results elucidate the modulation  mechanism of charged exterior surfaces on ionic transport through short nanopores,  which is important for the design and fabrication of porous membranes.     Keywords:  Surface conductance, e lectric double layers, short n anopores, charged exterior  surfaces, ion concentration polarization.    3    Introduction  Recent nanofabrication techniques allow the creation of various nanopores,  spanning from polymer nanopores 1 with tens of micrometers in length to two - dimensional nanopores with an atomic thickness. 2 When nanopores are placed  between two reservoirs to form an ionic current loop, these nanofluidic devices offer  a versatile platform for investigating the transport of io ns and fluids in confined  spaces, which can shed light on various applications of nanopores, such as  biosensing,3, 4 ionic circuits,5 desalination,6, 7 and energy conversion.8-10   Under the highly confined space of nanopores, interfacial  effects, in particular  surface charges at the solid -liquid interface, provide significant modulation of ion  transport,11 which originates from the electrostatic interaction s among surface  charges and mobile ions. 12 In long nanopores, charged inner -pore surfaces usually  provide the mos t significant influence on the transport of ions and charged  biomolecules,11 through the formation of electric double layers (EDLs). EDLs alter the  ionic concentration distributions inside nanopores and enable the ionic selectivity of  nanopores.13, 14   Due to the strong selective transport of counterions in long nanopores, the ionic  concentration becomes enriched and depleted at the exit and entrance of the  nanopore, respectively. This phenomenon , known as the ion concentration  polarization (ICP), 15 has many potential applications in the preconcentration of  charged analytes,16 and material separation.17 ICP leads to S-shaped current-voltage  (I-V) curves, indicating that a limiting current is reached under relatively high voltages.  18 In these cases, ICP affect s the ionic current through the modulation of the ionic  concentration at one entrance and inside nanopores, which can become more  significant in short nanopores because of the enhanced electric field strength.19 4    Because surface charges of inner-pore walls, instead of those on the membrane  surface, determine the properties of ion current through the pore with a high ratio of  the pore length to the diameter, in earlier simulations20-22 and theoretical analysis,13, 23,  24 the exterior surface charges of nanopores have seldom been considered. As pore  length decreases to the nanoscale , due to the smaller length-diameter ratio, the  charged exterior surface of the nanopore provides a stronger influence on ionic  transport than that of the charged inner surface .25-27 With the modification using  polymer molecules  on exterior membrane surfaces to reach different charge  polarities, Tagliazucchi et al.25 realized an ultra-thin current rectifier using a 15 nm in  diameter nanopore with 20 nm in length. In nanofluidic simulations with n anopores of  various lengths, Ma et al.26 found that exterior charged surfaces dominated the total  ionic current rectification when the length -diameter ratio was less than ~3.5. Using  nanopores of 20 nm in length and 8 nm in diameter functionalized with bipolar  polyelectrolyte brushes on the inne r and exterior surfaces, Lin et al. 27 found that  bipolar functional groups carri ed on exterior surface s exhibited stronger ability to  induce ion current rectification than charges on inner surfaces.   Charged exterior pore walls modulate ionic transport usually through the  formation of ion pools at the pore entrance and additional passageways at the pore  exit for ionic transport. 19, 28 It was also found that ions in EDL might even exhibit  enhanced ionic mobility compared to the ions at the pore center .29 In our earlier  work with a conical nanopore of 100 nm in length, 2 and 20 nm in radii of the tip and  base opening s, surface charges on the exterior surface were shown to attract a  substantial amount of additional counterions, which provided an ion pool and resulted  in current rectification. 19 From the simulation of osmotic energy conversion under  natural salt gradients, 30 the charged exterior surface on the low -concentration side  could significantly promote the diffusion of counterions, which was mainly attributed  to the tangential ionic transport along the exterior charged surface.28, 31 5    With the development in nanofabrication and surface modification  techniques,  short nanopores of controlled dimensions and surface properties can find a variety of  different applications, because they offer higher resolution in detection, 32 and larger  throughput.28 In this study,  modulation of charged exterior surfaces of short  nanopores on ionic transport properties was systematically investigated through   simulations with the adjusted surface charges. We show that for nanopores with sub- 200 nm  length, charged exterior surfaces  significantly influence ionic current , with  improved cation and anion current s. Based on the detailed analysis of the ionic flux,  electric field strength, and fluid flow, the modulation mechanism of charged exterior  surfaces on ionic current was determined. Through adjusting the charged area on the  membrane surface, the effective widt h (Lcs) of the charged ring region near the  nanopore was found which may be useful in the propagation of ionic transport from  single nanopores to porous membranes. The effective magnitude of Lcs was explored  under different parameters of the nanopore and applied conditions. With the  quantitative relationship between Lcs and the pore length, diameter, surface charge  density, applied voltage as well as salt concentration, o ur results provide important  guidelines for the design of nanopore arrays in thin porous membranes.  Simulation Details  Nanofluidic simulations were conducted with COMSOL Multiphysics. As shown  in Figure S1, nanopores connect two  reservoirs with 5 Î¼m in radius and 5 Î¼m in  length. The diameter of nanopores varied from 4 to 55 nm, and the nanopore length  was changed from 20 to 1000 nm  to investigate the ionic transport through  nanopores under different length scales.28 Default pore diameter and length were 10  and 50 nm, respectively.14, 30, 33, 34 The system temperature was maintained at 298 K,  and the dielectric constant of water was set to 80. Because KCl solution s are  commonly applied in biosensing based on nanopores,35-37 KCl solutions varying from  10 to 500 mM in concentration were chosen in the simulations, of which 100 mM was  the default concentration . T he diffusion coefficients of K + and Cl âˆ’ ions were  considered as  1.96Ã— 10âˆ’9 and 2.03Ã— 10âˆ’9 m2/s, respectively. 38 Because of the  symmetric geometry and surface charge distribution of nanopores, positive voltages 6    were applied across nanopores which  varied from 0 to 1 V . Ionic current at the  default voltage of 1 V was used for data analysis.  In our simulation models, we used coupled Poisson-Nernst-Planck and Navier - Stokes equations (Equations 1-4),19, 32 which describe the ionic distributions near  charged surfaces, ionic transport in aq ueous solutions, and fluid flow under confined  spaces.     2 1 N ii i z FCï¥ïª ï€½ ï€½ï€­ïƒ¥ïƒ‘  (1)     0i i i iii i Fz C DC D C RT ïªïƒ¦ïƒ¶ïƒ— ï€½ ïƒ— ï€­ ï€­ ï€½ïƒ§ïƒ·ïƒ¨ïƒ¸ Juïƒ‘ ïƒ‘ ïƒ‘ ïƒ‘  (2)    ï€¨ ï€© 2 1 0 N ii i p z FCï­ïª ï€½ ï€­ ï€­ ï€½ïƒ¥uïƒ‘ ïƒ‘ ïƒ‘  (3)    0ïƒ‘ïƒ— ï€½u  (4)  in which Ï† and N are the electrical potential and number of ion types. F, R, T, Âµ, and  p are the Faradayâ€™s constant, gas constant, temperature , liquid viscosity, and  pressure. Îµ is the dielectric constant, and u is the fluid velocity.  Ji, Ci, Di, and zi are  the ionic flux, concentration, diffusion coefficient, and valence of ionic species i (K+  and Clâˆ’ ions), respectively. The ionic current (I) under different voltages was obtained  by integrating the ionic flux of cations and anions over the boundary of the reservoir   with Equation 5.    2  diiS i I F z Sïƒ¦ïƒ¶ï€½ïƒ— ïƒ§ïƒ·ïƒ¨ïƒ¸ïƒ¥ïƒ² Jn   (5)  where S represents the reservoir boundary, and n is the unit normal vector.  To understand the influence of charged exterior surfaces on the ionic properties  of nanopores, simulation models of nanopores with uniformly charged surfaces (ACS)  and only charged inner-pore surface s (ICS) have been built . The surface charge  density vari ed from âˆ’0.04 to âˆ’0.16 C/m 2, of which âˆ’0.08 C/m 2 was used as the 7    default value.32, 39, 40 The same mesh strategy as used in our previous publications  was applied (Figure S2).19, 28, 41 For charged surfaces, t he mesh size of 0.1 nm was  applied which is sufficient to fully consider the influence of EDLs on the transport of  ions and liquids. The convergence of ionic current through the nanopore was used to  determine the minimum size of mesh  on pore walls ( Figure S2 ). More simulation  details are provided in Table S1.  Results and Discussion     Figure 1. Ionic behaviors in nanopores with all charged surfaces (ACS) and charged  inner-pore surfaces (ICS) under different voltages and pore lengths. (a) Current- voltage curves. Nanopores with all neutral surfaces are denoted as the NCS case.  Black and red dash lines are the theoretical fitting with Equation 6 and Equation 7,  respectively. (b) Difference in ionic current under various voltages calculated with  (IACS â€“ IICS)/IICS. The inset shows the simulation scheme. (c) Difference in ionic current  contributed by total ions, K+ ions, and Clâˆ’ ions under various pore lengths calculated  as (IACS â€“ IICS)/IICS. The yellow region indicates the difference in current within Â± 10%.  8    (d) Cation transfer numbers in ACS and ICS cases under different pore lengths.  Insets show the simulation models of ACS and ICS. Surface charges are shown in  red.  The pore diameter is 10 nm.  Short nanopores with thicknes ses ranging from  10 to 100 nm 42-44 are usually  employed for the detection of biological molecules ,3 such as DNA, RNA, and protein  molecules, bio-nanoparticles,45, 46 like liposomes, and viruses, as well as  artificial  particles of varying sizes. 47, 48 Here, 10-nm-in-diameter nanopores with 50 nm in  length w ere first chosen to investigate the influence of charged exterior nanopore  surfaces on ionic transport inside the pores.  Figure 1a shows the I-V curves obtained in nanopores with  different charged  surfaces. Three simulation cases  were considered , i.e.  nanopores with all neutral  surfaces, NCS, nanopores with only charged inner surfaces, ICS, and nanopores  with all charged surfaces, ACS. From the classical theory, nanopores can be  considered a resistor in the ionic circuit. For  the NCS case, the absence of surface  charges results in a uniform ion concentration inside the nanopore, which does not  change with the applied voltage. The I-V curve in the NCS case  is linear which can  be well -fitted using Equation 6 with the consideration of the pore resistance and  access resistance.35     2 41/ / ( 2 ) / ( ) ï°ï« ï«ï€½ ï€½ ï€« ï€½ ï€«t p ac LIR dV R R V dV  (6)  where V is the applied voltage. Rt, Rp, and Rac are total resistance, pore resistance,  and access resistance , respectively. ï« is the conductivity of the aqueous  solution. L  and d and the length and diameter of the nanopore.   In the ICS case, the inner-pore surface is negatively charged  at â€“0.08 C/m 2,  such that an EDL layer is formed near the charged surface. Due to the accumulation  of counterions in the EDLs, the ion current in the ICS case is enhanced compared to  the current through a neutral pore. The ICS and NCS nanopore s also differ in their I-9    V characteristics. At the low voltage regime, below 0.2 V, an ICS nanopore exhibits a  linear I-V curve. However, as the voltage increases, the I-V curve becomes S-shaped.  The nonlinearity of the pore behavior is caused by concentration polarization  (ICP)  that induces the formation of a depletion zone at one pore entrance where the  concentration of both cations and anions is lower than in the bulk .15 ICP results from  ion selectivity and becomes more pronounced at higher voltages . As the length of a  nanopore increases, ICP gradually weakens at the same applied voltages. When the  nanopore length exceeds 200 nm, I-V curves in the  ICS case show a linear  characteristic in the whole examined voltage range between 0 to 1 V (Figure S3).  As mentioned above, in many previous simulations, the exterior surface charges  of nanopores were ignored, particularly when nanopores had micrometer -scale  lengths.20-22 Recent studies25, 26 show that a s the pore length shortens, the exterior  surface charge may have a significant impact on the ion current inside the nanopores.  Here, charged exterior surfaces are considered in the ACS case. The I-V curve for  the ACS case is almost linear, which can be fitted using  Equation 7 referring to the  work of Lee et al.49 with the consideration of the surface charge effect on the pore  resistances and access resistances.     1 2 4 1 2 14 ï« ï° ï¡ ï¢ ï€­ ïƒ©ïƒ¹ ïƒªïƒºï€½ ïƒ— ï€½ ïƒ— ïƒ— ï€« ïƒªïƒº ï€«ïƒªïƒº ï€«ïƒ«ïƒ» Du Du LI V G V ld d l d  (7)  where G is the nanopore conductance. Î± = 2 is a geometrical prefactor , and Î² = 2 is  a numerical constant.  lDu is the Dukhin length , lDu=ï«s/ï«b with ï«s and ï«b the surface  conductivity and bulk conductivity, respectively.   The good fit between Equation 7 and the simulation result also indicates that the  surface charges in the ACS case affect both the ionic transport inside the nanopore  and at the nanopore opening. Note that although ICP is still present in the ACS case  (Figure 2a), the limiting current does not appear in the  I-V curve, which will be 10    explained later. The current values in the ACS case are ~2.5 times higher than those  in the NCS case. Compared with the ICS system, ionic transport in the ACS  nanopore is greatly improved as well. Figure 1b shows the relative current difference  in both ACS and ICS cases, which becomes more pronounced with applied voltages.  At 1 V, the presence of charges on the membrane exterior increases ion current by  ~125%.  In Figure 1c, the current difference between ACS and ICS systems at 1 V is  analyzed as a function of pore length. The current difference is calculated as (IACS-IICS)  / IICS, where IACS and IICS represent the ion current obtained from the ACS and ICS  cases, respectively. The pore diameter is kept at 10 nm, and the pore length varies  from 5 to 1000 nm. When the pore length exceeds 200 nm, the difference in currents  is less than 1 2% and  the influence of exterior surface charges can be ignored.  However, as the pore length decreases to sub -200 nm, the current difference  between ICS and ACS  cases, including cation current, anion current,  and total  current, increases significantly with further shortening of the pore length . In  nanopores with a length of ~ 10-20 nm, the current difference between both cases  reaches a maximum that can exceed ~240%. Here, we focus on the total current and  K+ ion current because of the cationic selectivity of the negatively charged nanopores.  When the pore length  is further reduc ed to 5 nm, the difference between total  currents decreases but remains at a high value of ~150%. Based on the comparison  between ion currents in both ICS and ACS cases, we conclude the charged exterior  surfaces have a significant impact on the transport of cations and anions inside the  short nanopores.  As negatively charged nanopores are selective to cations, we also considered a  cation transfer number (t+)14 under various pore lengths. For nanopores with a length  of less than 10 nm, t+ in the ACS case is higher than that in the ICS case , Figure 1d.  Figure 1d also shows that increasing the pore length from 5 to 100 nm enhances the 11    cation selectivity in both cases, ICS and ACS nanopores, and t+ saturates at ~0.95 at  the pore length of ~100 nm. Surprisingly, the cation transfer number in the ACS case  is smaller than that in the ICS case , most probably due to an enhanced contribution  of Clâˆ’ to ion current , which is discussed below . For nanopores longer than 200 nm,  the cation selectivity is determined by the charged inner pore surface , and the  influence of the charged exterior surface can be ignored.  Figure 2. Details of ion transport in both ACS and ICS cases. (a) Concentration  distributions of all ions along the pore axis at different voltages. (b) Radial  distributions of ion concentration and electric field strength in the center cross-section  of the nanopore. The orange and magenta represent the regions of bulk conductance  (Gb) and surface conductance (Gs) inside nanopores, respectively. The surface  conductance was determined within 1 nm from the surface.  (c) Radial distributions of  ionic current at the center cross-section of the nanopore. Each value was obtained  through the integration of the ionic flux over a 0.05-nm-wide segment. Inset shows  the total bulk current (Ib) and surface current (Is) in both cases. (d) 2D concentration  12    distributions of total ions (color maps) and fluid flow (white lines with arrows). Black  arrows show the center cross-section of the nanopore.   To reveal the regulation mechanism of exterior surface charge s on ion ic  transport in short nanopores, we investigated the details  of ionic properties of  nanopores,28, 41 such as the distribution of ion concentration, electric field strength,  and ion flux, as shown in Figure 2. Figure 2a shows the ion concentration distribution  along the pore axis.  In both ICS and ACS cases, ICP  causes voltage-dependent  formation of ionic depletion and enrichment zones at the left and right ends of the  nanopore, respectively. In the ACS case, the charged exterior surfaces provide an  additional passageway for counterions (Figure 2d), such that more ions can enter the  nanopore, resulting in a higher ion concentration than  that in the ICS case. Note that  in the ACS case , the lowest concentration appears outside of the nanopore. The  higher ionic concentration and enhanced surface conductance in the ACS system  prevent the formation of the limiting current at higher voltages such that the I-V  curves are linear.  Based on the continuity of ion current through the nanopore, the electrochemical  properties of the center cross-section of the pore can be used to analyze the impact  of the charged exterior surface on the ion ic transport quantitatively.28, 41 Figures 2b  and 2c show the radial distributions of  the ion concentration, electric field strength,  and ionic current at the center cross-section of the nanopore . In the nanopore, the  ionic current is directly related to the number and migration rate of charge carriers,  which are determined by the ion concentration and electric field strength  based on  the Nernst-Einstein equation. From the concentration distribution, EDLs form within  ~1 nm of the charged inner surface  with the accumulation of cations, as predicted  from the Debye length. 12 Comparing both radial distributions of ion concentration in  the ACS and ICS cases, charged exterior surfaces increase the ion concentration in  the center and EDLs regions of the nanopore by ~70% and ~5%, respectively . The 13    electric field strength at the central cross -section shows no obvious radial  dependence, which represents almost  a constant value  at different radial positions .  To our surprise, the electric field strength at the central cross-section of the nanopore  in the ACS  case is roughly twice that in the ICS case. T he larger electric field  strength can provide the ions with a much higher migration rate. With the charged  exterior surfaces of the nanopore, both the ion concentration and ionic migration rate  are enhanced, which can result in a significant increase in ionic current.   Figure 2c illustrates the radial distributions of ion current at the central cross - section of the nanopore in both cases of ICS and ACS, which follow a similar trend to  that of ion concentration distribution. Due to the higher ion concentration in the EDLs,  a larger local ion current appears near the charged inner  surface. In charged  nanopores, t he conductance can be divided into surface conductance in the EDL  region near charged surfaces, and bulk conductance in the center region of the  nanopore. Considering that the Debye length in 0.1 M KCl solution is ~1 nm, the  current contributed by ions within 1 nm from the pore wall and ions 1 nm away from  the surface is defined as the surface current  Is, and bulk current Ib, respectively.30, 49  From Figure 2c, a larger current appears at each position on the cross -section in the  ACS case compared to the ICS system, which results  in larger bulk and surface  currents. As shown in the inset of Figure 2c, the surface current and the bulk current  are increased significantly by ~142% and ~112%  respectively when exterior surface  charges are added.   In aqueous solutions, the directional transport of hydrated ions can lead to the   fluid flow i.e. electroosmotic flow (EOF).14 By including the Navier -Stokes equations  in the simulations, the fluid flow characteristics could be captured which can reflect  the flow trajectory of ions. As seen in Figure 2, when surface charges are considered  on the exterior pore walls, cations accumulate near the charged surface which can  be driven by the applied electric field to migrate parallel to the exterior surface. In the 14    ACS case, all the EDLs regions near charged pore walls can serve as an additional  passageway of the counterion s, which leads to a much larger surface current. T he  EDLs near the charged exterior surface provide a pool of K + ions, which can quickly  migrate into the nanopore and result in a higher ion concentration  at the orifice and  inside the nanopore . Note that MD simulations of ionic transport in nanopores  revealed a possibility for the counter ions to exhibit enhanced mobility in the EDL  region compared to the pore center.29 In the ICS case  under high electric field  strengths, on the other hand, ion depletion at the entrance of the nanopore becomes  more significant due to insufficient ion su pply, which leads to the occurrence of  limiting current.  It is important to note  that besides the enhancement in ionic transport, charged  exterior surfaces also promote the velocity of EOF inside the nanopore. Figure S4  shows that the velocity of EOF in the ACS case is ~40% larger than that of the ICS  case. The EOF can enhance the transport of counter ions through enhancement of  the concentration and migration rate of counterions inside the nanopore. Importantly,  in the  ACS case , more K+ ions accumulate inside the nanopore , leading to better  screening of the surface charge s by the counterions, allowing more Cl âˆ’ ions to be  transported through the nanopore. The maximum current of  K+ ions and Clâˆ’ ions in  the ACS case are increased by up to 112% and 256%, respectively, compared to the  ICS nanopore (Figure S5). The enhancement of current due to Clâˆ’ ions is responsible  for the decreased transference number of the ACS system shown in Figure 1. 15      Figure 3. Details of ion transport through a nanopore containing a charged ring at  both entrances. The width of the ring is denoted as Lcs. (a) Enhancement in ionic  current by the presence of a charged ring as a function of Lcs. Inset shows the  simulation scheme, in which dashed red lines indicate the surface charges. The  diameter and length of the nanopore were 10 and 50 nm, respectively. (b) Radial  concentration distributions of all ions in the center cross-section of the nanopore  under different Lcs. Inset shows the ion concentration at the center of the nanopore  under different Lcs. (c) Potential difference across the nanopore and both orifices of  the nanopore under different Lcs. V1 is the voltage between the entrance and the  working electrode. V2 is the voltage across the nanopore. V3 is the voltage between  the pore exit and the ground. (d) Radial distributions of the electric field strength in  the center cross-section of the nanopore under different Lcs.  In our simulation model, the nanopore is located at the center of the membrane  with a diameter of  5 Âµ m. Figures 1 and 2  provide evidence that exterior surface  charges significantly influence ionic transport through nanopores. As the next step,  16    we decided to determine the effective length of the charged area outside a pore,  considering a ring with surface charges of thickness, Lcs, placed at both pore  entrances which may be important to the design of nanopore arrays . To this end, we  conducted simulations of ion current as a function of Lcs 28, 30, 31, 41 that varied from 0  to 5000 nm. The two values of  Lcs=0 and Lcs=5000 nm correspond to the ICS and  ACS nanopores, respectively. As shown in Figure 3a, increasing Lcs from 0 to ~20  nm leads to an increase in ion current by ~114% from ~2.2 nA to ~4.7 nA. As Lcs  further increases, the total  current gradually saturates at the value of the ACS case.  The effective length of the charged ring region on the exterior surface is defined as  the value of Lcs_eff at which the total current reaches 95% of that in the ACS case. For  the nanopores considered in Figure 3a, the Lcs_eff is ~20 nm under 1 V in 0.1 M KCl.  Ionic current through the nanopore depends on  Lcs, which modulates ion  concentration and the electric field strength inside the pore. To understand how the  exterior surface charge s influence the ionic transport inside nanopores, the radial  distributions of ion concentration and electric field strength at the center cross-section  of the nanopore were investigated under different values of Lcs. We found that the ion  concentration distributions under different Lcs share a similar profile, Figure 3b. The  ion concentration in the region beyond the EDLs increases from 242 mM to 343 mM  with Lcs increasing from 0 to 20 nm. Note that the total resistance of the  nanopore  system includes the access resistance on both sides of the pore and pore resistance.  The applied voltage can  then be divided into t hree parts V1, V2, and V3 (Figure 3c).  Ionic distributions in Figure S6a  show that an increase in Lcs increases the ion  concentration at the nanopore entrance, due to the parallel transport of ions to the  charged exterior surface,  which weakens the ionic depletion phenomenon. The  increased ion concentration effectively reduces the access resistance, which in turn  induces a larger potential difference across the nanopore (V2). As shown in Figure 3c,  with Lcs increasing from 0 to 20 nm, the potential differences at the pore entrance (V1) 17    and across the nanopore (V2) decrease and increase by ~40% and ~20%,  respectively, which is also observed in the axial distributions of the electric potential  (Figure S6b). When Lcs reaches 20 nm, the electric field strength at the center cross - section of the nanopore is enhanced by ~86%, from 7 to 13 mV/nm compared to Lcs  = 0 nm, Figure 3d.    Figure 4. Characteristics of ionic transport through a nanopore under various Lcs on  both exterior membrane surfaces.  (a) Distributions of the total current in the plane  located at 1 nm above the membrane surface (A1-A2 and A1 ' -A2 ' ). (b) 2D  concentration distributions of K+ ions (color maps) and equipotential lines (white  lines). The inset shows the radial distribution of the electric potential at a distance of  0.5 nm from the exterior surface, i.e. the center of the EDLs. (c) Distributions of the  total current along the vertical plane with 1 nm in length on both sides of the  membrane (B1-B2 and B1 ' -B2 ' ). (d) Scheme of locations where current distributions  were obtained. A1-A2 and A1 ' -A2 '  represent the plane located at 1 nm above the  membrane surface. B1-B2 and B1 ' -B2 '  denote a surface with 1 nm in length along the  18    inner surface located at the pore entrance and exit. Arrows show the directions of the  ionic flux. Because the thickness of the EDLs is âˆ¼1 nm in 0.1 M KCl, the planes at 1  nm and 0.5 nm above the membrane surface were selected as the boundary and  center of EDLs.  We also looked in detail at ionic transport at the pore entrance and exit, where  we analyzed the distributions of ion current , see Figure 4. 28, 41 Note that the current  distributions were analyzed to show the contribution of external surface conductance  to the total  current inside the nanopore.  A1-A2 and  A1 ' -A2 '  represent two planes  located at 1 nm above the membrane, around  the boundary of the EDLs on both  sides of the nanopore . B1-B2 and B1 ' -B2 '  are toroidal surfaces with a length of 1 nm  along the inner pore wall, which are perpendicular to the membrane. The  distributions of ion current along the A1-B2-A2 and  A1 ' -B2 ' -A2 ' surfaces were obtained.  At the pore entrance, counterions can enter the pore directly by passing across A1-B2,  or accumulating in the EDL region near the charged exterior surface and then  migrating across B1-B2. Similar passageways for ions can be seen at the pore exit.  As shown in Figure 4a, when Lcs is 0 nm, i.e. the ICS case, counterions enter the  nanopore mainly through the surface A1-B2, and exit the pore at a similar position  ( A1 ' -B2 ' ). The current shows a single peak near the charged inner surface of the  nanopore, and the ion flux parallel to the exterior surfaces is almost zero.  When the  exterior surfaces are charged, the EDLs regions provide a fast passageway for ionic  transport which induce s an obvious surface current.  Abundant counterions pass  through the B2-A2 plane resulting in a distinct peak of ionic flux near the boundary of  the charged surface (Figure 4a). These counterions can enter the nanopore through  B1-B2 plane under the voltage drop along the exterior surface, which  leads to a  considerable ion flux as shown in Figure 4c.  In Figure 4b, the radial potential  difference at the pore entrance  is ~0.15 V which drags ions in the EDLs into the 19    nanopore rapidly. With Lcs increasing from 0 to 20 nm the radial potential difference  along the charged exterior surface gradually enhances , which can provide an  enhanced promotion in the ion flux. On the exit side, most ions leave the pore directly  through A1 ' -B2 '  leading to a large current peak . The surface charges on the exterior  surface can provide electrostatic attraction to counterions to confine their movement  inside the EDLs region . This amount of ions then enter the bulk through the  B2 ' -A2 ' .  Figure 4c  shows the ionic current through the B 1-B2, and  B1 ' -B2 '  planes, which  represents the ion flux entering and leaving the nanopore through the EDLs regions.  With the increase of Lcs, both ion fluxes through the B1-B2 and  B1 ' -B2 '  increase and  reach the maximum at Lcs ~20 nm. Since counter ions tend to migrate into the bulk  directly on the exit side, the ion flux through the  B1 ' -B2 '  is lower than that through the  B1-B2.   Figure S7 shows the contribution of current through different cross-sections to  the total current. When only the inner surface is charged, the ion flux passing through  the A1-B2 surface accounts for 60% of the total ion flux. After the exterior surfaces are  charged, the rapid transport of counterions along the exterior surface becomes to  dominate the total ionic flux.  When the Lcs reaches 20 nm, the ion current passing  through the B1-B2 plane accounts for 75% of the total current. In Figure S7 b, surface  charges on t he exterior surface have a relatively weak  influence on ionic transport,  which increases the proportion of ion flux along the exterior surface from ~48% to  ~53% of the total current. 20        Figure 5. Illustrations of the ionic transport under electric fields in the ICS (a) and  ACS (b) cases. Yellow and red thick arrows show the flux of cations in EDLs near  charged surfaces and in the pore center, respectively. Curved thin arrows show the  ionic flux at the orifices of the nanopore. Dashed red lines indicate the surface  charges.   Figure 5 shows the mechanisms of ion transport in ACS and ICS  cases under  electric fields. The total resistance includes the access resistance at both ends and  the pore resistance. Inside the pore, the conductance is composed of the surface  conductance near the charged inner wall and the bulk conductance in the pore center.  When the exterior surfaces are neutral, counterions mainly transport across the  boundary of the access resistance region through ionic diffusion, which then migrate  through the pore under electric fields. At the exit end, the counterions diffuse across  the boundary of the access resistance region into the bulk. Due to the weak ionic  supply by ionic diffusion and the fast ionic migration under applied voltages, ionic  21    depletion and enrichment appear at the pore entrance  and exit ends, respectively.   Consequently, a depletion zone is created which limits the transmembrane current.   We believe that in the ACS case, the EDLs near charged exterior surfaces of the  nanopore not only provide a high-concentration pool of the counterions but also  serve as a fast passageway for ionic transport.19, 30 Thus, in the ACS system, besides  the ionic diffusion near the access resistance region, a large number of counterions  can enter the nanopore through the migration in the EDLs along the exterior surfaces,  which weakens the ionic depletion at the pore entrance effectively. These ions then  pass across the pore through the pore center or the EDLs region near the inner  surface leading to current increase. On the exit side, the exterior surface charges  confine a portion of ionic transport in the EDLs regions through electrostatic  interaction. The charged exterior surfaces increase the area for ionic diffusion from  the pore to the bulk.   Following the concepts of surface conductance and bulk conductance inside  charged nanopores,30, 49 we propose exterior surface conductance (Ges) as the ionic  flux along charged membrane surface, and orifice bulk conductance ( Gob) as the  ionic migration at the pore entrance. The voltage drop at the pore entrance due to  access resistance facilitates sourcing the ions from the bulk, which leads to both  exterior surface conductance and orifice bulk conductance. 22      Figure 6. Effective length of the charged ring region on the exterior surface (Lcs_eff)  under various conditions, such as (a) the pore length, (b) the diameter, (c) the  surface charge density, (d) the voltage, (e) the salt concentration, and (f) the salt type.   From Figure 3a, with the charged region around the nanopore expanding, the  ionic current through the nanopore can be promoted. When the Lcs reaches a certain  threshold ( ~20 nm) , i.e. the effective Lcs, the ion current does not increase with  further enlarging of Lcs. The effective Lcs shows the required membrane dimension for  independent nanopores. It is  important in the propagation of ionic transport through  single nanopores to cases with  nanopore array s,28, 31 which can have many  applications in nanofluidic high-throughput detection, high -performance osmotic  energy conversion , and seawater desalination.  Here, the effective lengths of the  charged exterior surface under various conditions have been investigated. Through  23    varying the value of Lcs, when the ionic current reaches 95% of the current in the  ACS case, the corresponding Lcs is the effective value  (Lcs_eff) which represents the  required minimum charged surface area.  Figure 6 shows the required Lcs_eff as a function of pore length and diameter,  surface charge density, voltage, salt concentration , and salt type . As the nanopore  length increases, the magnitude of Lcs_eff decreases following the relationship of 1/L.  The increase in the pore length also decreases the electric field strength inside the  pore, which weakens the concentration polarization at both ends of the nanopore. As  the pore gets longer, due to the increased pore resistance, the voltage over the pore  mouth decreases, which also reduces the ion transport in the EDL regions near the  charged exterior surface. For long nanopores, the influence of the charged exterior  surface can be ignored (Figure S8a).  For 50-nm-long nanopores with a larger diameter, the access resistance takes a  larger portion of the total resistance.35 The resulting higher potential difference at the  pore mouth enhances the migration of counterions in the EDL region s near the  charged exterior surface. Lcs_eff increases as the diameter varies from 5 to 30 nm.  However, with the diameter further increasing to 50 nm, the orifice bulk conductance  gradually dominates the whole conductance, and the promotion of charged exterior  surface on ionic current weakens  (Figure S8 b). Also, because larger pore size  decreases the ionic selectivity of the nanopore, 13 the concentration polarization at  both ends becomes less significant. The Lcs_eff decreases with the pore diameter.   The surface charge density is  another important property of nanopores , which  determines the ionic selectivity of nanopores .13 In nanopore s with higher surface  charge densities, a higher ion flux due to the larger surface conductance depletes the  ion concentration at the pore entrance , such that a larger Lcs is required to sustain   the ion supply  through the exterior surface conductance. Under the surface charge 24    density varying from âˆ’0.04 to âˆ’0.16 C/m2, the Lcs_eff increases from ~5 to ~40 nm.   (Figure 6c).   The influence of applied voltage on the Lcs_eff is similar to that of the surface  charge density. Under electric fields, ionic migration provides the main contribution to  the total current through a nanopore. As the applied voltage increases, ion ic  migration enhances inside the nanopore, especially in the EDLs regions near the  charged pore wall, which deplet es counterions at the pore entrance. As a result, a  larger Lcs_eff is required to maintain the  large ionic transport. From Figure 6d, the  Lcs_eff increases linearly with the applied voltage.  For aqueous solutions used in the work, two properties were considered, i.e. the  concentration and salt type. In KCl solutions, the surface charges can be screened  more efficiently  under a higher  salt concentration,12 which weakens the surface  conductance across the nanopore. In this case, the bulk conductance dominates the  total ionic flux , resulting in a  shorter Lcs_eff. From Figure 6e, Lcs_eff is inversely  proportional to the solution concentration. The  thickness of the EDLs i.e. the Debye  length ï¬ is inversely proportional  to the square roo t of the solution concentration   (Table S2) . On the logarithmic scale, the Lcs_eff and ï¬ satisfy a linear relationship  (Figure S9 ). Finally, we looked into the dependence of Lcs_eff on the type of salt,  which necessitated considering different values of diffusion coefficients.  Here, LiCl,  NaCl, KCl, and KF are used to probe the effect of different counterions and coions on  Lcs_eff. The ion diffusion coefficients follow the order of Cl âˆ’>K+> F âˆ’>Na+>Li+.38 As  shown in Figure S8 f, the differences in currents obtained  in ACS and ICS pores in  the four salt solutions follow a similar trend, i.e. they first increase and then remain   constant as Lcs increases. Figure 6f shows that the Lcs_eff remains almost constant in  salt solutions with different diffusion coefficients.  4. Conclusions 25    Systematical simulations have been  conducted to investigate  the influence of  charged exterior surfaces on ionic transport through nanopores.  We show that the  EDLs near the charged membrane surfaces provide an ion pool and an additional  passageway for counterions, which can migrate along the EDLs into the nanopore  under the potential difference parallel to the exterior surface. This exterior surface  conductance weakens the ion dissipation at the pore entrance but enhances the ionic  enrichment inside the nanopore . Compared with th e ICS case, charged exterior  surfaces improve the ion concentration in the central a nd EDLs regions at the center  cross-section of  the nanopore by ~70% and ~5%, respectively. The electric field  strength at the center cross-section is also increased by ~100%. The combination of  the increase in ion concentration and the enhancement in ionic migration rate results  in a 125% increase in the ion current in the ACS case than that in the ICS case. With  the adjustment of the charged membrane area, the effective Lcs is found ~20 nm. In  addition, quantitative relationships between the e ffective Lcs and the nanopore  parameters and applied conditions are explored. The e ffective Lcs has a reciprocal  relationship with the pore length and salt concentration but is linearly proportional to  the surface charge density and voltage . Effective Lcs also increases with the pore  diameter squared  and remains independent of the ionic species.  We think the  elucidated modulation mechanism of charged exterior surfaces on the ionic transport  through short nanopores can provide theoretical guidance for the design and  preparation of porous membranes in various applications.  Author Contributions  Long Ma:  Methodology ( equal); Formal analysis  (lead); Software (lead);  Validation (lead); Writing â€“ Original D raft (lead) ; Writing â€“ Review & E diting  (equal). Zhe L iu: Software (equal); Data Curation (equal); Visualization. Jia Man:   Data C uration (equal). Jianyong Li : Investigation (equal) . Zuzanna S. Siwy :  Supervision (equal) ; Writing â€“ Review & E diting ( equal). Yinghua Qiu : 26    Conceptualization (lead); Resources (lead); Supervision (lead); Methodology (lead);  Investigation (lead); Writing â€“ Original Draft (equal); Writing â€“ Review & Editing (lead);  Funding Acquisition (lead).  Conflict of Interest  There are no conflicts to declare.  Acknowledgment  This research was supported by the National Natural Science Foundation of  China (52105579), the Natural Science Foundation of Shandong Province  (ZR2020QE188), the Guangdong Basic and Applied Basic Research Foundation  (2023A1515012931), the Natural Science Foundation of Jian gsu Province  (BK20200234), and the Qilu Talented Young Scholar Program of Shandong  University.   References  1 Y. Qiu, P. Hinkle, C. Yang, H. E. Bakker, M. Schiel, H. Wang, D. Melnikov, M. Gracheva, M.  E. Toimil-Molares, A. Imhof and Z. S. Siwy, ACS Nano, 2015, 9, 4390-4397.  2 G. Danda and M. DrndiÄ‡, Curr. Opin. Biotechnol., 2019, 55, 124-133.  3 L. Xue, H. Yamazaki, R. Ren, M. Wanunu, A. P. Ivanov and J. B. Edel, Nat. Rev. Mater. ,  2020, 5, 931-951.  4 Y. Zhang, G. Wu, W. Si, J. Ma, Z. Yuan, X. Xie, L. Liu, J. Sha, D. Li and Y. Chen,  Nanoscale, 2017, 9, 930-939.  5 R. A. Lucas, C.-Y. Lin, L. A. Baker and Z. S. Siwy, Nat. Commun., 2020, 11, 1568.  6 S. P. Surwade, S. N. Smirnov, I. V. Vlassiouk, R. R. Uno cic, G. M. Veith, S. Dai and S. M.  Mahurin, Nat. Nanotechnol., 2015, 10, 459-464.  7 Y. You, V. Sahajwalla, M. Yoshimura and R. K. Joshi, Nanoscale, 2016, 8, 117-119.  8 Z. Zhang, L. Wen and L. Jiang, Nat. Rev. Mater., 2021, 6, 622-639.  9 Z. Hao, Q. Zhang, X. Xu, Q. Zhao, C. Wu, J. Liu and H. Wang, Nanoscale, 2020, 12, 15923- 15943.  10 J. Hwang, T. Sekimoto, W. -L. Hsu, S. Kataoka, A. Endo and H. Daiguji, Nanoscale, 2017,  9, 12068-12076.  11 R. B. Schoch, J. Y. Han and P. Renaud, Rev. Mod. Phys., 2008, 80, 839-883.  12 J. N. Israelachvili, Intermolecular and Surface Forces , Academic Press, Burlington, MA,  3rd edn., 2011.  13 I. Vlassiouk, S. Smirnov and Z. Siwy, Nano Lett., 2008, 8, 1978-1985.  14 Y. Qiu and L. Ma, Phys. Fluids, 2022, 34, 112010.  15 S. J. Kim, Y.-A. Song and J. Han, Chem. Soc. Rev., 2010, 39, 912-922.  16 M. Gholinejad, A. Jabari Moghadam, D. -T. Phan, A. K. Miri and S. A. Mousavi Shaegh,  Phys. Fluids, 2021, 33, 051301.  17 S. J. Kim, S. H. Ko, K. H. Kang and J. Han, Nat. Nanotechnol., 2010, 5, 297-301.  18 S. J. Kim, Y.-C. Wang, J. H. Lee, H. Jang and J. Han, Phys. Rev. Lett., 2007, 99, 044501.  19 L. Ma, Z. Li, Z. Yuan, C. Huang, Z. S. Siwy and Y. Qiu, Anal. Chem., 2020, 92, 16188- 16196. 27    20 L.-H. Yeh, M. Zhang and S. Qian, Anal. Chem., 2013, 85, 7527-7534.  21 W.-L. Hsu, D. J. E. Harvie, M. R. Davidson, H. Jeong, E. M. Goldys and D. W. Inglis, Lab  Chip, 2014, 14, 3539-3549.  22 J. Liu, M. Kvetny, J. Feng, D. Wang, B. Wu, W. Brown and G. Wang, Langmuir, 2012, 28,  1588-1595.  23 D. Stein, M. Kruithof and C. Dekker, Phys. Rev. Lett., 2004, 93, 035901.  24 R. B. Schoch and P. Renaud, Appl. Phys. Lett., 2005, 86, 253111.  25 M. Tagliazucchi, Y. Rabin and I. Szleifer, ACS Nano, 2013, 7, 9085-9097.  26 Y. Ma, J. Guo, L. Jia and Y. Xie, ACS Sens, 2018, 3, 167-173.  27 C.-Y. Lin, J.-P. Hsu and L.-H. Yeh, Sens. Actuators, B, 2018, 258, 1223-1229.  28 L. Ma, Z. Li, Z. Yuan, H. Wang, C. Huang and Y. Qiu, J. Power Sources , 2021, 492,  229637.  29 Y. Qiu and Y. Chen, Mol. Simul., 2015, 41, 1187-1192.  30 L. Ma, X. An, F. Song and Y. Qiu, J. Phys. Chem. Lett., 2022, 13, 5669-5676.  31 K. Yazda, K. Bleau, Y. Zhang, X. Capaldi, T. St -Denis, P. Grutter and W. W. Reisner,  Nano Lett., 2021, 21, 4152-4159.  32 Y. Qiu, I. Vlassiouk, Y. Chen and Z. S. Siwy, Anal. Chem., 2016, 88, 4917-4925.  33 S. Dutt, B. I. Karawdeniya, Y. M. N. D. Y. Bandara, N. Afrin and P. Kluth, Anal. Chem.,  2023, 95, 5754-5763.  34 X. Zhu, J. Hao, B. Bao, Y. Zhou, H. Zhang, J. Pang, Z. Jiang and L. Jiang, Sci. Adv., 2018,  4, eaau1665.  35 Y. Qiu, Z. S. Siwy and M. Wanunu, Anal. Chem., 2019, 91, 996-1004.  36 M. Wanunu, W. Morrison, Y. Rabin, A. Y. Grosberg and A. Meller, Nat. Nanotechnol., 2010,  5, 160-165.  37 K. Chen, I. Jou, N. Ermann, M. Muthukumar, U. F. Keyser and N. A. W. Bell, Nat. Phys.,  2021, 17, 1043-1049.  38 E. L. Cussler, Diffusion: Mass Transfer in Fluid Systems , Cambridge University Press,  Cambridge, 2009.  39 Y. Qiu, C. Yang, P. Hinkle, I. V. Vlassiouk and Z. S. Siwy, Anal. Chem., 2015, 87, 8517- 8523.  40 L. Cao, F. Xiao, Y. Feng, W. Zhu, W. Geng, J. Yang, X. Zhang, N. Li, W. Guo and L. Jiang,  Adv. Funct. Mater., 2017, 27, 1604302.  41 L. Ma, K. Lin, Y. Qiu, J. Zhuang, X. An, Z. Yuan and C. Huang, J. Phys. Chem. C , 2021,  125, 14195-14203.  42 B. N. Miles, A. P. Ivanov, K. A. W ilson, F. DoÄŸan, D. Japrung and J. B. Edel, Chem. Soc.  Rev., 2013, 42, 15-28.  43 R. Wei, D. Pedone, A. ZÃ¼ rner, M. DÃ¶ blinger and U. Rant, Small, 2010, 6, 1406-1414.  44 M. Graf, M. Lihter, D. Altus, S. Marion and A. Radenovic, Nano Lett., 2019, 19, 9075-9083.  45 A. Darvish, G. Goyal, R. Aneja, R. V. K. Sundaram, K. Lee, C. W. Ahn, K. -B. Kim, P. M.  Vlahovska and M. J. Kim, Nanoscale, 2016, 8, 14420-14431.  46 N. Arjmandi, W. Van Roy and L. Lagae, Anal. Chem., 2014, 86, 4637-4641.  47 C. Wen, S. Zeng, Z. Zhang and S.-L. Zhang, Anal. Chem., 2018, 90, 13483-13490.  48 M. Tsutsui, K. Yokota, T. Nakada, A. Arima, W. Tonomura, M. Taniguchi, T. Washio and T.  Kawai, ACS Sens., 2018, 3, 2693-2701.  49 C. Lee, L. Joly, A. Siria, A. -L. Biance, R. Fulcrand and L. Bocquet, Nano Lett., 2012, 12,  4037-4044.",
      "references": [
        "Intermolecular and Surface Forces",
        "Diffusion: Mass Transfer in Fluid Systems"
      ],
      "meta_data": {
        "arxiv_id": "2402.04945v1",
        "doi": "10.1039/D3NR04467J",
        "authors": [
          "Long Ma",
          "Zhe Liu",
          "Jia Man",
          "Jianyong Li",
          "Zuzanna S. Siwy",
          "Yinghua Qiu"
        ],
        "published_date": "2024-02-07T15:25:20Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Investigates how surface charges located on the *exterior membrane surfaces* (outside the pore) modulate ionic transport through *short* nanopores (sub-200 nm). Key findings: (1) Exterior charges can boost conductance by increasing both (i) charge-carrier concentration at pore mouths/inside pore and (ii) carrier migration speed via stronger electric fields; (2) exterior electric double layers (EDLs) act as an ion reservoir (â€œion poolâ€) and provide an additional tangential transport pathway (â€œexterior surface conductanceâ€), mitigating entrance depletion and suppressing limiting-current behavior, yielding near-linear Iâ€“V; (3) exterior charges increase the electric field inside the pore (~2Ã— at the center cross-section under tested conditions); (4) defines an effective charged-ring width at pore entrances (Lcs_eff) needed to achieve ~95% of the fully charged-exterior (ACS) current, and quantifies scaling laws: Lcs_eff increases linearly with surface charge density and voltage, decreases inversely with pore length and salt concentration, shows non-monotonic/geometry-dependent behavior with diameter, and is largely insensitive to ionic species in tested salts.",
        "methodology": "Continuum multiphysics simulations in COMSOL using coupled Poissonâ€“Nernstâ€“Planck (PNP) for electrostatics and ion transport plus Navierâ€“Stokes (NS) for fluid flow (electroosmotic flow). Compares boundary-condition scenarios: NCS (all neutral), ICS (only inner-pore wall charged), ACS (inner + exterior membrane surfaces charged), and intermediate cases with a charged exterior ring of width Lcs at both pore mouths. Analyzes axial/radial ion concentration, electric field, and decomposes current into bulk vs near-wall (EDL) â€˜surfaceâ€™ contributions (within 1 nm of wall). Uses access resistance + pore resistance models to interpret Iâ€“V, including conductance expressions incorporating Dukhin length for charged cases.",
        "experimental_setup": "Simulation domain: axisymmetric nanopore connecting two cylindrical reservoirs (radius 5 Âµm, length 5 Âµm) separated by a membrane. Pore geometry: diameter 4â€“55 nm, length 20â€“1000 nm (focus on short pores; default d=10 nm, L=50 nm). Electrolyte: aqueous KCl, 10â€“500 mM (default 100 mM), T=298 K, water dielectric=80; ion diffusivities DK+=1.96eâˆ’9 and DClâˆ’=2.03eâˆ’9 mÂ²/s. Voltage: 0â€“1 V (positive), default analysis at 1 V. Surface charge density: âˆ’0.04 to âˆ’0.16 C/mÂ² (default âˆ’0.08 C/mÂ²). Mesh: refined near charged surfaces (0.1 nm) to resolve EDL; convergence checked via current. Metrics/validation: Iâ€“V curves compared to classical access+pore resistance model for neutral pores and to a charged-pore conductance model (with Dukhin length) for ACS; Lcs_eff defined where current reaches 95% of ACS current. Additional salt-type test: LiCl, NaCl, KCl, KF (varying diffusion coefficients) to assess species dependence of Lcs_eff.",
        "limitations": "Relies on continuum PNPâ€“NS framework: neglects molecular-scale effects (finite ion size, hydration/steric effects, ionâ€“ion correlations, dielectric decrement, surface roughness), which can be important for very small pores (~few nm) and high fields. Assumes fixed, uniform surface charge densities and idealized geometries; real membranes may have heterogeneous charge regulation, chemical specificity, and roughness. Uses symmetric reservoirs and applies only positive voltages; rectification under asymmetric conditions not explored here. Surface conductance partition uses a fixed 1 nm cutoff tied to Debye length at 0.1 M; may not generalize across concentrations without adjustment. No experimental validation presented; findings are simulation-based and depend on chosen boundary conditions and material parameters (e.g., dielectric constant, viscosity).",
        "future_research_directions": "(1) Experimental verification with fabricated short nanopores/thin membranes whose exterior charge can be independently tuned; measure Iâ€“V, ICP signatures, and compare inferred Lcs_eff. (2) Extend to charge-regulating surfaces and pH-dependent chemistry; include heterogeneous/patchy exterior functionalization. (3) Incorporate beyond-PNP physics (steric PNP, modified Poissonâ€“Boltzmann, ion correlations, dielectric decrement) and/or MDâ€“continuum coupling to assess robustness for sub-10 nm pores and high voltages. (4) Study asymmetric geometries and bipolar exterior/inner patterns for rectification and diode-like behavior; explore negative voltages and time-dependent driving. (5) Translate Lcs_eff concept to interacting nanopore arrays: quantify poreâ€“pore coupling via overlapping exterior EDL pathways, optimize spacing for throughput in desalination/energy conversion. (6) Explore multivalent electrolytes, mixtures, and surface-slip effects on exterior surface conductance and EOF; assess implications for biomolecule sensing (noise, capture rate) and selectivity.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling",
      "full_text": "111 A Disruptive Research Playbook for Studying Disruptive Innovations MARGARET-ANNE STOREY,Department of Computer Science, University of Victoria, Canada DANIEL RUSSOâˆ—, Department of Computer Science, Aalborg University, Denmark NICOLE NOVIELLI, Department of Computer Science, University of Bari, Italy TAKASHI KOBAYASHI,Department of Computer Science, Tokyo Institute of Technology, Japan DONG WANG,Department of Information Science and Technology, Kyushu University, Japan As researchers, we are now witnessing a fundamental change in our technologically-enabled world due to the advent and diffusion of highly disruptive technologies such as generative AI, Augmented Reality (AR) and Virtual Reality (VR). In particular, software engineering has been profoundly affected by the transformative power of disruptive innovations for decades, with a significant impact of technical advancements on social dynamics due to its the socio-technical nature. In this paper, we reflect on the importance of formulating and addressing research in software engineering through a socio-technical lens, thus ensuring a holistic understanding of the complex phenomena in this field. We propose a research playbook with the goal of providing a guide to formulate compelling and socially relevant research questions and to identify the appropriate research strategies for empirical investigations, with an eye on the long-term implications of technologies or their use. We showcase how to apply the research playbook. Firstly, we show how it can be used retrospectively to reflect on a prior disruptive technology, Stack Overflow, and its impact on software development. Secondly, we show it can be used to question the impact of two current disruptive technologies: AI and AR/VR. Finally, we introduce a specialized GPT model to support the researcher in framing future investigations. We conclude by discussing the broader implications of adopting the playbook for both researchers and practitioners in software engineering and beyond. CCS Concepts: â€¢ Software and its engineering ; â€¢ Human-centered computing â†’Collaborative and social computing; Computer supported cooperative work ; â€¢ Computing methodologies â†’Artificial intelligence; Additional Key Words and Phrases: keywords here ACM Reference Format: Margaret-Anne Storey, Daniel Russo, Nicole Novielli, Takashi Kobayashi, and Dong Wang. 2024. A Disruptive Research Playbook for Studying Disruptive Innovations. ACM Trans. Softw. Eng. Methodol. 37, 4, Article 111 (February 2024), 28 pages. https://doi.org/10.1145/1122445.1122456 âˆ—Corresponding author. Authorsâ€™ addresses: Margaret-Anne Storey, mstorey@uvic.ca, Department of Computer Science, University of Victoria, P. O. Box 3055, STN CSC, Victoria BC, Canada; Daniel Russo, daniel.russo@cs.aau.dk, Department of Computer Science, Aalborg University, A.C. Meyers Vaenge 15, 2450, Copenhagen, Denmark; Nicole Novielli, nicole.novielli@uniba.it, Department of Computer Science, University of Bari, Via Orabona, 4, Bari, Italy; Takashi Kobayashi, tkobaya@c.titech.ac.jp, Department of Computer Science, Tokyo Institute of Technology, 2-12-1 Ookayama, Meguro-ku, 152-8552, Tokyo, Japan; Dong Wang, p.hanel@essex.ac.uk, Department of Information Science and Technology, Kyushu University, 744 Motooka, Nishi-ku, 819-0395, Fukuoka, Japan. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Â© 2024 Association for Computing Machinery. 1049-331X/2024/02-ART111 $15.00 https://doi.org/10.1145/1122445.1122456 ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024. arXiv:2402.13329v1  [cs.SE]  20 Feb 2024111:2 Storey et al., 2024 1 INTRODUCTION â€œDisruptive technologies arenâ€™t disruptive by themselves, it is how people use them that makes them disruptiveâ€ â€”Anonymous As software engineering researchers, our passion lies in understanding and developing theories about software engineering and crafting software engineering solutions. Yet, in our pursuit of scientific and technological excellence, we occasionally lose sight of the broader purpose: the developers, the end users of software, and the societal impact our work is meant to serve. This oversight is not just a personal lapse but a professional one, as our role as scientists demands that we create value for society at large. It is easy to fall into the trap of assuming that the research questions that fascinate us will inherently fascinate and serve others. However, this assumption can often lead us astray. Recognizing this crucial gap, we have crafted a provocative playbook, offering strategic guidance to those among us who struggle to identify compelling and socially relevant research questions in software engineering. The playbook, we hope, will provoke researchers to pause and to take a step back to reflect on what other questions they could be answering through their research, and what the impact of those answers may be on society at large. We propose a playbook, rather than a framework, as it is a more actionable approach to broadening the research questions we ask. We also introduce â€œMyResearchPlaybookâ€, a specialized GPT model, as an adjunct to our research playbook, to help in the brainstorming activity the playbook invokes. We justify the need for such a playbook, by tracing the arc of human history. This arc reveals a continuum of technological innovation, from the earliest tools enhancing physical capabilities, to the sophisticated digital systems of today that amplify our intellectual prowess. This relentless pursuit of advancement has been a hallmark of human civilization [54]. Each technological breakthrough has brought us forward as a societyâ€“from primitive hunting tools to the development of language, writing systems, and beyondâ€“and has fundamentally altered how we work, communicate, and create. These leaps not only showcase our ingenuity but also challenge us to reflect on the role and impact of our creations within the larger societal context. Marshall McLuhan, a visionary scholar active in the 1960s, astutely observed that disruptive technologies follow intrinsic laws: they amplify certain human abilities, render previous technologies obsolete, and when pushed to their extremes, create a need for new innovations to address emergent challenges [49]. McLuhanâ€™s insights compel us to reflect not only on the immediate benefits of new technologies, but also to anticipate their long-term and potentially unforeseen impacts. Consider the automobile: while it has revolutionized transportation, its excessive use has led to traffic congestion and significant environmental degradation, necessitating further innovation and intervention. The realm of software engineering has not been immune to the transformative power of disruptive innovations since it is fundamentally socio-technical, intertwining technical advancements and social dynamics. Despite rapid progress through innovations like high-level programming languages and social coding tools, the integration of tools such as static analysis and automated debugging remains a challenge. Parnin and Orsoâ€™s study highlighted that the efficacy of these tools is contingent not only on their technical capabilities, but also (and maybe more importantly) on their adoption and use by developers in real-world scenarios [60]. This underscores the necessity for a holistic approach in software engineering, ensuring that technical innovations are seamlessly aligned with and supportive of the human elements of software development. Generative AI technology, along with Augmented Reality (AR) and Virtual Reality (VR), exempli- fies the current wave of disruptive technologies showing the potential to revolutionize the field ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.A Disruptive Research Playbook for Studying Disruptive Innovations 111:3 of software engineering. These innovations are not merely incremental improvements; they are catalysts for fundamental change, challenging existing paradigms, and creating new opportunities for advancement and understanding. The integration of AI is transforming various aspects of software engineering, automating routine tasks, and injecting creativity into software design and development, while simultaneously lowering the barrier to software authorship. Concurrently, AR and VR are emerging as potential disruptors, with technologies such as the Metaverse [ 88] anticipated to unlock opportunities that enhance e.g., our understanding of complex dependencies. These technologies also play a crucial role in improving collaborative efforts during development and bolstering educational support, as highlighted by Krause [42] and Fernandes [31]. While AI, AR, and VR are the disruptors of today, shaping the current and future landscape of software engineering, it is important to note that they are examples in a long line of innovations. To navigate the ever-evolving landscape of software engineering in our research, we can not overlook the long-term implications of technologies or their use. Thus, the research playbook we propose was intentionally designed to be provocative, versatile, and technology-agnostic. This ensures that it retains its relevance and applicability, providing a reliable guide through the ongoing waves of innovation and disruption in software engineering. Given the sweeping changes brought about by disruptive technologies, it is imperative for research within this domain to adopt a socio-technical lens, as the lesson of Parnin and Orso shows us, ensuring a holistic understanding that encompasses both the technological innovations and the social contexts in which they operate. This approach goes beyond merely selecting the right research methodologies or adhering to specific methods, as guided by established resources in software engineering [67, 86]. While these guides are invaluable for ensuring methodological rigor, they fall short in assisting researchers to formulate critical questions that delve into the broader and long-term impacts on both technical and socio-technical phenomena in the field. Here, we underscore the importance of integrating a socio-technical perspective in all studies within this domain. This integration is crucial not only for harnessing the full potential of disruptive technologies, but also for mitigating potential negative impacts, and fostering a sustainable and inclusive technological future. The emphasis here is on encouraging researchers to adopt a holistic perspective that encompasses not just research methods, but also a critical examination of the underlying questions asked and the wider phenomena impacted by disruptive technologies in soft- ware engineering. This comprehensive approach ensures a balanced and thorough understanding of the subject matter and its socio-technical implications. Our paper begins by defining what constitutes a â€œdisruptive technologyâ€, followed by an overview of key disruptive technologies that have significantly impacted software engineering (Section 2). Subsequently, our research playbook is presented in detail (Section 3), serving as a tool to evaluate innovative technologies in software engineering. As we present the playbook, we illustrate how the playbook can be used to retrospectively reflect on a prior disruptive technology, Stack Overflow, and its impact on software engineering. This retrospective application of the playbook revealed to us how relatively little research emphasis there has been on studying social and human aspects. To demonstrate practical forward looking applications of the research playbook, we explore two specific disruptive technologies in software engineering that are still emerging: AI and AR/VR. Specific example scenarios are provided to illustrate how the playbook can be employed within these domains (Section 4). In Section 4 we also introduce â€œMyResearchPlaybookâ€, a specialized GPT model to assist in the brainstorming activity the playbook invokes for the second scenario. We conclude the paper by discussing the broader implications of adopting the playbook, highlighting its potential benefits for both researchers and practitioners (Section 5). ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.111:4 Storey et al., 2024 2 DISRUPTIVE SOFTWARE ENGINEERING TECHNOLOGIES DISRUPT RESEARCH Frequent instances of technological leaps and creative breakthroughs have a history of shattering existing stable environments, elevating performance across various dimensions, and profoundly re- shaping entire societies. These profound shifts are termed â€œdisruptive technologiesâ€ by Christensen et al. [15] This concept has been expanded to encompass innovations not only in technology, but also in products, processes, and business models, known as â€œdisruptive innovationâ€ [23]. Christensenâ€™s original definition of disruptive innovation, which focuses on the competitive dy- namics in economic activity, contains two meanings. The first is the disruption of economic market structures, and the second refers to the disruption experienced by incumbent firms. Different from sustainable innovations, which operate changes in the market that are predictable by introducing incremental changes in existing business methods, disruptive innovations introduce significant changes in the habits of customers, thus leading to changes in processes and business models that are not predictable in their emergence and evolution. Usually, such disruptive innovations emerge from a group of customers whose needs appear not fully addressed by mainstream solutions. The quality of the innovative product or service then improves over time, thus reaching the mainstream market and introducing major changes in the daily habits of people. Christensen further observed that normally successful and stable businesses fail not at the introduction of sustainable innovations (they are well positioned to stay on top of these changes technology and market wise), but fail in the face of disruptive innovations which often lead to cheaper new products and shifts in new emerging markets [15]. Established companies may be slow to recognize shifts in market needs, may face inertia to change and fail to recognize the potential of the new innovations, and consequently stick with formerly effective processes and tools that are no longer successful in the face of disruptive change. The Internet can be seen as an example of a disruptive technology. Originally designed to address the needs of research and military institutions for robust networking technology for collaboration purposes [46], the Internet evolved over time and eventually had a profound impact on individual and collective aspects of our society. It dramatically reshaped the way we communicate with each other, while enhancing our ability to access and share information, and enabling the creation of new business models. Companies that embraced the internet (e.g., the case of eCommerce), were more successful than those that did not. This conceptualization of disruptive innovation, originally conceived to model market processes, also holds true for the software engineering domain. In software engineering, instances of such transformative breakthroughs are frequent. For instance, the introduction of Integrated Develop- ment Environments (IDEs) revolutionized the interaction between developer and development support tools. They reduced the friction of using different tools, and they standardized processes and tools across teams. IDE plug-ins replaced the need for standalone tools like Computer-Aided Software Engineering (CASE) tools, which enabled simplified environment setup and tool integra- tion. Social knowledge-sharing platforms like Stack Overflow disrupted the barrier of collaboration and drastically changed the use of traditional documentation, such as FAQs and manuals, and promoted collaborative learning and problem solving. In the contemporary context, the advances achieved by large-scale language modeling (LLM) and virtual reality/augmented reality (AR/VR) through deep learning, and dramatic advances in computing power are undeniably remarkable. To further enhance the disruptive potential of such technologies, we note that they are accessible to a wider audience compared to technologies introduced in the past, whose benefits were thanks to interaction paradigms such as the use of natural language for LLM-based tools (e.g., Copilot). ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.A Disruptive Research Playbook for Studying Disruptive Innovations 111:5 Fig. 1. The McLuhanâ€™s Tetrad showcasing the four laws that every new medium follows. We find ourselves in a dynamic phase of disruptive innovation that has the potential to reshape how we interact with technology and experience reality. Just as formerly successful businesses may fail in the face of disruptive rather than sustainable innovations, researchers also need to consider if they too may suffer from inertia and too much faith in existing approaches for doing research when faced with disruptive innovations. Disruptive innovations change not only the market, but also the value propositions that need to be understood. In response to this observation, our paper offers a comprehensive and action inspiring research playbook that encourages researchers to reflect on the socio-technical phenomena affected by disruptions, such as AI and AR/VR adoption, and the essence of the research questions they should be asking (based on the ideas of how these phenomena may be impacted). We further leverage the disruption of generative AI on research itself by designing a specialized GPT model, called â€œMyResearchPlaybookâ€. That is, not only do we anticipate that generative AI is disrupting software engineering, but it may also directly disrupt how we do research and disrupt the very questions that we pose in our work. But first, we present the design of the playbook next. 3 THE DESIGN OF A DISRUPTIVE RESEARCH PLAYBOOK We designed and developed a research playbook to guide and assist in the evaluation and prediction of how disruptive innovations may impact software engineering. Frameworks to guide software engineering research have been proposed before [27, 75, 86], but these focused on selecting and using research methods with little attention to the importance of selecting a pertinent research question. Our playbook, in contrast, emphasizes a step by step process, starting with the selection of research problems to be considered, followed by which socio-technical phenomena may be affected by the introduction of a new technology, and which ideas should be considered. That is, it emphasizes the construction and selection of research questions. Lastly, the playbook guides a reflection on the selection of research strategies to use while studying the potential disruptive impacts of a new technology, with a focus on human and social aspects. The playbook is inspired by two research frameworks, discussed below. Throughout, we use a former disruptive technology in software engineering, Stack Overflow, which disrupted how developers share, curate, and learn knowledge, to illustrate how the playbook can be used not just to question the impact of new technologies, but also to reflect on the impact and research that has been done on prior disruptive technologies. ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.111:6 Storey et al., 2024 Table 1. Disruptive Technologies in Software Development: What they enhanced, what they made obsolete, what they retrieved from prior technologies, and what they flipped into over time. Disruptive TechnologyDate Emerged Machine languages1960â€™s High-levelProg Langs, OO, Unix1970â€™s Internet (Repositories, Libraries,Email, ICQ+Bots,Open Source)1980â€™s IDEs,Automated Testing1990â€™s Social media,Stack Overflow,GitHub, CI, DevOps2000â€™s Enhancement Programming,Problem solving Abstractions,Design compositionPseudocode Distributed dev (merging, reuse),Communication Agile,quality, reliability,repeatability Community building,Community knowledge sharing,Faster deployments Obsolescence Manual Computations,Natural language,Paper Knowledge ofmachineprogramming Tarballs and diffs,Lost source code,Duplicate efforts Custom tools,Waterfall processes,Business Analysts,Testers Manuals (technical writers),Systems admins, Emails Retrieval Mathematics Natural language designIntellectual Property,Communities of PracticeStandardizedenvironments/process Social developers,Collaborative problem solving,knowledge sharing Reverse/Flip High levelprogramminglanguages, OS control Collaborative devtools, open source IDEs,Automated testing Social coding,automated processesArtificial Development(behaviour/social data driven) 3.1 Framing the Research Landscape with McLuhanâ€™s Tetradic Dimensions When conducting research, the importance of articulating and crafting oneâ€™s guiding research goals and questions can never be overstated [12, 27]. At the heart of any empirical research study or evaluation is the phenomenon or phenomena to be studied and the ideas we have about them, while the research questions become the navigational compass points that are considered in the research study. Marshall McLuhan recognized some decades ago the importance of choosing probing questions to ask about new media [49] and their impact on humans and society. By media, McLuhan refers to any technology that augments or enhances a human or their capabilities. For example, he considered a car, a poem, a pen, and clothing all as forms of media. 1 McLuhan recognized from his studies of media that any technology, once it is put to use, exhibits the same four laws [50]. These state that a new disruptive innovation will enhance or accelerate some aspect of a humanâ€™s capabilities, it will retrieve some characteristics or affordances from older innovations, it will make obsolete some existing innovation, and lastly when overused or used to an extreme over time it will reverse the very capabilities it strove to provide and eventually flip into a newer technology. Predicting what will come next is not trivial, and McLuhan also noted that understanding what a technology retrieves requires a deep historical understanding of the technologies that came before it [50]. McLuhan illustrated these four laws using a tetrad (see Figure 1), a visualization to help a researcher understand or even predict the impact a new medium may have. This tetrad is often referred to by media scholars as McLuhanâ€™s tetrad , and it has been commonly used in recent years to evaluate technological innovations [79]. The tetrad poses four questions to evaluate a new technology: (1) What does the technology enhance or amplify? (2) What does the technology make obsolete? (3) What does the technology retrieve that had been obsolesced earlier? (4) What does the technology reverse or flip into when pushed to extremes or overused? Table 1 shows representative examples of past disruptive innovations in software engineering and how McLuhanâ€™s tetrad can offer insights on what they enhanced, what they made obsolete, what they retrieved from the past, and what they flipped into over time. 1Although McLuhan used the term media more than technology, we use the more familiar term of technology throughout our paper. ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.A Disruptive Research Playbook for Studying Disruptive Innovations 111:7 Fig. 2. McLuhanâ€™s Tetrad applied to Stack Overflow as one example of a disruptive innovation in SE. For each law, we reflect on how Stack Overflow has had an impact on software developers and their practices. Impacts noted with an asterisk were suggested by ChatGPT 4 on Oct 16, 2023. McLuhanâ€™s tetrad is used as the first step in our playbook for evaluating new technologies in order to reflect on the impact of prior disruptive technologies (such as Stack Overflow). Figure 2 shows how McLuhanâ€™s tetrad can be used to reflect on the impact that Stack Overflow has had in software engineering. Researchers have studied how Stack Overflow has enhanced (sped up) the task of finding answers to technical questions during coding and debugging tasks [ 7, 19, 45, 70, 84], how it has enhanced how developers across the world â€œcollaborateâ€ by crowdsourcing this knowledge [6, 9, 14, 35, 47, 81], and how it has been effectively leveraged for retrieving technical information [20] and supporting learning [21, 77]. Researchers have also investigated how Stack Overflow has made other technologies (such as email) obsolete or less important [73], and how it has retrieved characteristics from prior technologies, such as the support of mentors [33]. What Stack Overflow is reversing into is, however, an important and ongoing topic for research. While code snippets from Stack Overflow answers might be seen as a crucial part of developersâ€™ knowledge, their effective reuse is far from trivial [83, 87]. Some researchers have concerns that developers use code snippets from Stack Overflow without fully understanding how they work, potentially lowering code quality [25, 51, 66, 89], introducing bugs [2] and security risks [3, 32]. There is also a growing concern that the quality of answers is in general decreasing over time [89]. Because of these problems related to quality, and the challenges of customizing snippets from Stack Overflow, developers have a desire for not just finding solutions, but finding solutions that are personalized to their needs/products. Some recent research has investigated how Stack Overflow is â€œflippingâ€ into generative AI solutions, and how generative AI has the potential to reshape software engineering practice [28] and replace community-based question answering [16]. McLuhanâ€™s tetrad is a provocative tool to guide reflective and predictable research, but more guidance is needed to help decide which phenomena (or actors) and conceptual ideas should be the focus of a study, and how to study these phenomena and ideas. The next part of the playbook provides guidance on these decisions. 3.2 Identifying Research Phenomena Using McGrathâ€™s Research Framework An essential facet of research design, particularly relevant to software engineering studies, is the coherent structuring of research phenomena, theoretical ideas or constructs, and methodological ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.111:8 Storey et al., 2024 choices. A pivotal framework that encapsulates these components is Joseph E. McGrathâ€™s triadic domain configuration, comprised of the substantive, conceptual, and methodological domains [48]. Here, we detail each of McGrathâ€™s domains and illustrate their application in the realm of innovative research and software engineering, forming the second part of our playbook for studying disruptive innovations through software engineering research. We continue to reflect on prior research about Stack Overflow to illustrate the playbook. 3.2.1 The Substantive Domain: Embodying Research Phenomena in SE.The substantive domain serves as the research empirical backbone, forming the focus of the questions posed. It focuses on the specific phenomena or subjects that prompt academic curiosity and investigation. The phenomena are typically the units of analysis in any research study that help frame the focus of the study. McLuhanâ€™s questions about how a technology enhances, makes obsolete, retrieves, and reverses should be asked with these phenomena in mind. Some example phenomena that have been the focus of research on Stack Overflow include: â€¢ Software Artifacts: Looking through libraries of software engineering research, we see that there is extensive research on Stack Overflow that studies software artifacts as the unit of analysis, notably questions and the quality of answers, for example see [10]. â€¢ Individual Developers: Although much of the research on Stack Overflow tended to focus on the quality or content of questions and answers, some researchers considered the direct impact on developers, for example when considering the emotions developers experience using Stack Overflow [58]. â€¢ Software Teams: Ponzanelli et al. considered teams as the unit of analysis when they studied how integrating Stack Overflow in the IDE may support how teams better support each other [65]. â€¢ Software Projects: Research by Squire et al. studied how companies made the decision to move or not move to Stack Overflow from other channels [72]. â€¢ Society: A societal concern of software engineering is to increase diversity and support inclusion. Research by Ford et al. investigated how the design of Stack Overflow may have introduced barriers and led to less participation by some genders [34]. â€¢ Software Processes and Methodologies: Stack Overflow is perceived as an invaluable and authoritative knowledge base by software developers, thus fostering the practice of code reuse from discussion threads on the Q&A site. This phenomenon motivated researchers to investigate Stack Overflowâ€™s contribution to the evolution of software processes. For example, Tang and Nadi [78] investigated how recommendations gleaned from Stack Overflow comment-edit pairs influenced software maintenance practices. â€¢ Communities: Several studies about Stack Overflow investigated how using it supported crowdsourcing of documentation at a community level [11, 55]. 3.2.2 The Conceptual Domain: Developing Ideas and Theoretical Constructs.Progressing from the empirical realm of the substantive domain, the researcher enters the conceptual domain. This is the domain where new ideas are generated and built upon, and where theoretical constructs are leveraged and developed to enable a deeper understanding of the complexities inherent to the substantive domain. Stol et al. described the role of theory in software engineering research and discuss how to leverage theories in software engineering research [75], while SjÃ¸berg et al. described how to build on and extend theories in software engineering [71]. More recently, Lorey et al., catalogued how social science theories such as the Technology Acceptance Model, the diffusion of innovation and coordination theories, have been used in software engineering research [ 44]. ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.A Disruptive Research Playbook for Studying Disruptive Innovations 111:9 Fig. 3. Methodological Domain: Empirical and non-empirical research strategies. Empirical strategies (on the right) are annotated with the quality criteria they have potential to maximize, and which strategies involve direct involvement of human participants (all except data strategies) with the blue person icon. The quality criteria shown on the outside of the empirical circle reveals the inevitable tradeoffs made when a particular strategy is selected. These and other theories provide fodder for ideas that can be explored in software engineering research that considers the disruptive impact of innovative technologies on pertinent phenomena. Returning to our illustrative example, research on Stack Overflow has led to new taxonomies to capture the kinds of programming questions asked and answered [81]. Other studies have integrated or built on theories to explain their research insights on effective community-based knowledge creation and sharing. Ford et al. provided empirical evidence of the effectiveness of mentoring strategies to support question answering in Stack Exchange [33]; Calefato and colleagues provided empirically-driven guidelines for effective question writing in Stack Overflow. Beyond the question quality, they also investigated the role played by emotions while writing questions and suggest a neutral style increases the likelihood of having a useful answer [18]. 3.2.3 The Methodological Domain: Guiding Research Strategies.The methodological domain attends to the practical dimensions of conducting research. It involves the identification and application of appropriate methods and strategies for data collection, analysis, and interpretation. This domain serves to connect the insights gained from the substantive and conceptual domains with the practicalities of empirical investigation. There are many resources in software engineering to guide the choice of research methods [27, 75, 76]. We note that McGrathâ€™s circumflex, in its original form, considers human and social aspects as a core consideration in the strategy classification it presentsâ€“aspects that are often overlooked in SE research. McGrathâ€™s framework, with its focus on human and behavioural aspects, was customized by Storey et al. [ 76] for software engineering research contexts to account for the prevalence of data studies (data mining and simulation studies) in SE. This extension is referred to as the â€œWho, What, How Frameworkâ€ and it summarizes research strategies into four broad categories: laboratory studies and experiments (strategies that increase the ability to control the behaviour of human subjects), field studies and experiments (strategies that have the potential to increase realism), respondent strategies (that have the potential to increase generalizability), and ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.111:10 Storey et al., 2024 Fig. 4. A research playbook for evaluating the impact of disruptive technologies. Step 1 builds on McLuhanâ€™s tetrad to brainstorm broad questions. Step 2 identifies phenomena and ideas to study. Step 3 creates a matrix of research questions. Step 4 suggests research strategies that focus on human aspects. The steps may be followed iteratively, refining phenomena, ideas and questions in terms of their potential research impact. data studies and experiments (that also have potential for generalizability and high data precision). The first three strategies in Figure 3 are annotated with a blue person icon to emphasize strategies that directly involve human participants as the unit of observation. 3.3 The Research Playbook McGrathâ€™s triadic domain framework, and Storey et al. â€™s extension for SE, underlines the intercon- nected nature of empirical phenomena, theoretical ideas and constructs, and methodological choices in research design. By grounding research investigations in McGrathâ€™s triadic domain framework and McLuhanâ€™s tetrad, researchers can enhance the coherence and robustness of their studies, generating meaningful insights that contribute to the advancement of software engineering. In Figure 4 we show the research playbook we propose. We use the term playbook rather than framework because of the step-by-step guidance it provides on the research process to follow, described below. Although we number and present the steps in order, it is important to note that iteration may and often should occur throughout the process of designing a research program or projects. Below, we elaborate further how the playbook can be used to reflect on prior research As an illustrative example, we used the research on the use of collaborative platforms, such as Stack Overflow, in different areas of software engineering. â€¢ Step 1: Disruptive technologies are only disruptive when they are put to use by people for some application. For this step, McLuhanâ€™s tetrad is used to sketch probing questions inspired by the four laws of new technologies. This first step encourages a researcher to step back and reflect on the different possible impacts of a disruptive technology in software engineering. â€¢ Step 2: For this second step, careful attention is paid to which phenomena should be studied in turn guiding the units of analysis for the study, but also which ideas we have about the ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.A Disruptive Research Playbook for Studying Disruptive Innovations 111:11 impact of the technology on those phenomena. These ideas may come from experience (as software developers ourselves), or they may emerge from other theories in or outside of software engineering. Choosing phenomena that relate to human and social aspects of software engineering is encouraged. â€¢ Step 3: In this third and critical step, the researcher selects, crafts, clarifies, and justifies their research question(s) considering carefully the different possibilities that the combination of steps 1 and 2 may bring to light. For this step, we suggest creating a table or matrix combining overarching questions posed by McLuhanâ€™s tetrad as rows, and the possible phenomena as columns. Cells in the matrix build on ideas (insights) we may have about the phenomena to be studied. These ideas may come from existing research or from knowledge we have from our own experiences. This crucial step of brainstorming and selecting research questions involves further contemplation about the potential impact or actionability of the results from studying these questions, as well as identifying the type of and novelty of the knowledge contribution that may arise, asking is it more understanding about the problem domain, or the design or refinement of a technology, or evaluation of the technology [29]. â€¢ Step 4: For each research question (if more than one), this step involves selecting and justifying research strategies that ideally directly involve human participants. In Table 2 and Figure 5 we show how the playbook is used to reflect on prior research on the impact of Stack Overflow on software development. Table 2 shows a matrix of research questions (each cell is a question, guided by the tetrad) that was already asked about the phenomena of documentation and programmers. Other phenomena could have been selected for consideration here (for example, teams or tools). In Figure 5 we show the different research strategies applied by the noted prior research on Stack Overflow. Each choice is a trade off. The choice of a user study means control over human subjects was possible [4]. A respondent strategy (used by Ford et al. [34]) increased the generalizability of their results on barriers to using Stack Overflow, while the field study used by Merchant et al. [53] on signals in Stack Overflow increased the realism of their study. The choice of a data mining strategy by Calefato et al. [17], had high data precision (making it potentially easier to replicate but at the expense of control over human subjects and of realism). Using the playbook to reflect on a prior disruptive technology, helped us see some patterns in prior research. Notably, most research focuses on the enhances dimension, with some research on what it made obsolete. Less research (at least so far) has considered McLuhanâ€™s retrieve and reverse dimensions. Also we noticed, that much of cited research on Stack Overflow did not consider human and social aspects, but rather emphasized technical aspects in their work. This insight is also corroborated by systematic literature studies on Stack Overflow, such as the study by Meldrum et al. [52]. In the next two sections, we show how the playbook can guide future research. We delve into two pivotal scenarios that epitomize disruptive innovations in software engineering. These scenarios serve as practical illustrations of how our proposed Playbook can be instrumental in guiding and enriching the investigation of emerging technologies in software engineering practices. 4 THE PLAYBOOK IN ACTION To demonstrate the utility and versatility of the Playbook, this paper will focus on two revolutionary technologies that are redefining the landscape of software engineering. The first scenario explores the burgeoning role of large language models, particularly generative AI, which are rapidly evolving and increasingly influencing current development practices. This disruption presents a unique opportunity to observe and analyze a disruptive technology as it emerges and integrates into the fabric of software development. ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.111:12 Storey et al., 2024 Phenomena to Study (Stack Overflow) Triadic Dimensions Documentation Individual Developers Enhances How can Stack Overflow be used to augment API documentation? [82] Does StackOverflow enhance the skills of student developers? [13] Retrieves How does the SO voting feature from sites such as Digg impact doc- umentation quality? [74] How do badges (from the realm of boy scouts) steer user behaviour in Stack Overflow? [5] Obsolesces Does Stack Overflow replace the need for formal API documenta- tion? [61] Can a Stack Overflow Bot replace a formal onboarding process in open source? [26] Reverses Does Stack Overflow lead to incomplete documentation over time? [61] Does Stack Overflow, if overused, negatively impact student learn- ing? [64] Table 2. Step3: This shows various research questions that formerly investigated how Stack Overflow impacted two phenomena: documentation and developers. Moving forward, the second scenario examines the potential of augmented and virtual reality (AR/VR) in software engineering. Despite years of development, AR/VR technologies stand on the brink of a paradigm shift in how software is developed and interacted with. The following section will not only explore the implications of AR/VR but will also demonstrate how tools like ChatGPT can be harnessed for brainstorming and conceptualizing research studies in software engineering. 4.1 Applying the Playbook to the Disruption of generative AI in Program Synthesis In the past several years, there has been a growing interest in harnessing the power of artificial intelligence (AI) and machine learning (ML) technologies to enhance and automate various aspects of the software development life cycle [8]. For instance, code completion, a feature that has been Fig. 5. Step 4: This figure shows how we can use this part of the playbook to reflect back on the choice of research strategies to select research studies. Three of these strategies as mentioned directly involve human subjects, but we noted that most studies on Stack Overflow relied on data strategies (many trading precision of data and generalizability for control over human subjects). ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.A Disruptive Research Playbook for Studying Disruptive Innovations 111:13 embedded in integrated development environments (IDEs) for years [ 57], assists developers by anticipating their intentions and providing relevant recommendations while coding [85]. We are also witnessing a pivotal moment in the progression of AI-supported tools, as systems like Copilot [1], ChatGPT-4 [59], or LLaMA [80] herald the arrival of a new epoch characterized by disruption and innovation within the field. With its sophisticated capabilities, Copilot can decipher code context and semantics from minimal input, subsequently producing suggestions for not only the next few lines but also entire functions in certain instances [22]. The extraordinary user experience furnished by these state-of-the-art large language models (LLMs) is exemplified by their extensive capabilities, superior response quality, and the significant potential to enhance user productivity [69]. Even users initially skeptical have been won over by the transformative influence of these contemporary LLMs on the software development process [69]. In the following, we show how the playbook can help guide and inform research aimed at systematically evaluating the ramifications of generative AI within the complex socio-technical ecosystem of software engineering. Step 1: McLuhanâ€™s tetrad is used to brainstorm the impacts of applying LLMs on program synthesis. The important point of using McLuhanâ€™s tetrad is to broaden the questions that may be explored in the research. In the following, we show one possible iteration of this step in the playbook. Of course, depending on the context of the researcher and research done to date, what comes to mind in this step may vary greatly. â€¢ In what ways will LLMs enhance the process of writing code? Faster coding, tests and improved documentation are likely outcomes of using LLMs. In addition to better productivity [40], quality may also be enhanced and could lead to improved time estimates and faster onboarding. â€¢ What will LLMs make obsolete when used for program synthesis? LLMs may make tools like Stack Overflow obsolete, reduce the need for search, reduce the need for manually created documentation, and even reduce demands for education about how to code. â€¢ What might LLMs retrieve that had been obsolesced before? LLMs bring back the ability to effectively explain a solution in natural language (see https://github.com/features/copilot), a more common skill from the earlier days of programming. â€¢ What will LLMs reverse if used to an extreme for program synthesis? Although LLMs are thought to improve the abilities of developers, they may over time reduce their abilities should developers trust their output and not spend time understanding the code generated. They may have less understanding over time of why and how the code works. Furthermore, a blind trust could lead to lower quality code, vulnerabilities, and more bugs that are harder to identify and fix, also due to LLM hallucinations, which is a well-known limitation of generative AI. Figure 6 shows how McLuhanâ€™s tetrad is used to assist in brainstorming the impacts generative AI may have on program synthesis capabilities and to consider broader side effects, such as loss of developer skill writing code. Using McLuhanâ€™s tetrad as a lens helps to provoke impacts that did not immediately come to mind when we think about how LLMs will only enhance program synthesis speeding up development time. We note that recently published research focuses mostly on how generative AI enhances or makes obsolete former tooling in SE, with less emphasis on what it will retrieve (such as use of natural language) or reverse into (such as over reliance and poor quality). Step 2: Deciding which phenomena to study and identifying which ideas we may have that we can build on. The possible research impacts that emerged from the first step helps to suggest ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.111:14 Storey et al., 2024 Fig. 6. Step 1: McLuhanâ€™s tetrad applied to generative AI in program synthesis to consider many possible impacts. phenomena and ideas that can be studied, with attention paid in particular to human and social aspects, as guided by the intention behind the playbook. The phenomena and ideas that become the focus of study are naturally shaped by the prior research and experiences and interests of the researcher. There is already substantial research on the impacts of LLMs on program synthesis and much of this research has focused on understanding the impact of generation on program code quality and accuracy [39]. There is also some research that has investigated the impact on developers, e.g., the good day project at GitHub studied how the use of copilot impacted the productivity and satisfaction of developers using Copilot [62]. Our playbook aims to nudge consideration of human and social aspects, thus we consider the impact of this disruptive technology on both individual developers and teams during this application of the playbook. For this step, we also need to consider what other research has been done, but also look to what signals are coming from industry about how the technology is having an impact. Step 3: Creating a matrix of possible research questions concerning phenomena and ideas that can be studied. Researchers often commit prematurely to research questions â€“ this step of the playbook slows down that process and encourages a more careful consideration of the impacts, inspired by the tetrad, crossed with possible phenomena and ideas about those phenomena to brainstorm and inspire new research questions. As mentioned above, here the playbook encourages us to select and justify specific research questions for further study. Each cell contains a potential research question to study. The rows of this matrix correspond to the McLuhan laws (the tedradic dimensions, as mentioned above) and the columns correspond to the phenomena selected for further study: teams and individual developers. The cells are populated with research questions we have brainstormed for further research building in some cases on the ideas we already have, see Table 3. Step 4: Research method selection and justification. As a final step, and based on the research questions shown in the matrix created in step 3, the playbook promotes the selection of research strategies that align with the goals of directly studying human participants (e.g., developers or other stakeholders). In Table 4 we present research strategies aligned with the questions presented in Table 3. ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.A Disruptive Research Playbook for Studying Disruptive Innovations 111:15 Phenomena to Study (LLMs) Triadic Dimensions Individual Developers Team Enhances How do LLMs enhance the produc- tivity for developers in terms of writing code? Do LLMs help a team focus more on requirements than on writing code? Retrieves Do LLMs bring back more focus on using natural language when talk- ing about their code? How can LLMs be integrated as a Bot or agent as a member of the team? Obsolesces Do LLMs make obsolete some cod- ing skills for developers? Do LLMs make the nerd culture ob- solete on the team? Reverses When used extensively for program synthesis, is there a lack of trust by developers in the code they gener- ate using LLMs? When used extensively for program synthesis, do LLMs lead to a tangi- ble loss of team knowledge? Table 3. Step 3: Matrix for brainstorming research questions for the disruptive technology of LLMs to Program Synthesis for the phenomena of Individual Developer and Team. These questions are what we the authors brainstormed (as prompted by the playbook). Research Strategies (LLM) Triadic Dimensions Individual Developers Teams Enhances Controlled experiments with hu- man participants, large-scale sur- veys, mining studies? Field or Case Study, Ethnography, Behavioral measurements Obsolesces Interview and Surveys Interview and Surveys Retrieves Longitudinal studies Field studies Reverses Controlled experiment Longitudinal case study Table 4. Step 4: High-Level Research Strategies to investigate research questions for LLMs on Individual Developers and Teams The impact of generative AI on enhancing developer productivity can be assessed through experiments with human participants, as done by Peng et al. [62] who investigate the impact of using Copilot on a web-development task. Evidence collected in the lab through controlled studies can be complemented by surveys on the perceived impact of generative AI tools on the developersâ€™ productivity. To investigate the impact of LLMs at the team level on requirements engineering, ethnographic field studies might be more suitable. As for the questions that consider McLuhanâ€™s retrieve dimension, a longitudinal observational study may assess how Copilot adoption influences how developers formulate solutions using natural language as they prompt. Indeed, recent research on prompt engineering explores how developers craft natural-language inputs to LLMs to produce their desired outputs. A longitudinal case study can be used to study how bots are adopted and used by a team over time. Interview or survey studies could help identify what coding skills are made obsolete by the adoption of tools based generative AI and what toxic cultural elements are becoming obsolete because of the emergence of new programming practices supported by these tools at the team level. Controlled experiments may align with research questions about the lack of trust by developers in the code generated by humans vs. LLMs ( reverse dimension). This research approach was ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.111:16 Storey et al., 2024 successfully adopted to investigate human-bot interaction on Stack Overflow by Murgia et al. [56]. A longitudinal case study, using interviews, may help reveal a potential loss of knowledge at the level of the team over time. Reflection on using the Playbook for Research on Generative AI in Program Synthesis. The playbook is not designed to be prescriptive but rather to aid in an iterative brainstorming process and to help question the choice of research questions and strategies, and along the way to reflect on the impact any potential research questions and designs will lead to. In this application, the playbook helped identify questions that asked how generative AI could enhance outcomes of program synthesis, but also consider what was made obsolete, what was retrieved and what it may reverse into. We found it relatively easy to identify what AI may enhance and make obsolete. But what generative AI retrieved took more thought. Ethical issues, that were more of a concern even in the early days of open source and the internet, and chatbots, popular before social media went mainstream, were identified as features retrieved from older technologies. In terms of what generative AI may reverse into, we found it relatively easy to speculate what may happen if this technology is overused (e.g., lack of trust), but what new technology it will flip into is not yet obvious. Our playbook was designed with the goal to promote more consideration of human and social aspects, aspects often overlooked by much of our research [76]. Selecting human and social phe- nomenon helped steer us in this direction. But even with a human or social phenomena at the core of a research question, a researcher may choose a research strategy that does not directly involve human participants as the unit of observation. But this flexibility of how the playbook can be used is a strength, while we hope that using the playbook will help nudge our research in the direction of a heavier emphasis on human and social aspects of software engineering. Step 3 involves brainstorming research questions inspired by McLuhanâ€™s four laws and social and human phenomena to study. Identifying research questions for all cells in the matrix (for the phenomenon we selected) was not trivial. This inspired the development of a customized GPT for our playbook, based on ChatGPT, to help open up this brainstorming activity. We apply this GPT to the second use case of the research playbook next. Applying the GPT to the above scenario is an exercise we leave for the reader to explore! 4.2 Applying the Playbook to the Disruption of AR/VR on Software Team Communication and Collaboration In recent years, Augmented Reality (AR) and Virtual Reality (VR) technologies have emerged as potent tools with transformative potential in various fields, including software development. Their application in enhancing team communication and collaboration, especially in remote work contexts, has garnered significant attention [43]. Traditional collaboration tools and methods are evolving rapidly, with AR/VR technologies offering immersive and interactive experiences that could redefine the landscape of team dynamics and productivity [42]. We recognize that applying the playbook is not always straightforward (as mentioned above), especially with very new technologies that show potential for disruption in SE but have not been studied much. To that end, we designed â€œMyResearchPlaybook, â€ a specialized GPT model, as a significant adjunct to our research playbook, to help in the brainstorming activity the playbook invokes (see Appendix A for details on this GPT model). Here, we apply MyResearchPlaybook to streamline the process of investigating the complex interactions and impacts of AR/VR technologies on software team collaboration. As a prompt to this specialized GPT model, we asked: How will AR/VR tools disrupt software team communication and collaboration? Below, we show the outcome of using MyResearchPlaybook to assist in this research brainstorm- ing activity (Appendix A provides details on how the GPT model was used): ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.A Disruptive Research Playbook for Studying Disruptive Innovations 111:17 Step 1: Using McLuhanâ€™s tetrad to conceptualize the influence of AR/VR on team dynamics. The application of McLuhanâ€™s tetrad by MyResearchPlaybook aids in expanding the scope and depth of research inquiries. We present an iteration of this process as applied to AR/VR on software team communication and collaboration, noting that we manually refined the output generated by MyResearchPlaybook by adding references to literature we found that related to the provided suggestions: â€¢ How will AR/VR enhance communication and collaboration in software teams? These technologies are likely to introduce more immersive and interactive ways of working together, potentially improving understanding, engagement, and efficiency in collaborative tasks[42]. Enhanced spatial awareness and visual communication offered by AR/VR can lead to more effective teamwork and problem-solving [24, 37, 38]. â€¢ What traditional methods might AR/VR make obsolete in software team environments? Traditional video conferencing and text-based communication tools may become less relevant, as AR/VR provide more holistic and engaging interaction methods. The need for physical presence in office spaces could also diminish, as AR/VR create virtual spaces that mimic real-world interaction [63]. â€¢ What might AR/VR retrieve that was previously obsolete in team collaboration? AR/VR could bring back a sense of â€™physicalâ€™ presence and closeness that remote teams have lost, revitalizing the face-to-face interaction dynamics in a digital format [30]. â€¢ What could be the reversal effects of extreme use of AR/VR in software teams? Over-reliance on these technologies could potentially lead to a decrease in face-to-face social skills and physical team bonding. The high immersion could also lead to difficulties in distinguishing between virtual and real-life interactions, and there might be an increased risk of digital fatigue or disorientation [41]. Step 2: Deciding which phenomena to study and identifying which ideas we may have that we can build on (if any). Our playbook emphasizes the importance of human and social dynamics, particularly in the realm of emerging AR/VR technologies. In applying the playbook to this domain, for this step we delve into the multifaceted effects of AR/VR on both individual contributors and collaborative teams. This exploration entails a thorough review of existing research, while also paying close attention to industry trends and real-world applications of AR/VR in software development environments. It is crucial to understand not only the technological capabilities of AR/VR tools but also their nuanced impact on team dynamics, communication styles, and the overall work culture. By incorporating these insights, we aim to offer a comprehensive perspective on how AR/VR technologies are reshaping the landscape of software team collaboration, highlighting both the opportunities and challenges presented by this disruptive innovation. Step 3: Creating a matrix of possible research questions concerning phenomena and ideas that can be studied. Researchers often commit prematurely to research questions â€“ this step of the playbook slows down that process and encourages a more careful consideration of the impacts, inspired by the tetrad, crossed with possible phenomena and ideas about those phenomena to brainstorm research questions. As mentioned above, here the playbook encourages us to select and justify specific research questions for further study. For this scenario and step, MyResearchPlaybook generated these questions for us which where each cell contains a potential research question to study. The rows of this matrix correspond to the McLuhan laws (the tetradic dimensions, as mentioned above) and the columns correspond to the phenomena we selected in Step 2 for further study: AR/VR in individual work and AR/VR in team collaboration, see Table 5. ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.111:18 Storey et al., 2024 Phenomena to Study (AR/VR) Triadic Dimensions Individual Work Team Collaboration Enhances How does AR/VR enhance individ- ual productivity and engagement in software development tasks? How does AR/VR improve collabo- rative problem-solving and project management in software teams? Retrieves Does AR/VR bring back a sense of physical presence and interaction in individual remote work settings? In what ways does AR/VR retrieve traditional face-to-face collabora- tion dynamics in software teams? Obsolesces Does AR/VR render traditional desktop-based software develop- ment tools obsolete for individual developers? Does the use of AR/VR in team settings make conventional remote communication tools (like video calls) obsolete? Reverses When used to an extreme, does AR/VR lead to a disconnection from the physical workspace for individ- ual developers? Could an overreliance on AR/VR technologies in software teams lead to a loss of interpersonal skills and team cohesion? Table 5. Step 3: Matrix for brainstorming research questions for the disruptive technology of AR/VR in Software Team Communication and Collaboration for the phenomena of Individual Work and Team Collaboration. MyResearchPlaybook generated the research questions shown in the Tableâ€™s cells. Step 4: Research strategy selection and justification. As a final step, and based on the research questions outlined in the matrix generated in step 3, we again turn to the MyResearchPlaybook to propose research strategies that align with the goals of directly studying human participants (e.g., software developers and teams) and that address the questions generated in step 3 (see Table 6). The impact of AR/VR on individual developersâ€™ productivity and engagement can be assessed through experiments with human participants. These studies can be complemented by surveys on the perceived impact of AR/VR tools on developersâ€™ productivity and well-being. To investigate the impact of AR/VR at the team level, field studies might be more appropriate, including case studies and ethnographic approaches to understand the nuanced changes in team dynamics and collaboration. As for the retrieve dimensions, a longitudinal study could help assess the long-term effects of AR/VR adoption on individual work habits and team collaboration practices. Interviews or survey studies could be instrumental in identifying the skills and traditional work practices made obsolete by the adoption of AR/VR. To investigate potential negative effects or over-reliance on AR/VR technologies (reverse dimension), controlled experiments and qualitative studies, such as focus groups or in-depth interviews, could be used to gather insights from both individual developers and teams. Reflection on using the Playbook for Research on AR/VR on Software Team Communication. Al- though the MyResearchPlaybook GPT model provided a structured and systematic approach for applying the playbook to the impact of AR/VR on team communication and collaboration, we also found some limitations. Its effectiveness is reliant on the existing body of research and data available. In the rapidly evolving field of AR/VR, this can be a significant limitation, as current research may not yet fully capture the long-term effects or emerging trends. This limitation potentially lead to gaps in the playbookâ€™s ability to generate comprehensive and forward-looking research questions for this use case. Further, the generalized framework provided by MyResearchPlaybook may not adequately address the unique and specific contexts within which AR/VR technologies ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.A Disruptive Research Playbook for Studying Disruptive Innovations 111:19 Research Strategies (AR/VR) Triadic Dimensions Individual Work Team Collaboration Enhances Experiments with human partici- pants, Surveys Field Studies, Case Studies, Ethnog- raphy Obsolesces Interviews, Surveys Interviews, Surveys Retrieves Longitudinal studies Longitudinal studies, Field studies Reverses Controlled experiments, Qualitative studies Qualitative studies, Longitudinal case studies Table 6. Step 4: High-Level Research Strategies for AR/VR on Individual Work and Team Collaboration are deployed. The nuances of different software development environments, team cultures, and individual differences in technology adoption are challenging to encapsulate in a standardized model, possibly leading to oversimplified conclusions. Further iterations to refine the questions need to take these different contexts into consideration, but we see how using the playbook helped with an initial version of research questions. Another limitation we note is that the MyResearchPlaybook may operate on the assumption of linear and predictable technological progression. However, the development and adoption of AR/VR technologies are influenced by a myriad of factors, including economic, social, and regulatory aspects, which may lead to non-linear and unforeseen outcomes. This can limit the playbookâ€™s ability to accurately forecast and analyze the future implications of AR/VR in software teams. However, this limitation is also something a human may encounter in any case. We noted, that the playbook, while a powerful tool, required significant human oversight and interpretation. The selection of phenomena, dimensions, and research questions, although guided by MyResearchPlaybook, ultimately depends on the researcherâ€™s insights and decisions. The GPT also did not identify literature references for us to consider. Thus, we followed the suggestions with a brief study of the literature to identify references that can inspire the research brainstorming activity. These manual steps introduce a subjective element, which can lead to biases or oversight of critical aspects but are also an asset by keeping human creativity an essential part of our research process. 5 PROVOKING A PARADIGM SHIFT IN SOFTWARE ENGINEERING RESEARCH Technological advancements in software engineering are undeniably shaping the future of the discipline. However, as we discuss the ramifications of disruptive technologies, it is paramount to understand that software engineering is fundamentally a socio-technical domain. Beyond the algorithms and architectures, the human and social dimensions provide a comprehensive perspective on the implications of these technologies. Large-scale software projects, collaborative development practices, and the rise of social coding platforms have transformed not only the methodologies but also the dynamics of who contributes and how they collaborate. McLuhanâ€™s Insight on Technologies as Human Extensions : McLuhan posited that every technology, or â€œmedia\", serves as an extension of human capabilities. Introducing a disruptive technology into software engineering is not just about adding a new tool or methodology. When we introduce disruptive technologies like AR, VR, or AI into the software engineering landscape, we are not merely integrating a new tool [36]. We are ushering in entities that interact with, augment, and sometimes challenge human capabilities. For instance, the rise of AR and VR is not just about enhanced visualization; it is about redefining how we interact with software systems. Similarly, ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.111:20 Storey et al., 2024 AIâ€™s role is not confined to task automation; it has profound implications on developer roles, team dynamics, and the entire software development lifecycle. Beyond Incrementalism: Generative AI, coupled with the immersive experiences offered by AR and VR, is more than a technological leap; itâ€™s a paradigm shift. These technologies are not just about enhancing existing practices but are inaugurating a new era of creativity and interaction. We must move beyond the safety of incremental â€˜delta papersâ€™ and dare to engage with research that is truly disruptive. This means challenging the traditional benchmarks for evaluating tools and outcomes, urging a broader, more profound, and more meaningful exploration. Redefining Evaluation and Impact : Rejecting benchmarks as the sole measure of success, we must embrace a more holistic evaluation of technological impacts. Benchmarks, while valuable, often do not capture the multifaceted impact of disruptive technologies on the socio-technical aspects of software development. A more comprehensive evaluation framework that considers user experience, societal impact, and ethical considerations is critical especially when disruptive technologies are introduced - what matters and what should be valued has possibly changed. Balancing Automation with Augmentation : Our focus must extend beyond mere automation. The true potential of disruptive technologies lies in their ability to augment human capabilities, enriching the creative and collaborative aspects of software engineering. This shift from automation to augmentation involves considering how these technologies can enhance, rather than replace, human skills and ingenuity. McGrathâ€™s Framework and the Socio-Technical Imperative : Building on McGrathâ€™s research framework, it is evident that the phenomena we study in software engineering are not just technical constructs. They encompass developer communities, collaboration patterns, ethical considerations, user experiences, and societal impacts. The human and social dimensions, especially in the face of disruptive technologies like AR, VR, and AI, are not mere footnotes. They form the crux of software engineering research. The human and social facets are not peripheral; they are at the heart of software engineering research, especially in the context of disruptive technologies. Framing the Right Questions : The trajectory of our research and innovations in software engineering is heavily influenced by the questions we pose. Inspired by McLuhanâ€™s tetrad, our inquiries should not be limited to the immediate functionalities or obsolescence of a technology. We must probe deeper into the broader socio-technical ripples. For instance, how does the integration of AI in software engineering reshape team hierarchies or inclusivity paradigms? What emerging practices or norms are developer communities adopting in response to these innovations? How do these technological shifts impact perceptions of software reliability, trust, or ethical standards? Fur- thermore, as technologies like generative AI or AR/VR redefine software developmentâ€™s landscape, they challenge established notions of expertise and collaboration. The democratization brought about by these technologies introduces a plethora of voices and perspectives into software engineer- ing. While this diversity is a strength, it also presents challenges in communication, collaboration, and conflict mediation. As we find ourselves at the forefront of these technological breakthroughs, it is crucial to pause and reflect on the wider socio-technical consequences. We need to push our boundaries, daring to ask deeper questions that delve into the complex interplay between humans, technologies, and society. Designing the Future of Research Creativity : To truly liberate our passion for research in this rapidly evolving landscape, we need to establish structures that foster creativity, interdisciplinary collaboration, and bold exploration. This involves creating academic and industrial environments that support risk-taking, encourage exploration of unconventional ideas, and facilitate the integra- tion of diverse perspectives. Disrupting Research with Disruptive Innovations : As we explore disruptive innovations that are impacting software engineering, we are also recognizing how these innovations also impact ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.A Disruptive Research Playbook for Studying Disruptive Innovations 111:21 the very essence of how we do research. The internet and social media both impacted our ways of doing and sharing research, and likewise, tools like ChatGPT are having a similar if not bigger disruption. We found ourselves using ChatGPT throughout the writing of our paper and design of the playbook (somewhat reluctantly at first), but finally embraced the role ChatGPT can play by designing a custom GPT to support the use of the playbook. As mentioned above, our intent is that it should not replace the creativity and deep reflection we as researchers do, but it can be used to support brainstorming and pointers to ideas and resources we may not have known about. In summary, as we navigate the complexities of disruptive technologies in software engineering, our role as researchers and practitioners demands a provocative, forward-thinking approach. We must challenge existing paradigms, embrace the socio-technical nature of our field, and seek to understand and shape the broader implications of our work. By doing so, we can ensure that our contributions to the field of software engineering are not only technologically advanced but also socially responsible and ethically sound. It is through this holistic understanding and approach that we can truly harness the transformative power of disruptive technologies in software engineering. 6 CONCLUSION Software engineering, as a discipline, has seen, since its inception, many disruptive technologies that have augmented and extended the capabilities of its engineers who design, write, maintain and ensure the quality of software programs. These disruptions, like other forms of media and technologies, all follow the same four laws that McLuhan noted in that they will enhance software engineering capabilities, they will make obsolete some existing technologies, they will retrieve aspects of prior technologies, and they will ultimately reverse the desired outcomes that inspired them when used over time. As researchers, we have the privilege and opportunity to study the impacts these technologies introduce. However, often our research is slow to respond to the disruptions new technologies evoke, and we take an overly narrow approach and focus on the short term technical advantages they introduce. McLuhan recognized this tendency for people to focus on positive influences decades ago and his observations of the four laws of media led to his tetrad of provoking and non-obvious questions to ask about new technologies. The playbook we introduce, builds on McLuhanâ€™s four laws and likewise is designed to provoke and prompt new research directions that ask more critical questions reflecting on what has come before as well as what will happen, especially to socio-technical aspects of software engineering, when the technologies are used and possibly overused over time. We combine McLuhanâ€™s tetrad with research frameworks that help us craft pertinent research questions about socio-technical aspects in software engineering, while also selecting research strategies that are aligned with studying the phenomena that may matter the most over time - that of human and social aspects. We apply the playbook in two ways. First we use it to reflect on a prior disruptive technology on software engineeringâ€”Stack Overflowâ€”and to consider some of the landscape of research that investigated the impact of it on individual developers and development teams. This retrospective application of the playbook not only illustrated the playbook, but also revealed that this prior research was predominantly focused on McLuhanâ€™s â€œenhanceâ€ and â€œobsoleteâ€ dimensions, and this research had, especially initially, little emphasis on studying social and human aspects. We also applied the playbook to two disruptive technologies that are showing disruption on software engineering. The playbook helps steer towards new research directions that focus on human and social aspects. Finally, along with the playbook, we provide a customized ChatGPT called MyResearchPlaybook that some researchers may find useful as they brainstorm and design their research questions and studies. We expect we are just at the beginning of these new research assistant tools that will help us expand, accelerate, broaden and improve our research over time. ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.111:22 Storey et al., 2024 In conclusion, software is designed by people for people. How technology will change human lives, capabilities and values changes faster than we can imagine, and demands that we anticipate and pay close attention to the changes that may occur. The playbook we present, we hope, will provide a way for researchers to pause before selecting a particular research question or strategy, and to reflect on what else they could study that may be ultimately more important to consider with such an uncertain future in mind. ACKNOWLEDGMENTS We are grateful to the organizers and participants of the NII Shonan Meeting (No.191) â€œHuman Aspects in Software Engineering\" (March 2023) for their valuable insights and discussions that in- spired the genesis of this playbook. Additionally, our thanks go to the attendees of The Copenhagen Symposium on Human-Centered Software Engineering AI (November 2023) whose constructive feedback was crucial in refining our work (Grants G-2023-21020 and CF23-0208). The contributions from both events have been instrumental in shaping the perspectives presented in this study. We also thank Cassandra Petrachenko for her carefully editing and improving our paper. REFERENCES [1] 2021. GitHub Copilot. https://copilot.github.com [2] Rabe Abdalkareem, Emad Shihab, and Juergen Rilling. 2017. On code reuse from StackOverflow: An exploratory study on Android apps. Information and Software Technology 88 (2017), 148â€“158. https://doi.org/10.1016/j.infsof.2017.04.005 [3] Yasemin Acar, Michael Backes, Sascha Fahl, Doowon Kim, Michelle L. Mazurek, and Christian Stransky. 2016. You Get Where Youâ€™re Looking for: The Impact of Information Sources on Code Security. In2016 IEEE Symposium on Security and Privacy (SP) . 289â€“305. https://doi.org/10.1109/SP.2016.25 [4] Yasemin Acar, Michael Backes, Sascha Fahl, Doowon Kim, Michelle L. Mazurek, and Christian Stransky. 2016. You Get Where Youâ€™re Looking for: The Impact of Information Sources on Code Security. In2016 IEEE Symposium on Security and Privacy (SP) . 289â€“305. https://doi.org/10.1109/SP.2016.25 [5] Ashton Anderson, Daniel Huttenlocher, Jon Kleinberg, and Jure Leskovec. 2013. Steering User Behavior with Badges. In Proceedings of the 22nd International Conference on World Wide Web (Rio de Janeiro, Brazil) (WWW â€™13) . Association for Computing Machinery, New York, NY, USA, 95â€“106. https://doi.org/10.1145/2488388.2488398 [6] Muhammad Asaduzzaman, Ahmed Shah Mashiyat, Chanchal K Roy, and Kevin A Schneider. 2013. Answering questions about unanswered questions of Stack Overflow. In Proceedings of the 10th International Working Conference on Mining Software Repositories . IEEE, 97â€“100. [7] Alberto Bacchelli, Luca Ponzanelli, and Michele Lanza. 2012. Harnessing Stack Overflow for the IDE. In 2012 Third International Workshop on Recommendation Systems for Software Engineering (RSSE) . 26â€“30. https://doi.org/10.1109/ RSSE.2012.6233404 [8] Marco Barenkamp, Jonas Rebstadt, and Oliver Thomas. 2020. Applications of AI in classical software engineering. AI Perspectives 2, 1 (2020), 1. [9] Anton Barua, Stephen W. Thomas, and Ahmed E. Hassan. 2014. What Are Developers Talking about? An Analysis of Topics and Trends in Stack Overflow. Empirical Softw. Engg. 19, 3 (jun 2014), 619â€“654. https://doi.org/10.1007/ s10664-012-9231-y [10] Anton Barua, Stephen W Thomas, and Ahmed E Hassan. 2014. What are developers talking about? an analysis of topics and trends in stack overflow. Empirical software engineering 19 (2014), 619â€“654. [11] Ohad Barzilay, Christoph Treude, and Alexey Zagalsky. 2013. Facilitating crowd sourced software engineering via stack overflow. Finding Source Code on the Web for Remix and Reuse (2013), 289â€“308. [12] Abraham Bernstein and Natasha Noy. 2014. Is This Really Science? The Semantic Webberâ€™s Guide to Evaluating Research Contributions. Technical Report. [13] Trishala Bhasin, Adam Murray, and Margaret-Anne Storey. 2021. Student experiences with github and stack overflow: An exploratory study. In 2021 IEEE/ACM 13th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE) . IEEE, 81â€“90. [14] Amiangshu Bosu, Christopher S Corley, Dustin Heaton, Debarshi Chatterji, Jeffrey C Carver, and Nicholas A Kraft. 2013. Building reputation in StackOverflow: An empirical investigation. InProceedings of the 10th International Working Conference on Mining Software Repositories . IEEE, 89â€“92. [15] Joseph L. Bower and Clayton M. Christensen. 1995. Disruptive Technologies: Catching the Wave. In Harvard Business Review. 43â€“53. Issue Januaryâ€“February 1995. ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.A Disruptive Research Playbook for Studying Disruptive Innovations 111:23 [16] Gordon Burtch, Dokyun Lee, and Zhichen Chen. 2023. The Consequences of Generative AI for UGC and Online Community Engagement. IEEE Transactions on Software Engineering (2023). https://doi.org/10.2139/ssrn.4521754 [17] Fabio Calefato, Filippo Lanubile, Maria Concetta Marasciulo, and Nicole Novielli. 2015. Mining Successful Answers in Stack Overflow. In 2015 IEEE/ACM 12th Working Conference on Mining Software Repositories . 430â€“433. https: //doi.org/10.1109/MSR.2015.56 [18] Fabio Calefato, Filippo Lanubile, and Nicole Novielli. 2018. How to Ask for Technical Help? Evidence-Based Guidelines for Writing Questions on Stack Overflow. Inf. Softw. Technol. 94, C (feb 2018), 186â€“207. [19] Eduardo C. Campos, Martin Monperrus, and Marcelo A. Maia. 2016. Searching Stack Overflow for API-Usage-Related Bug Fixes Using Snippet-Based Queries (CASCON â€™16) . IBM Corp., USA, 232â€“242. [20] Kaibo Cao, Chunyang Chen, Sebastian Baltes, Christoph Treude, and Xiang Chen. 2021. Automated Query Reformula- tion for Efficient Search Based on Query Logs From Stack Overflow. In2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE) . 1273â€“1285. https://doi.org/10.1109/ICSE43902.2021.00116 [21] Preetha Chatterjee, Minji Kong, and Lori Pollock. 2020. Finding help with programming errors: An exploratory study of novice software engineersâ€™ focus in stack overflow posts. Journal of Systems and Software 159 (2020), 110454. https://doi.org/10.1016/j.jss.2019.110454 [22] Mark Chen, Jakub Tworek, Heewoo Jun, Qijing Yuan, Henryk P. de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Niru Joseph, Greg Brockman, Alexander Ray, Rishita Puri, Gabriel Krueger, Mike Petrov, Hala Khlaaf, Girish Sastry, Pamela Mishkin, Ben Chan, Scott Gray, Nick Ryder, Maxim Pavlov, Alex Power, Lukasz Kaiser, Maximilian Bavarian, Carolyn Winter, Philippe Tillet, Felipe P. Such, David Cummings, Matthias Plappert, Fotios Chantzis, Eric Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Andrew Paino, Nick Tezak, Jerry Tang, Igor Babuschkin, Sujith Balaji, Shivendra Jain, Will Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Eri Morikawa, Alec Radford, Melody Knight, Miles Brundage, Matej Murati, Katja Mayer, Peter Welinder, Brian McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code. https://arxiv.org/abs/2107.03374 [23] Clayton M. Christensen and Michael Overdorf. 2000. Meeting the Challenge of Disruptive Change. InHarvard Business Review. 66â€“76. Issue Marchâ€“April 2000. [24] Elizabeth F Churchill and Dave Snowdon. 1998. Collaborative virtual environments: an introductory review of issues and systems. virtual reality 3 (1998), 3â€“15. [25] Georgios Digkas, Nikolaos Nikolaidis, Apostolos Ampatzoglou, and Alexander Chatzigeorgiou. 2019. Reusing Code from StackOverflow: The Effect on Technical Debt. In 2019 45th Euromicro Conference on Software Engineering and Advanced Applications (SEAA) . 87â€“91. https://doi.org/10.1109/SEAA.2019.00022 [26] James Dominic, Jada Houser, Igor Steinmacher, Charles Ritter, and Paige Rodeghero. 2020. Conversational Bot for Newcomers Onboarding to Open Source Projects. In Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops (Seoul, Republic of Korea) (ICSEWâ€™20). Association for Computing Machinery, New York, NY, USA, 46â€“50. https://doi.org/10.1145/3387940.3391534 [27] Steve Easterbrook, Janice Singer, Margaret-Anne Storey, and Daniela Damian. 2008. Selecting Empirical Methods for Software Engineering Research. In Guide to Advanced Empirical Software Engineering , Forrest Shull, Janice Singer, and Dag I. K. Sjoberg (Eds.). Springer, London, 285â€“311. https://doi.org/10.1007/978-1-84800-044-5_11 [28] Christof Ebert and Panos Louridas. 2023. Generative AI for Software Practitioners. IEEE Software 40, 4 (2023), 30â€“38. https://doi.org/10.1109/MS.2023.3265877 [29] Emelie EngstrÃ¶m, Margaret-Anne Storey, Per Runeson, Martin HÃ¶st, and Maria Teresa Baldassarre. 2020. How software engineering research aligns with design science: a review. Empirical Software Engineering 25 (2020), 2630â€“2660. [30] Jingchao Fang, Victoria Chang, Ge Gao, and Hao-Chuan Wang. 2021. Social Interactions in Virtual Reality: What Cues Do People Use Most and How. In Companion Publication of the 2021 Conference on Computer Supported Cooperative Work and Social Computing (Virtual Event, USA) (CSCW â€™21 Companion) . Association for Computing Machinery, New York, NY, USA, 49â€“52. https://doi.org/10.1145/3462204.3481772 [31] Flipe Fernandes and ClÃ¡udia Werner. 2022. A Systematic Literature Review of the Metaverse for Software Engineering Education: Overview, Challenges and Opportunities. PRESENCE: Washington, WA, USA (2022). [32] Felix Fischer, Konstantin BÃ¶ttinger, Huang Xiao, Christian Stransky, Yasemin Acar, Michael Backes, and Sascha Fahl. 2017. Stack Overflow Considered Harmful? The Impact of Copy & Paste on Android Application Security. In 2017 IEEE Symposium on Security and Privacy (SP) . 121â€“136. https://doi.org/10.1109/SP.2017.31 [33] Denae Ford, Kristina Lustig, Jeremy Banks, and Chris Parnin. 2018. \"We Donâ€™t Do That Here\": How Collaborative Editing with Mentors Improves Engagement in Social Q&A Communities. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (Montreal QC, Canada) (CHI â€™18) . Association for Computing Machinery, New York, NY, USA, 1â€“12. https://doi.org/10.1145/3173574.3174182 [34] Denae Ford, Justin Smith, Philip J Guo, and Chris Parnin. 2016. Paradise unplugged: Identifying barriers for female participation on stack overflow. In Proceedings of the 2016 24th ACM SIGSOFT International symposium on foundations ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.111:24 Storey et al., 2024 of software engineering . 846â€“857. [35] Scott Grant and Buddy Betts. 2013. Encouraging user behaviour with achievements: An empirical study. In 2013 10th Working Conference on Mining Software Repositories (MSR) . 65â€“68. [36] Catherine M. Hicks. 2024. Psychological Affordances Can Provide a Missing Explanatory Layer for Why Interventions to Improve Developer Experience Take Hold or Fail. (2024). https://files.osf.io/v1/resources/qz43x/providers/osfstorage/ 65b2f3ae4aa63c07d9df22ec?action=download&direct&version=5 Publisher: OSF. [37] Adrian Hoff, Christoph Seidl, Mircea F Lungu, and Michele Lanza. 2023. Preparing Software Re-Engineering via Freehand Sketches in Virtual Reality. In 39th IEEE International Conference on Software Maintenance and Evolution . IEEE, to appear. [38] Adrian H. Hoppe, Florian van de Camp, and Rainer Stiefelhagen. 2021. ShiSha: Enabling Shared Perspective With Face-to-Face Collaboration Using Redirected Avatars in Virtual Reality. Proc. ACM Hum.-Comput. Interact. 4, CSCW3, Article 251 (jan 2021), 22 pages. https://doi.org/10.1145/3432950 [39] Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. 2023. Large Language Models for Software Engineering: A Systematic Literature Review. arXiv:2308.10620 [cs.SE] [40] Saki Imai. 2022. Is GitHub Copilot a Substitute for Human Pair-Programming? An Empirical Study. In Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings (Pittsburgh, Pennsylvania) (ICSE â€™22) . Association for Computing Machinery, New York, NY, USA, 319â€“321. https://doi.org/10.1145/3510454. 3522684 [41] Mara Kaufeld, Martin Mundt, Sarah Forst, and Heiko Hecht. 2022. Optical see-through augmented reality can induce severe motion sickness. Displays 74 (2022), 102283. https://doi.org/10.1016/j.displa.2022.102283 [42] Alexander Krause-Glau, Malte Hansen, and Wilhelm Hasselbring. 2022. Collaborative program comprehension via software visualization in extended reality. Information and Software Technology 151 (2022), 107007. [43] Veronika KrauÃŸ, Alexander Boden, Leif Oppermann, and RenÃ© Reiners. 2021. Current practices, challenges, and design implications for collaborative ar/vr application development. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems . 1â€“15. [44] Tobias Lorey, Paul Ralph, and Michael Felderer. 2022. Social Science Theories in Software Engineering Research. In Proceedings of the 44th International Conference on Software Engineering (Pittsburgh, Pennsylvania) (ICSE â€™22) . Association for Computing Machinery, New York, NY, USA, 1994â€“2005. https://doi.org/10.1145/3510003.3510076 [45] Rafael Lotufo, Leonardo Teixeira Passos, and Krzysztof Czarnecki. 2012. Towards improving bug tracking systems with game mechanisms. In 9th IEEE Working Conference of Mining Software Repositories, MSR 2012, June 2-3, 2012, Zurich, Switzerland , Michele Lanza, Massimiliano Di Penta, and Tao Xie (Eds.). IEEE Computer Society, 2â€“11. https: //doi.org/10.1109/MSR.2012.6224293 [46] Stephen Lukasik. 2011. Why the Arpanet Was Built. IEEE Annals of the History of Computing 33, 3 (2011), 4â€“21. https://doi.org/10.1109/MAHC.2010.11 [47] Lena Mamykina, Bella Manoim, Manas Mittal, George Hripcsak, and BjÃ¶rn Hartmann. 2011. Design Lessons from the Fastest Q&a Site in the West. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (Vancouver, BC, Canada)(CHI â€™11) . Association for Computing Machinery, New York, NY, USA, 2857â€“2866. https: //doi.org/10.1145/1978942.1979366 [48] Joseph E McGrath. 1995. Methodology matters: Doing research in the behavioral and social sciences. In Readings in Humanâ€“Computer Interaction . Elsevier, 152â€“169. [49] Marshall McLuhan. 1977. Laws of the Media. ETC: A Review of General Semantics (1977), 173â€“179. [50] Marshall McLuhan. 2017. The medium is the message. In Communication theory . Routledge, 390â€“402. [51] Sarah Meldrum, Sherlock A. Licorish, Caitlin A. Owen, and Bastin Tony Roy Savarimuthu. 2020. Understanding stack overflow code quality: A recommendation of caution. Science of Computer Programming 199 (2020), 102516. https://doi.org/10.1016/j.scico.2020.102516 [52] Sarah Meldrum, Sherlock A. Licorish, and Bastin Tony Roy Savarimuthu. 2020. Exploring Research Interest in Stack Overflow - A Systematic Mapping Study and Quality Evaluation. CoRR abs/2010.12282 (2020). arXiv:2010.12282 https://arxiv.org/abs/2010.12282 [53] Arpit Merchant, Daksh Shah, Gurpreet Singh Bhatia, Anurag Ghosh, and Ponnurangam Kumaraguru. 2019. Signals Matter: Understanding Popularity and Impact of Users on Stack Overflow. In The World Wide Web Conference (San Francisco, CA, USA) (WWW â€™19) . Association for Computing Machinery, New York, NY, USA, 3086â€“3092. https: //doi.org/10.1145/3308558.3313583 [54] Joel Mokyr. 1992. The lever of riches: Technological creativity and economic progress . Oxford University Press. [55] Iraklis Moutidis and Hywel TP Williams. 2021. Community evolution on stack overflow.Plos one 16, 6 (2021), e0253010. [56] Alessandro Murgia, Daan Janssens, Serge Demeyer, and Bogdan Vasilescu. 2016. Among the Machines: Human-Bot Interaction on Social Q&A Websites. In Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems (San Jose, California, USA) (CHI EA â€™16) . Association for Computing Machinery, New York, NY, ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.A Disruptive Research Playbook for Studying Disruptive Innovations 111:25 USA, 1272â€“1279. https://doi.org/10.1145/2851581.2892311 [57] Gail Murphy, Mik Kersten, and Leah Findlater. 2006. How are java software developers using the eclipse ide? IEEE Software 23, 4 (2006), 76â€“83. [58] Nicole Novielli, Fabio Calefato, and Filippo Lanubile. 2014. Towards discovering the role of emotions in stack overflow. In Proceedings of the 6th international workshop on social software engineering . 33â€“36. [59] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL] [60] Chris Parnin and Alessandro Orso. 2011. Are automated debugging techniques actually helping programmers?. In Proceedings of the International Symposium on Software Testing and Analysis . ACM. [61] Chris Parnin, Christoph Treude, Lars Grammel, and Margaret-Anne Storey. 2012. Crowd documentation: Exploring the coverage and the dynamics of API discussions on Stack Overflow. Georgia Institute of Technology, Tech. Rep 11 (2012). [62] Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirere. 2023. The Impact of AI on Developer Productivity: Evidence from GitHub. arXiv preprint arXiv:2302.06590 (2023). http://arxiv.org/abs/2302.06590 [63] Tekla S. Perry. 2016. Virtual reality goes social. IEEE Spectrum 53, 1 (2016), 56â€“57. https://doi.org/10.1109/MSPEC. 2016.7367470 [64] Jaanus PÃ¶ial. 2021. Challenges of Teaching Programming in StackOverflow Era. In Educating Engineers for Future Industrial Revolutions, Michael E. Auer and Tiia RÃ¼Ã¼tmann (Eds.). Springer International Publishing, Cham, 703â€“710. [65] Luca Ponzanelli, Alberto Bacchelli, and Michele Lanza. 2013. Seahawk: Stack overflow in the ide. In 2013 35th International Conference on Software Engineering (ICSE) . IEEE, 1295â€“1298. [66] Chaiyong Ragkhitwetsagul, Jens Krinke, Matheus Paixao, Giuseppe Bianco, and Rocco Oliveto. 2021. Toxic Code Snippets on Stack Overflow. IEEE Transactions on Software Engineering 47, 3 (2021), 560â€“581. https://doi.org/10.1109/ TSE.2019.2900307 [67] P. Ralph et al. 2020. ACM SIGSOFT Empirical Standards. arXiv preprint arXiv:2010.03525 (2020). [68] Paul Ralph, Nauman bin Ali, Sebastian Baltes, Domenico Bianculli, Jessica Diaz, Yvonne Dittrich, Neil Ernst, Michael Felderer, Robert Feldt, Antonio Filieri, et al. 2020. Empirical standards for software engineering research. arXiv preprint arXiv:2010.03525 (2020). [69] Steven I Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin D Weisz. 2023. The programmerâ€™s assistant: Conversational interaction with a large language model for software development. In Proceedings of the 28th International Conference on Intelligent User Interfaces . 491â€“514. [70] Rodrigo F. Silva, Mohammad Masudur Rahman, Carlos Eduardo Dantas, Chanchal Roy, Foutse Khomh, and Marcelo A. Maia. 2021. Improved retrieval of programming solutions with code examples using a multi-featured score. Journal of Systems and Software 181 (2021), 111063. https://doi.org/10.1016/j.jss.2021.111063 [71] Dag I. K. SjÃ¸berg, Tore DybÃ¥, Bente C. D. Anda, and Jo E. Hannay. 2008. Building Theories in Software Engineering . Springer London, London, 312â€“336. https://doi.org/10.1007/978-1-84800-044-5_12 [72] Megan Squire. 2015. \" Should We Move to Stack Overflow?\" Measuring the Utility of Social Media for Developer Support. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering , Vol. 2. IEEE, 219â€“228. [73] Megan Squire. 2015. Should We Move to Stack Overflow? Measuring the Utility of Social Media for Developer Support. In Proceedings of the 37th International Conference on Software Engineering - Volume 2 (Florence, Italy) (ICSE â€™15) . IEEE Press, 219â€“228. [74] Megan Squire and Christian Funkhouser. 2014. \"A Bit of Code\": How the Stack Overflow Community Creates Quality Postings. In 2014 47th Hawaii International Conference on System Sciences . 1425â€“1434. https://doi.org/10.1109/HICSS. 2014.185 [75] Klaas-Jan Stol and Brian Fitzgerald. 2013. Uncovering theories in software engineering. In 2013 2nd SEMAT Workshop on a General Theory of Software Engineering (GTSE) . 5â€“14. https://doi.org/10.1109/GTSE.2013.6613863 [76] Margaret-Anne Storey, Neil A Ernst, Courtney Williams, and Eirini Kalliamvakou. 2020. The who, what, how of software engineering research: a socio-technical framework. Empirical Software Engineering 25 (2020), 4097â€“4129. [77] Siddharth Subramanian and Reid Holmes. 2013. Making sense of online code snippets. In Proceedings of the 10th International Working Conference on Mining Software Repositories . IEEE, 85â€“88. [78] Henry Tang and Sarah Nadi. 2021. On using Stack Overflow comment-edit pairs to recommend code maintenance changes. Empirical Softw. Engg. 26, 4 (jul 2021), 35 pages. https://doi.org/10.1007/s10664-021-09954-8 [79] John Thomas. 2007. Operationalizing McLuhanâ€™s tetrad to focus on innovation effects. (Jan. 2007). https://dr.lib.iastate. edu/handle/20.500.12876/68461 [80] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971 [cs.CL] [81] Christoph Treude, Ohad Barzilay, and Margaret-Anne D. Storey. 2011. How do programmers ask and answer questions on the web?. In Proceedings of the 33rd International Conference on Software Engineering, ICSE 2011, Waikiki, Honolulu , HI, USA, May 21-28, 2011 , Richard N. Taylor, Harald C. Gall, and Nenad Medvidovic (Eds.). ACM, 804â€“807. https: ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.111:26 Storey et al., 2024 //doi.org/10.1145/1985793.1985907 [82] Christoph Treude and Martin P Robillard. 2016. Augmenting API documentation with insights from stack overflow. In Proceedings of the 38th International Conference on Software Engineering . 392â€“403. [83] Christoph Treude and Martin P. Robillard. 2017. Understanding Stack Overflow Code Fragments. In 2017 IEEE International Conference on Software Maintenance and Evolution (ICSME) . 509â€“513. https://doi.org/10.1109/ICSME. 2017.24 [84] Bogdan Vasilescu, Vladimir Filkov, and Alexander Serebrenik. 2013. StackOverflow and GitHub: Associations between software development and crowdsourced knowledge. In Proceedings of the 2013 ASE/IEEE International Conference on Social Computing (Washington D.C., USA). IEEE, 188â€“195. [85] Stefan Wagner and GÃ¼nter Ruhe. 2018. A systematic review of productivity factors in software development. Arxiv (2018). https://arxiv.org/abs/1801.06475 [86] C. Wohlin, P. Runeson, M. HÃ¶st, M. Ohlsson, B. Regnell, and A. WesslÃ©n. 2012.Experimentation in Software Engineering . Springer Science & Business Media. [87] Yuhao Wu, Shaowei Wang, Cor-Paul Bezemer, and Katsuro Inoue. 2019. How Do Developers Utilize Source Code from Stack Overflow? Empirical Softw. Engg. 24, 2 (apr 2019), 637â€“673. https://doi.org/10.1007/s10664-018-9634-5 [88] Murat Yilmaz, Emer Oâ€™farrell, and Paul Clarke. 2023. Examining the training and education potential of the metaverse: Results from an empirical study of next generation SAFe training. Journal of Software: Evolution and Process (2023), e2531. [89] Tianyi Zhang, Ganesha Upadhyaya, Anastasia Reinhardt, Hridesh Rajan, and Miryung Kim. 2018. Are Code Examples on an Online Q&A Forum Reliable? A Study of API Misuse on Stack Overflow. In Proceedings of the 40th International Conference on Software Engineering (Gothenburg, Sweden) (ICSE â€™18) . Association for Computing Machinery, New York, NY, USA, 886â€“896. https://doi.org/10.1145/3180155.3180260 A THE MYRESEARCHPLAYBOOKGPT: ENHANCING RESEARCH ON DISRUPTIVE TECHNOLOGIES Software engineering is continuously being reshaped by disruptive technologies. These innovations necessitate a multifaceted approach to research, one that comprehends both the technological advancements and their socio-technical implications. In this regard, we introduce â€œMyResearch- Playbook, â€ a specialized GPT model, as a significant adjunct to our research playbook and to help in particular in the brainstorming activity the playbook invokes. In this section, we describe a playbook GPT we have designed to assist in using the playbook. In Section 4.2 we show how we applied this GPT to assist (not replace!) our activity of applying the playbook to the role of AR/VR in SE. A.1 GPTs: Tailored AI for Focused Inquiry GPTs, a subset of the broader Generative Pre-trained Transformer models by OpenAI, are customiz- able AI tools designed to address specific challenges and tasks. These models extend the capabilities of standard GPTs by focusing on particular domains or applications, offering more nuanced and relevant insights. An example of this is MyResearchPlaybook2, a GPT model configured to assist in exploring the interplay between disruptive technologies and their human-centric impacts. To make the instructions transparent and reusable with other LLMs, we published them in a GitHub repo3. A.2 MyResearchPlaybook: Bridging McLuhanâ€™s Theory and Software Engineering Research Drawing upon the theoretical framework of McLuhanâ€™s Triadic Dimensionsâ€”enhances, obsoletes, retrieves, and reversesâ€”MyResearchPlaybook aids in constructing a structured matrix for analysis. This matrix serves as a scaffold for developing research questions that are pivotal in understanding 2MyResearchPlaybook is accessible at this link in OpenAIâ€™s library: https://chat.openai.com/g/ g-ckghjdz0n-myresearchplaybook. 3Link to the GitHub repo with the instructions: https://github.com/danielrusso-org/MyResearchPlaybook. ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.A Disruptive Research Playbook for Studying Disruptive Innovations 111:27 the multifaceted effects of disruptive technologies such as AI, AR, VR, and their iterations in software engineering. A.3 Capabilities and Contributions â€¢ Analytical Matrix Construction: MyResearchPlaybook generates matrices aligning McLuhanâ€™s Dimensions with current research phenomena in software engineering, ensuring a compre- hensive exploration of potential impacts. â€¢ Formulation of Research Questions : It proposes specific research questions at the inter- section of each dimension and phenomenon, guiding researchers toward targeted areas of exploration. â€¢ Strategic Research Planning : The tool suggests initial research strategies in accordance with the ACM SIGSOFT Empirical Standards[68], which are refined based on the researcherâ€™s focus, to provide a detailed methodological approach. â€¢ Adaptability and Contextual Relevance : Acknowledging the diverse nature of questions in disruptive technology research, MyResearchPlaybook adapts its recommendations to various contexts, including empirical studies, literature reviews, and case studies. In summary, MyResearchPlaybook stands as an innovative addition to our research methodology, introducing a structured, rigorous approach to investigating disruptive technologies in software engineering. Its deployment aims to enhance the analytical depth and breadth of our studies, ensuring that our contributions are not only technologically advanced but also socio-technically informed and ethically sound. A.4 Limitations of MyResearchPlaybook While MyResearchPlaybook represents a significant advancement in AI-assisted research in the field of software engineering and disruptive technologies, it is imperative to acknowledge its limitations. This cautionary note serves to highlight areas where reliance on MyResearchPlaybook may require supplementation with human expertise and critical analysis. â€¢ Dependence on Pre-Existing Knowledge : MyResearchPlaybook operates within the bounds of its training data. This limitation means that the tool may not be aware of the very latest research developments or emerging technologies that have arisen post-training. â€¢ Lack of Domain-Specific Depth : While MyResearchPlaybook is tailored for research in disruptive technologies, its understanding may not match the depth of knowledge of a seasoned expert in a specific sub-field. It is a generalist tool and may not fully grasp highly specialized or niche areas. â€¢ Potential for Bias : Like any AI tool, MyResearchPlaybook may inadvertently reflect biases present in its training data. Researchers should be cautious of these potential biases and critically evaluate the suggestions and insights offered by the tool. â€¢ Absence of Creative Insight : AI models, including MyResearchPlaybook, excel at analyzing and synthesizing existing information but do not possess the creative or innovative capabilities of human researchers. They cannot generate novel theories or hypotheses that have not been previously conceptualized. â€¢ Need for Human Interpretation : The toolâ€™s outputs require interpretation and contex- tualization by knowledgeable researchers. MyResearchPlaybook can assist in identifying patterns and generating hypotheses, but the ultimate analysis and decision-making should rest with human researchers. ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.111:28 Storey et al., 2024 â€¢ Ethical Considerations: Ethical considerations, particularly in the field of disruptive tech- nologies, often require nuanced understanding and judgement that may be beyond the scope of an AI model like MyResearchPlaybook. Researchers are advised to use MyResearchPlaybook as a complement to, rather than a replace- ment for, their expertise and critical thinking skills. The tool should be viewed as one component in a broader methodological toolkit, offering support but not supplanting the essential human elements of curiosity, creativity, and ethical judgement in research. ACM Trans. Softw. Eng. Methodol., Vol. 37, No. 4, Article 111. Publication date: February 2024.",
      "references": [
        "GitHub Copilot.",
        "On code reuse from StackOverflow: An exploratory study on Android apps.",
        "You Get Where Youâ€™re Looking for: The Impact of Information Sources on Code Security.",
        "Steering User Behavior with Badges.",
        "Answering questions about unanswered questions of Stack Overflow.",
        "Harnessing Stack Overflow for the IDE.",
        "Applications of AI in classical software engineering.",
        "What Are Developers Talking about? An Analysis of Topics and Trends in Stack Overflow.",
        "Facilitating crowd sourced software engineering via stack overflow.",
        "Is This Really Science? The Semantic Webberâ€™s Guide to Evaluating Research Contributions.",
        "Student experiences with github and stack overflow: An exploratory study.",
        "Building reputation in StackOverflow: An empirical investigation.",
        "Disruptive Technologies: Catching the Wave.",
        "The Consequences of Generative AI for UGC and Online Community Engagement.",
        "Mining Successful Answers in Stack Overflow.",
        "How to Ask for Technical Help? Evidence-Based Guidelines for Writing Questions on Stack Overflow.",
        "Searching Stack Overflow for API-Usage-Related Bug Fixes Using Snippet-Based Queries",
        "Automated Query Reformula- tion for Efficient Search Based on Query Logs From Stack Overflow.",
        "Finding help with programming errors: An exploratory study of novice software engineersâ€™ focus in stack overflow posts.",
        "Evaluating large language models trained on code.",
        "Meeting the Challenge of Disruptive Change.",
        "Collaborative virtual environments: an introductory review of issues and systems.",
        "Reusing Code from StackOverflow: The Effect on Technical Debt.",
        "Conversational Bot for Newcomers Onboarding to Open Source Projects.",
        "Selecting Empirical Methods for Software Engineering Research.",
        "Generative AI for Software Practitioners.",
        "How software engineering research aligns with design science: a review.",
        "Social Interactions in Virtual Reality: What Cues Do People Use Most and How.",
        "A Systematic Literature Review of the Metaverse for Software Engineering Education: Overview, Challenges and Opportunities.",
        "Stack Overflow Considered Harmful? The Impact of Copy & Paste on Android Application Security.",
        "\"We Donâ€™t Do That Here\": How Collaborative Editing with Mentors Improves Engagement in Social Q&A Communities.",
        "Paradise unplugged: Identifying barriers for female participation on stack overflow.",
        "Encouraging user behaviour with achievements: An empirical study.",
        "Psychological Affordances Can Provide a Missing Explanatory Layer for Why Interventions to Improve Developer Experience Take Hold or Fail.",
        "Preparing Software Re-Engineering via Freehand Sketches in Virtual Reality.",
        "ShiSha: Enabling Shared Perspective With Face-to-Face Collaboration Using Redirected Avatars in Virtual Reality.",
        "Large Language Models for Software Engineering: A Systematic Literature Review.",
        "Is GitHub Copilot a Substitute for Human Pair-Programming? An Empirical Study.",
        "Optical see-through augmented reality can induce severe motion sickness.",
        "Collaborative program comprehension via software visualization in extended reality.",
        "Current practices, challenges, and design implications for collaborative ar/vr application development.",
        "Social Science Theories in Software Engineering Research.",
        "Towards improving bug tracking systems with game mechanisms.",
        "Why the Arpanet Was Built.",
        "Design Lessons from the Fastest Q&a Site in the West.",
        "Methodology matters: Doing research in the behavioral and social sciences.",
        "Laws of the Media.",
        "The medium is the message.",
        "Understanding stack overflow code quality: A recommendation of caution.",
        "Exploring Research Interest in Stack Overflow - A Systematic Mapping Study and Quality Evaluation.",
        "Signals Matter: Understanding Popularity and Impact of Users on Stack Overflow.",
        "The lever of riches: Technological creativity and economic progress",
        "Community evolution on stack overflow.",
        "Among the Machines: Human-Bot Interaction on Social Q&A Websites.",
        "How are java software developers using the eclipse ide?",
        "Towards discovering the role of emotions in stack overflow.",
        "GPT-4 Technical Report.",
        "Are automated debugging techniques actually helping programmers?.",
        "Crowd documentation: Exploring the coverage and the dynamics of API discussions on Stack Overflow.",
        "The Impact of AI on Developer Productivity: Evidence from GitHub.",
        "Virtual reality goes social.",
        "Challenges of Teaching Programming in StackOverflow Era.",
        "Seahawk: Stack overflow in the ide.",
        "Toxic Code Snippets on Stack Overflow.",
        "ACM SIGSOFT Empirical Standards.",
        "Empirical standards for software engineering research.",
        "The programmerâ€™s assistant: Conversational interaction with a large language model for software development.",
        "Improved retrieval of programming solutions with code examples using a multi-featured score.",
        "Building Theories in Software Engineering .",
        "\" Should We Move to Stack Overflow?\" Measuring the Utility of Social Media for Developer Support.",
        "\"A Bit of Code\": How the Stack Overflow Community Creates Quality Postings.",
        "Uncovering theories in software engineering.",
        "The who, what, how of software engineering research: a socio-technical framework.",
        "Making sense of online code snippets.",
        "On using Stack Overflow comment-edit pairs to recommend code maintenance changes.",
        "Operationalizing McLuhanâ€™s tetrad to focus on innovation effects.",
        "LLaMA: Open and Efficient Foundation Language Models.",
        "How do programmers ask and answer questions on the web?.",
        "Augmenting API documentation with insights from stack overflow.",
        "Understanding Stack Overflow Code Fragments.",
        "StackOverflow and GitHub: Associations between software development and crowdsourced knowledge.",
        "A systematic review of productivity factors in software development.",
        "Experimentation in Software Engineering .",
        "How Do Developers Utilize Source Code from Stack Overflow?",
        "Examining the training and education potential of the metaverse: Results from an empirical study of next generation SAFe training.",
        "Are Code Examples on an Online Q&A Forum Reliable? A Study of API Misuse on Stack Overflow."
      ],
      "meta_data": {
        "arxiv_id": "2402.13329v1",
        "authors": [
          "Margaret-Anne Storey",
          "Daniel Russo",
          "Nicole Novielli",
          "Takashi Kobayashi",
          "Dong Wang"
        ],
        "published_date": "2024-02-20T19:13:36Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes an actionable, socio-technical â€œresearch playbookâ€ to help software engineering (SE) researchers formulate socially relevant research questions about disruptive innovations. The playbook combines (i) McLuhanâ€™s tetrad (enhance/obsolesce/retrieve/reverse) to broaden impact-oriented questioning, (ii) McGrathâ€™s triadic domains (substantive/conceptual/methodological) plus the SE-specific â€œWho-What-Howâ€ strategy taxonomy to select phenomena, constructs, and human-centered empirical strategies. Demonstrates the playbook retrospectively on Stack Overflow and prospectively on generative AI (LLMs for program synthesis) and AR/VR for team collaboration, and introduces a custom GPT (â€œMyResearchPlaybookâ€) to assist brainstorming research-question matrices and candidate methods.",
        "methodology": "Design-science / methodological contribution rather than a new algorithm: the authors synthesize two established theoretical lenses (McLuhan tetrad; McGrath triadic domains) and an SE research-strategy taxonomy (lab/field/respondent/data strategies) into a step-by-step process: (1) apply tetrad to the technology-in-use; (2) choose socio-technical phenomena (units of analysis) and theoretical ideas; (3) generate a matrix of candidate research questions crossing tetrad dimensions with phenomena; (4) select and justify empirical strategies emphasizing direct study of humans/teams. A specialized GPT is configured with instructions to generate/structure the matrix and propose initial strategies aligned with empirical standards, with human refinement encouraged.",
        "experimental_setup": "No new controlled experiments or benchmark evaluations are conducted. Validation is illustrative and qualitative: (a) retrospective walkthrough using Stack Overflow as a prior disruptive technology, grounded in citations to existing empirical studies and a sample question-matrix; (b) forward-looking scenario walkthroughs for LLM-based program synthesis and AR/VR collaboration with example question matrices and high-level method suggestions (e.g., experiments, surveys, ethnography, longitudinal case studies). The custom GPT is demonstrated via an example prompt for AR/VR, with authors manually refining outputs and adding literature references.",
        "limitations": "Primarily conceptual/guide-level contribution with limited empirical validation of effectiveness (e.g., no study showing the playbook improves question quality, societal relevance, or research outcomes). Examples and matrices are partial and depend on authorsâ€™ and GPTâ€™s brainstorming, which may introduce subjectivity and bias. McLuhan tetrad can encourage speculative claims about â€˜reverse/flipâ€™ effects without evidence. The playbook is intentionally high-level and may underspecify operationalization details (measures, instruments, sampling, causal identification). MyResearchPlaybook inherits common LLM constraints: dependence on training data/coverage, potential hallucinations and bias, limited domain depth in niches, and weak support for up-to-date literature retrieval; requires significant human oversight and contextual tailoring.",
        "future_research_directions": "Empirically evaluate the playbookâ€™s utility (e.g., controlled/longitudinal studies comparing teams/researchers using vs. not using it on outcomes such as novelty, socio-technical coverage, actionability, and ethical rigor). Develop operational templates and measurement catalogs for each tetrad dimension and common SE phenomena (developers/teams/projects/communities/society). Apply the playbook to additional disruptive contexts (agentic AI, autonomous DevOps, secure-by-design copilots, regulation/AI governance, accessibility tech) and non-SE domains to test generality. Expand the method layer with explicit guidance for mixed-methods, causal inference, and participatory/values-sensitive design. Improve the GPT adjunct with verifiable citation retrieval (RAG over digital libraries), bias checks, context-specific prompting for different SE subfields, and workflows that keep human judgment central.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Two-stage LLM Fine-tuning with Less Specialization and More Generalization",
      "full_text": "arXiv:2405.02988v1  [math.CA]  5 May 2024 LADDER OPERATORS FOR GENERALIZED ZERNIKE OR DISK POLYNOMIALS MISAEL E. MARRIAGA Abstract. The aim of this work is to report on several ladder operators f or generalized Zernike polynomials which are orthogonal poly nomials on the unit disk D = {(x, y ) âˆˆ R2 : x2 + y2 â©½ 1} with respect to the weight function WÂµ (x, y ) = (1 âˆ’ x2 âˆ’ y2)Âµ where Âµ > âˆ’ 1. These polynomials can be expressed in terms of the univariate Jacobi polynomials and, thus, we s tart by deducing several ladder operators for the Jacobi polynomials. Due to the symmetry of the disk and the weight function WÂµ , it turns out that it is more convenient to use complex variables z = x + iy and Â¯z = x âˆ’ iy. Indeed, this allows us to systematically use the univariate ladder operators to dedu ce analogous ones for the complex generalized Zernike polynomials. Some of th ese univariate and bivariate ladder operators already appear in the literatur e. However, to the best of our knowledge, the proofs presented here are new. Las tly, we illustrate the use of ladder operators in the study of the orthogonal str ucture of some Sobolev spaces. 1. Introduction The so-called Zernike polynomials, originally introduced by Frits Zernik e in 1934 ([20]) to describe the diï¬€raction of the wavefront in the phase cont rast image mi- croscope, are used to characterize higher-order errors obser ved in interferometric analysis in precision optical manufacturing to achieve the desired pe rformance of the system. In ophthalmological practice, the Hartmann-Shack s ensor (or wave- front sensor) is used to determine the refractive errors of the h uman optical system, measuring slopes or normals of the wavefront at diï¬€erent points st arting from the displacement of some luminous points in a target. A systematic metho d of classify- ing forms of aberration is to express the corresponding function in an appropriate basis. Zernike polynomials are recognized as the standard basis of w avefront devel- opments by the Optical Society of America, (OSA). Any suï¬ƒciently r egular phase function deï¬ned on the unit disk can be represented by its Fourier e xpansion in terms of the Zernike polynomials with certain coeï¬ƒcients. The altera tion of these coeï¬ƒcients allows detection of the possible aberrations of the stud ied optical sys- tem. They are also used to describe the aberrations of the cornea or lens from an Date: May 7, 2024. MEM has been supported by the research project [PID2021- 122 154NB-I00], Ortogonalidad y AproximaciÂ´ on con Aplicaciones en Machine Learning y Teor Â´ Ä±a de la Probabilidad funded by MICIU/AEI/10.13039/501100011033 and by â€œERDF A Way of maki ng Europeâ€. The author has also been supported by the Comunidad de Madrid multiannual a greement with the Universidad Rey Juan Carlos under the grant Proyectos I+D para JÂ´ ovenes D octores, Ref. M2731, project NETA-MM. 12 MISAEL E. MARRIAGA ideal spherical shape in optometry and ophthalmology. Finally, they can be eï¬€ec- tively used in adaptive optics to cancel atmospheric distortion, allow ing images to be improved in IR or visual astronomy and satellite images. In [14], an application in quantum optics of the â€œgeneralized Zernike or disk polynomialsâ€ (see, for instance, [16]) is shown, resorting to the Lie a lgebra based investigation of the dynamics of quantum systems driven by two-mo de interaction Hamiltonians. This leads to su(2) and su(1,1) Lie algebraic structures, whose gen- erators, in fact, can appropriately be realized in terms of creation and annihilation operators through raising and lowering operators. Hence, study ing raising and low- ering operators (collectively known as ladder operators), which is o ne the main goals of this paper, is of much interest in the theory of quantum opt ics. From our point of view, Zernike polynomials are polynomials in two variab les which are orthogonal on the unit disk with respect to the Lebesgue measure. They are represented in polar coordinates as a product of a radial polyn omial part times a trigonometric function. The even polynomials are multiples of the co sine, and the odd polynomials are multiples of the sine. Generalized Zernike polyn omials are bivariate polynomials which are orthogonal in the unit disk x2 + y2 â©½ 1 with respect to the weight function (1 âˆ’ x2 âˆ’ y2)Âµ, where Âµ > âˆ’1 is a parameter and the special case Âµ = 0 is equivalent to the usual Zernike polynomials. In turn, these polynomials can be generalized to several variables represented as a product of univariate Jacobi polynomials shifted to the interval [0 , 1] and harmonic polynomials (see [3]). Another way of studying the generalized Zernike polynomials is to exp ress them in one complex variable, for which we identify R2 with C by setting z = x + iy and consider the unit disk as a subset of C. Expressing orthogonal polynomials in a complex variable can be more convenient, even essential, in some cas es, and may result in more elegant formulas and relations as well as simpler proofs . It is worth noting that the use of complex variables instead of cartesian ones t o achieve results connected with orthogonal polynomials is discussed in [19]. In this paper, we use the complex version of the generalized Zernike polyno- mials to show that they satisfy several ladder operators. These p olynomials are represented in terms of univariate Jacobi polynomials and powers o f the complex variable. We take advantage of this fact to deduce the ladder oper ators presented in the sequel and, thus, we also study several ladder operators s atisï¬ed by the Ja- cobi polynomials. We also include a brief discussion about the relevanc e of ladder operators in the study of orthogonal polynomials associated with S obolev inner products, which are inner products involving the derivatives of the polynomials. The outline of this paper is as follows. In Section 2, we provide the bas ic back- ground on bivariate orthogonal polynomials and the generalized Zer nike polynomi- als. Section 3 is concerned with the ladder operators for Jacobi po lynomials. We note that some of these univariate ladder operators already appe ar in the literature. However, to the best of our knowledge, the proofs presented he re are new. We col- lect the ladder operators for the generalized Zernike polynomials in S ection 4 and, in Section 5, we show that fundamental diï¬€erential and recurrenc e relations for the generalized Zernike polynomials can be deduced from the ladder oper ators. We dedicate the last section to discuss how ladder operators can be us ed to study the orthogonal polynomials associated with two Sobolev inner products deï¬ned on theLADDER OPERATORS FOR GENERALIZED ZERNIKE OR DISK POLYNOMIA LS 3 unit disk. These Sobolev orthognal polynomials have been studied pr eviously with- out involving bivariate ladder operators. It turns out that our pro ofs are simpler, which justiï¬es this brief sidenote. 2. Generalized Zernike or disk polynomials In this section, we recall the basic facts about bivariate orthogon al polynomials and the generalized Zernike polynomials that we will need in the sequel. Our main reference is [3]. 2.1. Bivariate orthogonal polynomials. We denote by Î  2 the linear space of real bivariate polynomials. For n â©¾ 0, let Î  2 n denote the linear space of real bivariate polynomials of total degree at most n. Evidentely, dim Î  2 n = ( n + 2 2 ) and Î  2 = â‹ƒ nâ©¾ 0 Î  2 n. We say that a sequence P = {Pn,m(x, y) : n â©¾ 0, 0 â©½ m â©½ n} of polynomials in Î  2 is a polynomial system (PS) if for all n â©¾ 0, the set Pn = {Pn,m(x, y) : 0 â©½ m â©½ n} consists of n + 1 linearly independent polynomials of total degree n, that is, deg Pn,m = n, 0 â©½ m â©½ n. In this way, a PS P is a basis of Î  2. Let âŸ¨Â·, Â·âŸ© : Î  2 Ã— Î  2 â†’ R be an inner product deï¬ned on polynomials. A poly- nomial P of degree n is called an orthogonal polynomial with respect to the inner product if âŸ¨P, QâŸ© = 0 , âˆ€Q âˆˆ Î  2 nâˆ’1. Given an inner product âŸ¨Â·, Â·âŸ© and a PS P = {Pn,m(x, y) : n â©¾ 0, 0 â©½ m â©½ n}, we will say that P is orthogonal with respect to the inner productif âŸ¨Pn,m, QâŸ© = 0 , âˆ€Q âˆˆ Î  2 nâˆ’1, for all n â©¾ 0 and 0 â©½ m â©½ n. Moreover, if âŸ¨Pn,m, Pi,j âŸ© = Hn,m Î´n,i Î´m,j , where Hn,m Ì¸= 0 for n â©¾ 0, then we say that P is a mutually orthogonal polynomial system. Here, Î´n,k denotes the Kronecker delta. Let W (x, y) be a weight function deï¬ned on a domain â„¦ âŠ† R2. That is, W (x, y) > 0 for all ( x, y) âˆˆ â„¦, and â„¦ has a nonempty interior. If an inner product is given by âŸ¨P, QâŸ© = âˆ« âˆ« â„¦ P (x, y) Q(x, y) W (x, y) dxdy, âˆ€P, Q âˆˆ Î  2, we say that the orthogonal polynomials, whenever they exist, are orthogonal with respect to the weight function W . 2.2. Generalized Zernike polynomials. Let D = {(x, y) âˆˆ R2 : x2 + y2 â©½ 1} denote the unit disk in R2. For Âµ > âˆ’1, we deï¬ne the weight function WÂµ(x, y) = (1 âˆ’ x2 âˆ’ y2)Âµ, (x, y) âˆˆ D, and the inner product âŸ¨P, QâŸ©Âµ = bÂµ âˆ« âˆ« D P (x, y) Q(x, y) WÂµ(x, y) dxdy,4 MISAEL E. MARRIAGA where bÂµ = ( âˆ« âˆ« D WÂµ(x, y) dxdy ) âˆ’1 = Âµ + 1 Ï€ . A mutually orthogonal polynomial system on the unit disk can be given in terms of the Jacobi polynomials in the polar coordinates ( x, y) = ( r cos Î¸, r sin Î¸), 0 â©½ r â©½ 1 and 0 â©½ Î¸ â©½ 2Ï€. Recall that the Jacobi polynomial of degree n is given by P (Î±,Î²) n (t) = 1 n! nâˆ‘ k=0 ( n k ) (k + Î± + 1)nâˆ’k(n + Î± + Î² + 1)k ( t âˆ’ 1 2 ) k , where, as usual, (a)0 = 1 , (a)k = a (a + 1) Â· Â· Â· (a + k âˆ’ 1), k â©¾ 1, denotes the Pochhammer symbol. These polynomials are orthogona l with respect to the univariate weight function wÎ±,Î² (t) = (1 âˆ’ t)Î±(1 + t)Î² , Î±, Î² > âˆ’1, t âˆˆ [âˆ’1, 1], and satisfy the diï¬€erential equation LÎ±,Î²[y] â‰¡ (1 âˆ’ t2)yâ€²â€² + (Î² âˆ’ Î± âˆ’ (Î± + Î² + 2) t) yâ€² = âˆ’n (n + Î± + Î² + 1) y. (1) Proposition 2.1 ([3]). For n â©¾ 0 and 0 â©½ j â©½ n 2 , deï¬ne P n,Âµ j,1 (x, y) = P (Âµ,nâˆ’2j) j (2r2 âˆ’ 1) rnâˆ’2j cos(n âˆ’ 2j)Î¸, P n,Âµ j,2 (x, y) = P (Âµ,nâˆ’2j) j (2r2 âˆ’ 1) rnâˆ’2j sin(n âˆ’ 2j)Î¸. (2) Then PÂµ = {P n,Âµ j,1 (x, y), n â©¾ 0, 0 â©½ j â©½ n 2 } âˆª { P Âµ j,2(x, y), n â©¾ 0, 1 â©½ j â©½ n 2 } constitutes a mutually orthogonal polynomial system with respect to WÂµ(x, y). Moreover, âŸ¨P n,Âµ j,Î½ , P m,Âµ k,Î· âŸ©Âµ = Hn,Âµ j Î´n,m Î´j,k Î´Î½,Î·, where Hn,Âµ j = (Âµ + 1)j (n âˆ’ j)! (n âˆ’ j + Âµ + 1) j! (n + 2)nâˆ’j (n + Âµ + 1) { Ã—2, n Ì¸= 2 j, Ã—1, n = 2 j. 2.3. Complex generalized Zernike polynomials. Due to the symmetry of the unit disk, expressing the generalized Zernike polynomials in complex va riables can be more convinient for our study and results in more elegant formula s and relations. For z âˆˆ C we write z = x + iy and, hence, considering the unit disk as a subset of C, we have D = {z âˆˆ C : zÂ¯z â©½ 1}. Deï¬nition 2.2. For k, j â©¾ 0, we deï¬ne QÂµ k,j (z, Â¯z) = j! (Âµ + 1)j zkâˆ’j P (Âµ,kâˆ’j) j (2zÂ¯z âˆ’ 1), k > j. (3) The polynomials (3) are normalized by QÂµ k,j (1, 1) = 1. We note that we can write ([15]) QÂµ k,j (z, Â¯z) = k! (Âµ + 1)k Â¯zjâˆ’k P (Âµ,jâˆ’k) k (2zÂ¯z âˆ’ 1). (4) They constitute a mutually orthogonal system with respect to the weight function wÂµ(z) = (1 âˆ’ zÂ¯z)Âµ, Âµ > âˆ’1, z âˆˆ D,LADDER OPERATORS FOR GENERALIZED ZERNIKE OR DISK POLYNOMIA LS 5 which satisï¬es wÂµ(z) = WÂµ(x, y), hence, wÂµ(z) dz = WÂµ(x, y) dxdy. More pre- cisely, the orthogonality is given by bÂµ âˆ« D QÂµ k,j (z, Â¯z) QÂµ m,â„“(z, Â¯z) wÂµ(z) dz = hÂµ k,j Î´k,m Î´j,â„“, (5) where hÂµ k,j = Âµ + 1 Âµ + k + j + 1 k! j! (Âµ + 1)k (Âµ + 1)j . (6) By (3), Re{QÂµ nâˆ’j,j(z, Â¯z)} = j! (Âµ + 1)j P n,Âµ j,1 (x, y), 0 â©½ j â©½ n 2 , Im{QÂµ nâˆ’j,j(z, Â¯z)} = j! (Âµ + 1)j P n,Âµ j,2 (x, y), 0 â©½ j â©½ n 2 , which are, up to a constant, the orthogonal polynomials of two rea l variables deï¬ned in (2). We must note that the polynomials QÂµ k,j (z, Â¯z) are invariant under the simulta- neous permutations of the variables {z, Â¯z} and the subindices {k, j }, that is, QÂµ k,j (z, Â¯z) = QÂµ j,k(Â¯z, z ). (7) This invariance will permeate the results presented in the sequel. 3. Ladder operators for Jacobi polynomials Throughout this section, we will introduce pairs of related ladder op erators for the univariate Jacobi polynomials and, in the next section, we use th em to deduce ladder operators for the complex generalized Zernike polynomials. S ome of these univariate and bivariate ladder operators already appear in the liter ature. However, to the best of our knowledge, the proofs presented here are new . Deï¬ne the operators A1[u] = du dt and A2[u] = (1 âˆ’ t2)du dt + [Î² (1 âˆ’ t) âˆ’ Î± (1 + t)] u. The action of these operators over the Jacobi polynomials is well kn own ([10, 18.9.15, 18.9.16]). The classical proof for the following result is based on the or- thogonality of the Jacobi polynomials. However, we provide an oper ational proof. Proposition 3.1. The univariate Jacobi polynomials satisfy A1[P (Î±,Î²) n (t)] = 1 2(n + Î± + Î² + 1) P (Î±+1,Î²+1) nâˆ’1 (t), Î±, Î² > âˆ’1, A2[P (Î±,Î²) n (t)] = âˆ’2(n + 1) P (Î±âˆ’1,Î²âˆ’1) n+1 (t), Î±, Î² > 0. (8) Proof. Using the diï¬€erential operator LÎ±,Î² deï¬ned in (1), we compute LÎ±,Î²A1[u] âˆ’ A1LÎ±,Î²[u] = ( 2t d dt + Î± + Î² + 2 ) A1[u], therefore, LÎ±+1,Î²+1A1[u] = A1 [ LÎ±,Î²[u] + ( Î± + Î² + 2) u ] . Letting u = P (Î±,Î²) n (t) and using (1), we get LÎ±+1,Î²+1A1[P (Î±,Î²) n (t)] = A1LÎ±,Î²[P (Î±,Î²) n (t)] + ( Î± + Î² + 2)A1[P (Î±,Î²) n (t)]6 MISAEL E. MARRIAGA = ( âˆ’n (n + Î± + Î² + 1) + Î± + Î² + 2) A1[P (Î±,Î²) n (t)] = âˆ’(n âˆ’ 1)(n + Î± + Î² + 2) A1[P (Î±,Î²) n (t)], which implies that A1[P (Î±,Î²) n (t)] = cÎ±,Î² n P (Î±+1,Î²+1) nâˆ’1 (t) for some constant cÎ±,Î² n . From the explicit expression of the Jacobi polynomials and comparing the le ading coeï¬ƒ- cients on both sides of this last equation, we get cÎ±,Î² n = 1 2 (n + Î± + Î² + 1). Similarly, LÎ±,Î² A2[u] âˆ’ A2LÎ±,Î²[u] = ( 2t d dt âˆ’ Î± âˆ’ Î² ) A2[u], or, equivalently, LÎ±âˆ’1,Î²âˆ’1A2[u] = A2 [ LÎ±,Î²[u] âˆ’ (Î± + Î²) u ] . Thus, LÎ±âˆ’1,Î²âˆ’1A2[P (Î±,Î²) n (t)] = ( âˆ’n (n + Î± + Î² + 1) âˆ’ Î± âˆ’ Î²) A2[P (Î±,Î²) n (t)] = âˆ’(n + 1)(n + Î± + Î²) A2[P (Î±,Î²) n (t)], which implies that A2[P (Î±,Î²) n (t)] = dÎ±,Î² n P (Î±âˆ’1,Î²âˆ’1) n+1 (t) for some constant dÎ±,Î² n . In order to deduce the value of dÎ±,Î² n , we note that we can write A2[u] = âˆ’(t âˆ’ 1)2 du dt âˆ’ (Î± + Î²)(t âˆ’ 1) u âˆ’ 2(t âˆ’ 1)du dt âˆ’ Î± u. Using this last identity and the explicit expression of the Jacobi polyn omials, we compare leading coeï¬ƒcients on both sides of A2[P (Î±,Î²) n (t)] = dÎ±,Î² n P (Î±âˆ’1,Î²âˆ’1) n+1 (t) and deduce that dÎ±,Î² n = âˆ’2 (n + 1). â–¡ For n â©¾ 0, let B1[u] = (1+ t) du dt +(n+Î±+Î² +1) u and B2[u] = (1 âˆ’t2)du dt âˆ’[2Î±+(1 âˆ’t) n] u. We note that the coeï¬ƒcients of these operators are not ï¬xed and depend on n. Proposition 3.2. For Î² > âˆ’1, the univariate Jacobi polynomials satisfy B1[P (Î±,Î²) n (t)] = ( n + Î± + Î² + 1) P (Î±+1,Î²) n (t), Î± > âˆ’1, B2[P (Î±,Î²) n (t)] = âˆ’2(n + Î±) P (Î±âˆ’1,Î²) n (t), Î± > 0. (9) Proof. First, we note that we can write LÎ±,Î²[u] = [2(1 + t) âˆ’ (t + 1)2]d2u dt2 + [2(Î² + 1) âˆ’ (Î± + Î² + 2)(1 + t)]du dt = 2(1 + t)d2u dt2 + 2(Î² + 1) du dt âˆ’ { (1 + t) d dt âˆ’ n } B1[u] âˆ’ n(n + Î± + Î² + 1) u. From here, if we commute LÎ±,Î² with B1, we get LÎ±,Î²B1[u] âˆ’ B1LÎ±,Î²[u] = 2(1 + t)d2u dt2 + 2(Î² + 1) du dtLADDER OPERATORS FOR GENERALIZED ZERNIKE OR DISK POLYNOMIA LS 7 = { (1 + t) d dt âˆ’ n } B1[u] + LÎ±,Î² [u] + n(n + Î± + Î² + 1) u, or, equivalently, LÎ±+1,Î² B1[u] = B1 ( LÎ±,Î²[u] âˆ’ n u ) + LÎ±,Î²[u] + n(n + Î± + Î² + 1) u. From (1), it follows that LÎ±+1,Î²B1[P (Î±,Î²) n (t)] = âˆ’n(n + Î± + Î² + 2) B1[P (Î±,Î²) n (t)], which implies that B1[P (Î±,Î²) n (t)] = cÎ±,Î² n P (Î±+1,Î²) n (t). Writing, B1[u] = ( t âˆ’ 1)du dt + (n + Î± + Î² + 1) u + 2 du dt , we can compare leading coeï¬ƒcients and obtain cÎ±,Î² n = n + Î± + Î² + 1. Notice that we can write B2[u] = âˆ’(t âˆ’ 1)2 du dt + n (t âˆ’ 1) u âˆ’ 2(t âˆ’ 1)du dt âˆ’ 2 Î± u. Hence, B2 [( t âˆ’ 1 2 ) n] = âˆ’2(n + Î±) ( t âˆ’ 1 2 ) n , which implies that B2[P (Î±,Î²) n (t)] is a polynomial of degree n. By the orthogonality of {P (Î±âˆ’1,Î²) n (t)}nâ©¾ 0 and comparing leading coeï¬ƒcients, we have B2[P (Î±,Î²) n (t)] = âˆ’2(n + Î±)P (Î±âˆ’1,Î²) n (t) + nâˆ’1âˆ‘ k=0 dÎ±,Î² k P (Î±âˆ’1,Î²) k (t), where dÎ±,Î² k hÎ±âˆ’1,Î² k = âˆ« 1 âˆ’1 B2[P (Î±,Î²) n (t)] P (Î±âˆ’1,Î²) k (t) wÎ±âˆ’1,Î² (t) dt, 0 â©½ k â©½ n âˆ’ 1, with hÎ±âˆ’1,Î² k = âˆ« 1 âˆ’1 ( P (Î±âˆ’1,Î²) k (t) ) 2 wÎ±âˆ’1,Î² (t) dt, which is non zero for Î± âˆ’ 1, Î² > âˆ’1 (see [13]). After integrating by parts and observing that p(t)wÎ±,Î² (t)|1 âˆ’1 = 0 for every polynomial p(t), we obtain âˆ« 1 âˆ’1 B2[P (Î±,Î²) n (t)] P (Î±âˆ’1,Î²) k (t) wÎ±âˆ’1,Î² (t) dt = âˆ’ âˆ« 1 âˆ’1 P (Î±,Î²) n (t) B1[P (Î±âˆ’1,Î²) k (t)] wÎ±,Î² (t) dt, and, therefore, dÎ±,Î² k hÎ±âˆ’1,Î² k = âˆ’ âˆ« 1 âˆ’1 P (Î±,Î²) n (t) B1[P (Î±âˆ’1,Î²) k (t)] wÎ±,Î² (t) dt. But B1[P (Î±âˆ’1,Î²) k (t)] is a polynomial of degree at most k. From the orthogonality of the Jacobi polynomials, we deduce that dÎ±,Î² k = 0 for 0 â©½ k â©½ n âˆ’ 1, and (9) is now proved. â–¡8 MISAEL E. MARRIAGA Notice that in (9), only the ï¬rst parameter of the Jacobi polynomia ls is shifted by one unit. Recall that the univariate classical Jacobi polynomials follo wing reï¬‚ection formula P (Î±,Î²) n (t) = ( âˆ’1)n P (Î²,Î±) n (âˆ’t). (10) Using (10), it is straightforward to verify that, for n â©¾ 0, the operators deï¬ned as C1[u] = (1 âˆ’t) du dt âˆ’(n+Î±+Î² +1) u and C2[u] = (1 âˆ’t2)du dt +[2Î² +(1+ t) n] u, act by shifting only the second parameter. Proposition 3.3. For Î± > âˆ’1, the univariate Jacobi polynomials satisfy C1[P (Î±,Î²) n (t)] = âˆ’(n + Î± + Î² + 1) P (Î±,Î²+1) n (t), Î² > âˆ’1, C2[P (Î±,Î²) n (t)] = 2( n + Î²) P (Î±,Î²âˆ’1) n (t), Î² > 0. (11) On one hand, the operators in (9) and (11) shift one parameter an d leave the degree of the polynomial unchanged. In contrast, the following un ivariate operators deï¬ned for n â©¾ 0 as D1[u] = (1+ t) du dt âˆ’n u and D2[u] = (1 âˆ’t2)du dt +[(n+Î²+1)(1 âˆ’t)âˆ’Î± (1+ t)] u, shift the parameter Î± as well as the degree of the polynomial. Proposition 3.4. For Î² > âˆ’1, the univariate Jacobi polynomials satisfy D1[P (Î±,Î²) n (t)] = ( n + Î²) P (Î±+1,Î²) nâˆ’1 (t), Î± > âˆ’1, D2[P (Î±,Î²) n (t)] = âˆ’2(n + 1) P (Î±âˆ’1,Î²) n+1 (t), Î± > 0. (12) Proof. Observe that D1[u] = B1[u] âˆ’ (2n + Î± + Î² + 1) u. Therefore, LÎ±,Î²D1[u] âˆ’ D1LÎ±,Î²[u] = LÎ±,Î² B1[u] âˆ’ B1LÎ±,Î²[u] = { (1 + t) d dt + n + Î± + Î² + 1 } D1[u] + LÎ±,Î² [u] + n(n + Î± + Î² + 1) u or, equivalently, LÎ±+1,Î²D1[u] = D1 ( LÎ±,Î²[u] + ( n + Î± + Î² + 1) u ) + LÎ±,Î² [u] + n(n + Î± + Î² + 1) u. From (1), it follows that LÎ±+1,Î²D1[P (Î±,Î²) n (t)] = âˆ’(n âˆ’ 1) (n + Î± + Î² + 1) D1[P (Î±,Î²) n (t)], which implies that D1[P (Î±,Î²) n (t)] = cÎ±,Î² n P (Î±+1,Î²) nâˆ’1 (t). Writing, D1[u] = ( t âˆ’ 1)du dt âˆ’ n u + 2 du dt , (13) we can compare leading coeï¬ƒcients and obtain cÎ±,Î² n = n + Î². Moreover, notice that we can write D2[u] = âˆ’(t âˆ’ 1)2 du dt âˆ’ (n + Î± + Î² + 1) ( t âˆ’ 1) u âˆ’ 2(t âˆ’ 1)du dt âˆ’ 2 Î± u.LADDER OPERATORS FOR GENERALIZED ZERNIKE OR DISK POLYNOMIA LS 9 Hence, D2 [( t âˆ’ 1 2 ) n] = âˆ’2(2n + Î± + Î² + 1) ( t âˆ’ 1 2 ) n+1 âˆ’ 2(n + Î±) ( t âˆ’ 1 2 ) n , which implies that D2[P (Î±,Î²) n (t)] is a polynomial of degree n + 1. By the orthogo- nality of {P (Î±âˆ’1,Î²) n (t)}nâ©¾ 0 and comparing leading coeï¬ƒcients, we have D2[P (Î±,Î²) n (t)] = âˆ’2(n + 1)P (Î±âˆ’1,Î²) n+1 (t) + nâˆ‘ k=0 dÎ±,Î² k P (Î±âˆ’1,Î²) k (t), where dÎ±,Î² k hÎ±âˆ’1,Î² k = âˆ« 1 âˆ’1 D2[P (Î±,Î²) n (t)] P (Î±âˆ’1,Î²) k (t) wÎ±âˆ’1,Î² (t) dt, 0 â©½ k â©½ n. Integrating by parts as in the proof of Proposition 3.2, we obtain dÎ±,Î² k hÎ±âˆ’1,Î² k = âˆ’ âˆ« 1 âˆ’1 P (Î±,Î²) n (t) D1[P (Î±âˆ’1,Î²) k (t)] wÎ±,Î² (t) dt. On one hand, D1[P (Î±âˆ’1,Î²) k (t)] is a polynomial of degree at most k for 0 â©½ k â©½ nâˆ’1. From the orthogonality of the Jacobi polynomials, we deduce that dÎ±,Î² k = 0 for 0 â©½ k â©½ n âˆ’ 1. On the other hand, by (13), we have D1 [( t âˆ’ 1 2 ) n] = n ( t âˆ’ 1 2 ) nâˆ’1 . Therefore, D1[P (Î±âˆ’1,Î²) n (t)] is a polynomial of degree at most n âˆ’ 1 and, by orthog- onality, dÎ±,Î² n = 0. This proves (12). â–¡ Using (12) and (10), we can show that the operators deï¬ned for n â©¾ 0 as E1[u] = (1 âˆ’t)du dt +n u and E2[u] = (1 âˆ’t2)du dt +[(n+Î±+1)(1+ t)âˆ’Î²(1âˆ’t)] u, shift the parameter Î² as well as the degree of the Jacobi polynomials. Proposition 3.5. For Î± > âˆ’1, the univariate Jacobi polynomials satisfy E1[P (Î±,Î²) n (t)] = ( n + Î±) P (Î±,Î²+1) nâˆ’1 (t), Î² > âˆ’1, E2[P (Î±,Î²) n (t)] = âˆ’2(n + 1) P (Î±,Î²âˆ’1) n+1 (t), Î² > 0. (14) We will now ï¬nish our list of ladder operators for Jacobi polynomials. W e deï¬ne the two operators with ï¬xed coeï¬ƒcients: F1[u] = (1 + t)du dt + Î² u and F2[u] = (1 âˆ’ t)du dt âˆ’ Î± u. Proposition 3.6. The univariate Jacobi polynomials satisfy F1[P (Î±,Î²) n (t)] = ( n + Î²) P (Î±+1,Î²âˆ’1) n (t), Î±, Î² âˆ’ 1 > âˆ’1, F2[P (Î±,Î²) n (t)] = âˆ’(n + Î±) P (Î±âˆ’1,Î²+1) n (t), Î± âˆ’ 1, Î² > âˆ’1. (15)10 MISAEL E. MARRIAGA Proof. We compute LÎ±,Î²F1[u] âˆ’ F1LÎ±,Î²[u] = 2 d dtF1[u], or, equivalently, LÎ±+1,Î²âˆ’1F1[u] = F1LÎ±,Î²[u]. From here together with (7), we deduce (15). â–¡ 4. Ladder operators for Zernike polynomials In this section, we derive ladder operators for the generalized Zer nike polynomials by directly employing ladder operators satisï¬ed by the univariate Ja cobi polyno- mials. We give several ladder operators that shift the parameter a nd degree of the complex generalized Zernike polynomials by at most one unit, that is, e ach ladder operator maps QÂµ k,j (z, Â¯z) to QÎ· m,â„“(z, Â¯z) with |Âµ âˆ’ Î·| â©½ 1, |k âˆ’ m| â©½ 1, |j âˆ’ â„“| â©½ 1. The ï¬rst two ladder operators for the complex generalized Zernike polynomials are deduced in the following theorem. Theorem 4.1. The complex generalized Zernike polynomials satisfy âˆ‚ âˆ‚z QÂµ k,j (z, Â¯z) = k (j + Âµ + 1) Âµ + 1 QÂµ+1 kâˆ’1,j (z, Â¯z), Âµ > âˆ’1, { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚Â¯z âˆ’ Âµ z } QÂµ k,j (z, Â¯z) = âˆ’Âµ QÂµâˆ’1 k+1,j (z, Â¯z), Âµ > 0. (16) Proof. Using (4), we compute âˆ‚ âˆ‚z QÂµ k,j (z, Â¯z) = 2 k! (Âµ + 1)k Â¯zjâˆ’k+1 ( P (Âµ,jâˆ’k) k ) â€² (2zÂ¯z âˆ’ 1). Notice that if we set t = 2 r âˆ’ 1 where r = zÂ¯z, then A1[P (Âµ,jâˆ’k) k (t)] = 2 ( P (Âµ,jâˆ’k) k ) â€² (2r âˆ’ 1). Then, from (8), we get âˆ‚ âˆ‚z QÂµ k,j (z, Â¯z) = ( j + Âµ + 1) k! (Âµ + 1)k Â¯zjâˆ’k+1 P (Âµ+1,jâˆ’k+1) kâˆ’1 (2zÂ¯z âˆ’ 1) = k (j + Âµ + 1) Âµ + 1 QÂµ+1 kâˆ’1,j (z, Â¯z), and we obtain the ï¬rst identity in (16). Using (4) again, we compute { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚Â¯z âˆ’ Âµ z } QÂµ k,j (z, Â¯z) = k! Â¯zjâˆ’kâˆ’1 (Âµ + 1)k [ 2(1 âˆ’ zÂ¯z)zÂ¯z ( P (Âµ,jâˆ’k) k ) â€² + [(j âˆ’ k)(1 âˆ’ zÂ¯z) âˆ’ Âµ z Â¯z] P (Âµ,jâˆ’k) k ] (2zÂ¯z âˆ’ 1).LADDER OPERATORS FOR GENERALIZED ZERNIKE OR DISK POLYNOMIA LS 11 The expression in brackets is precisely 1 2 A2[P (Âµ,jâˆ’k) k (t)] under the same change of variable as before. Then, from (8) it follows that { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚z âˆ’ Âµ z } QÂµ k,j (z, Â¯z) = 1 2 k! (Âµ + 1)k Â¯zjâˆ’kâˆ’1A2[P (Âµ,jâˆ’k) k ](2zÂ¯z âˆ’ 1) = âˆ’ (k + 1)! (Âµ + 1)k Â¯zjâˆ’kâˆ’1P (Âµâˆ’1,jâˆ’kâˆ’1) k+1 (2zÂ¯z âˆ’ 1) = âˆ’Âµ QÂµâˆ’1 k+1,j (z, Â¯z), and the second identity in (16) follows. We remark that this result can also be obtained by computing with (3) and using F1 and F2 instead of A1 and A2, respectively. We omit the details. â–¡ The following corollary is a consequence of the previous theorem and (7). Corollary 4.2. The complex generalized Zernike polynomials satisfy âˆ‚ âˆ‚Â¯z QÂµ k,j (z, Â¯z) = j (k + Âµ + 1) Âµ + 1 QÂµ+1 k,jâˆ’1(z, Â¯z), Âµ > âˆ’1, { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚Â¯z âˆ’ Âµ Â¯z } QÂµ k,j (z, Â¯z) = âˆ’Âµ QÂµâˆ’1 k,j+1(z, Â¯z), Âµ > 0. (17) The ladder operators in the following theorem shift the parameter Âµ by one unit, but keep the other two paramenter k and j unchanged and, hence, the total degree of the polynomials is preserved under the action of these operator s. Theorem 4.3. The complex generalized Zernike polynomials satisfy { z âˆ‚ âˆ‚z + j + Âµ + 1 } QÂµ k,j (z, Â¯z) = (k + Âµ + 1) ( j + Âµ + 1) Âµ + 1 QÂµ+1 k,j (z, Â¯z), Âµ > âˆ’1, { (1 âˆ’ zÂ¯z) z âˆ‚ âˆ‚z âˆ’ k (1 âˆ’ zÂ¯z) âˆ’ Âµ } QÂµ k,j (z, Â¯z) = âˆ’Âµ QÂµâˆ’1 k,j (z, Â¯z), Âµ > 0. (18) Proof. Using (3), we compute { z âˆ‚ âˆ‚z + j + Âµ + 1 } QÂµ k,j (z, Â¯z) = j! (Âµ + 1)j zkâˆ’j [ 2zÂ¯z ( P (Âµ,kâˆ’j) j ) â€² + (k + Âµ + 1)P (Âµ,kâˆ’j) j ] (2zÂ¯z âˆ’ 1), where the expression in brackets is B1[P (Âµ,kâˆ’j) j (t)] under the change of variables t = 2 r âˆ’ 1 where r = zÂ¯z. By (9), we have { z âˆ‚ âˆ‚z + j + Âµ + 1 } QÂµ k,j (z, Â¯z) = j! (Âµ + 1)j (k + Âµ + 1)zkâˆ’j P (Âµ+1,kâˆ’j) j (2zÂ¯z âˆ’ 1), = (k + Âµ + 1) ( j + Âµ + 1) Âµ + 1 QÂµ+1 k,j (z, Â¯z).12 MISAEL E. MARRIAGA Now, we compute{ (1 âˆ’ zÂ¯z) z âˆ‚ âˆ‚z âˆ’ k (1 âˆ’ zÂ¯z) âˆ’ Âµ } QÂµ k,j (z, Â¯z) = j! (Âµ + 1)j zkâˆ’j [ 2(1 âˆ’ zÂ¯z)zÂ¯z ( P (Âµ,kâˆ’j) j ) â€² âˆ’ ( j (1 âˆ’ zÂ¯z) + Î± ) P (Âµ,kâˆ’j) j ] (2zÂ¯z âˆ’ 1). The expression in brackets is 1 2 B2[P (Âµ,kâˆ’j) j (t)] under the same change of variable as before. Hence, from (9), we obtain { (1 âˆ’ zÂ¯z) z âˆ‚ âˆ‚z âˆ’ k (1 âˆ’ zÂ¯z) âˆ’ Âµ } QÂµ k,j (z, Â¯z) = âˆ’(j + Î±) j! (Âµ + 1)j zkâˆ’j P (Âµâˆ’1,kâˆ’j) j (2zÂ¯z âˆ’ 1) = âˆ’Âµ QÂµâˆ’1 k,j (z, Â¯z). We have now proved both equations in (18). â–¡ Corollary 4.4. The complex generalized Zernike polynomials satisfy{ Â¯z âˆ‚ âˆ‚Â¯z + k + Âµ + 1 } QÂµ k,j (z, Â¯z) = (k + Âµ + 1) ( j + Âµ + 1) Âµ + 1 QÂµ+1 k,j (z, Â¯z), Âµ > âˆ’1, { (1 âˆ’ zÂ¯z) Â¯z âˆ‚ âˆ‚Â¯z âˆ’ j (1 âˆ’ zÂ¯z) âˆ’ Âµ } QÂµ k,j (z, Â¯z) = âˆ’Âµ QÂµâˆ’1 k,j (z, Â¯z), Âµ > 0. (19) In contrast to the previous ladder operators, the follwing ones on ly shift the parameters k and j, and leave Âµ unchanged. Theorem 4.5. For Âµ > âˆ’1, the complex generalized Zernike polynomials satisfy{ (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚z + kÂ¯z } QÂµ k,j (z, Â¯z) = k QÂµ kâˆ’1,j (z, Â¯z), { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚Â¯z âˆ’ (k + Âµ + 1)z } QÂµ k,j (z, Â¯z) = âˆ’(k + Âµ + 1) QÂµ k+1,j(z, Â¯z). (20) Proof. From the representation (3), we get { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚Â¯z âˆ’ (k + Âµ + 1)z } QÂµ k,j (z, Â¯z) = j! (Âµ + 1)j zkâˆ’j+1 [ 2(1 âˆ’ zÂ¯z) ( P (Âµ,kâˆ’j) j ) â€² âˆ’ (k + Âµ + 1)P (Âµ,kâˆ’j) j ] (2zÂ¯z âˆ’ 1). Since the expression in brackets coincides with C1[P (Âµ,kâˆ’j) j (t)] where t = 2 r âˆ’1 and r = zÂ¯z, it follows from (11) that { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚Â¯z âˆ’ (k + Âµ + 1)z } QÂµ k,j (z, Â¯z) = âˆ’(k + Âµ + 1) j! (Âµ + 1)j zkâˆ’j+1P (Âµ,kâˆ’j+1) j (2zÂ¯z âˆ’ 1) = âˆ’(k + Âµ + 1) QÂµ k+1,j (z, Â¯z).LADDER OPERATORS FOR GENERALIZED ZERNIKE OR DISK POLYNOMIA LS 13 Similarly, we compute { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚z + kÂ¯z } QÂµ k,j (z, Â¯z) = j! (Âµ + 1)j zkâˆ’jâˆ’1 [ 2(1 âˆ’ zÂ¯z)zÂ¯z ( P (Âµ,kâˆ’j) j ) â€² + (jz Â¯z + k âˆ’ j)P (Âµ,kâˆ’j) j ] (2zÂ¯z âˆ’ 1). Since the expression in brackets coincides with 1 2 C2[P (Âµ,kâˆ’j) j (t)] under the same change of variables as before, it follows from (11) that { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚z + kÂ¯z } QÂµ k,j (z, Â¯z) = k j ! (Âµ + 1)j zkâˆ’jâˆ’1P (Âµ,kâˆ’jâˆ’1) j (2zÂ¯z âˆ’ 1) = k QÂµ kâˆ’1,j (z, Â¯z). This proves (20). This result can also be obtained by computing with ( 4) and using E2 and E1 instead of C1 and C2, respectively. We omit the details. â–¡ Corollary 4.6. For Âµ > âˆ’1, the complex generalized Zernike polynomials satisfy { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚Â¯z + jz } QÂµ k,j (z, Â¯z) = j QÂµ k,jâˆ’1(z, Â¯z), { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚z âˆ’ (j + Âµ + 1)Â¯z } QÂµ k,j (z, Â¯z) = âˆ’(j + Âµ + 1) QÂµ k,j+1(z, Â¯z). (21) The following ladder operators shift all three parameters by one un it. In partic- ular, both parameters k and j are shifted by the same amount and, consequently, the total degree of the polynomials is shifted by two units. Theorem 4.7. The complex generalized Zernike polynomials satisfy { z âˆ‚ âˆ‚z âˆ’ k } QÂµ k,j (z, Â¯z) = k j Âµ + 1 QÂµ+1 kâˆ’1,jâˆ’1(z, Â¯z), { (1 âˆ’ zÂ¯z)z âˆ‚ âˆ‚z + (j + 1)(1 âˆ’ zÂ¯z) âˆ’ ÂµzÂ¯z } QÂµ k,j (z, Â¯z) = âˆ’Âµ QÂµâˆ’1 k+1,j+1(z, Â¯z). (22) Proof. Observe that under the change of variable t = 2 zÂ¯z âˆ’ 1, we have D1[P (Âµ,jâˆ’k) k (t)] = { 2zÂ¯z ( P (Âµ,jâˆ’k) k ) â€² âˆ’ k P (Âµ,jâˆ’k) k } (2zÂ¯z âˆ’ 1), D2[P (Âµ,kâˆ’j) j (t)] = { 4(1 âˆ’ zÂ¯z)zÂ¯z ( P (Âµ,kâˆ’j) j ) â€² + 2[(k + 1)(1 âˆ’ zÂ¯z) âˆ’ Âµ z Â¯z]P (Âµ,kâˆ’j) j } (2zÂ¯z âˆ’ 1). From here, we get { z âˆ‚ âˆ‚z âˆ’ k } QÂµ k,j (z, Â¯z) = k! (Âµ + 1)k Â¯zjâˆ’kD1[P (Âµ,jâˆ’k) k ](2zÂ¯z âˆ’ 1) = j k! (Âµ + 1)k Â¯zjâˆ’kP (Âµ+1,jâˆ’k) kâˆ’1 (2zÂ¯z âˆ’ 1)14 MISAEL E. MARRIAGA = k j Âµ + 1 QÂµ+1 kâˆ’1,jâˆ’1(z, Â¯z), and { (1 âˆ’ zÂ¯z)z âˆ‚ âˆ‚z + (j + 1)(1 âˆ’ zÂ¯z) âˆ’ ÂµzÂ¯z } QÂµ k,j (z, Â¯z) = 1 2 j! (Âµ + 1)j zkâˆ’j D2[P (Âµ,kâˆ’j) j ](2zÂ¯z âˆ’ 1) = âˆ’(j + 1) j! (Âµ + 1)j zkâˆ’jP (Âµâˆ’1,kâˆ’j) j+1 (2zÂ¯z âˆ’ 1) = âˆ’Âµ QÂµâˆ’1 k+1,j+1(z, Â¯z), which proves (22). â–¡ Corollary 4.8. For Âµ > âˆ’1, the complex generalized Zernike polynomials satisfy { Â¯z âˆ‚ âˆ‚Â¯z âˆ’ j } QÂµ k,j (z, Â¯z) = k j Âµ + 1 QÂµ+1 kâˆ’1,jâˆ’1(z, Â¯z), { (1 âˆ’ zÂ¯z)Â¯z âˆ‚ âˆ‚Â¯z + (k + 1)(1 âˆ’ zÂ¯z) âˆ’ ÂµzÂ¯z } QÂµ k,j (z, Â¯z) = âˆ’Âµ QÂµâˆ’1 k+1,j+1(z, Â¯z). (23) QÂµ k,j (z, z) â†’ QÂµâˆ’1 m,â„“ (z, z) k k + 1 k âˆ’ 1 j âˆ’ 1 j j + 1 (16) (17) (18) (19) (22) (23) QÂµ k,j (z, z) â†’ QÂµ m,â„“(z, z) k k + 1 k âˆ’ 1 j âˆ’ 1 j j + 1 (20) (21) (20) (21) QÂµ k,j (z, z) â†’ QÂµ+1 m,â„“ (z, z) k k + 1 k âˆ’ 1 j âˆ’ 1 j j + 1 (18) (16) (17) (19) (22) (23) Figure 1. Illustration of how the ladder operators increase or decrease the parameters in QÂµ k,j (z, Â¯z). 5. Differential and recurrence relations for QÂµ k,j (z, Â¯z) We can combine the ladder operators introduced in the previous sec tion to deduce diï¬€erential and recurrence relations satisï¬ed by the complex gene ralized Zernike polynomials. Some of the relations involve polynomials with diï¬€erent par ameters. First, we deï¬ne the following diï¬€erential operator LÂµ = 2(1 âˆ’ zÂ¯z) âˆ‚2 âˆ‚z âˆ‚ Â¯z âˆ’ (Âµ + 1) ( z âˆ‚ âˆ‚z + Â¯z âˆ‚ âˆ‚Â¯z ) .LADDER OPERATORS FOR GENERALIZED ZERNIKE OR DISK POLYNOMIA LS 15 Proposition 5.1. The complex generalized Zernike polynomials satisfy LÂµQÂµ k,j (z, Â¯z) = Î»Âµ k,j QÂµ k,j (z, Â¯z), where Î»Âµ k,j = âˆ’2kj âˆ’ (Âµ + 1)(k + j). Proof. Observe that we can write LÂµ = { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚Â¯z âˆ’ (Âµ + 1)z } âˆ‚ âˆ‚z + { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚z âˆ’ (Âµ + 1)Â¯z } âˆ‚ âˆ‚Â¯z . Then, the result follows from (16) and (17). â–¡ Notice that the diï¬€erential equation LÂµP = Î»Âµ P makes sense for Âµ = âˆ’1. In fact, its polynomial solutions can be given in terms of the complex g eneralized Zernike polynomials ([12]). However, these polynomials solutions are n ot standard orthogonal polynomials ([4]). Proposition 5.2. For k, j â©¾ 0, deï¬ne Qâˆ’1 0,0(z, Â¯z) = 1 , Q âˆ’1 k,j (z, Â¯z) = (1 âˆ’ zÂ¯z) Q1 kâˆ’1,jâˆ’1(z, Â¯z), k, j â©¾ 1. (24) Then {Qâˆ’1 k,j (z, Â¯z)}k,jâ©¾ 0 satisfy Lâˆ’1Qâˆ’1 k,j (z, Â¯z) = Î»âˆ’1 k,j Qâˆ’1 k,j (z, Â¯z), with Î»âˆ’1 k,j = âˆ’2kj. Proof. Note that Lâˆ’1 = 2(1 âˆ’ zÂ¯z) âˆ‚2 âˆ‚z âˆ‚ Â¯z . Clearly, since Î»âˆ’1 0,0 = 0, we have Lâˆ’1Qâˆ’1 0,0(z, Â¯z) = Î»âˆ’1 0,0Qâˆ’1 0,0(z, Â¯z). For k, j â©¾ 1, we have Lâˆ’1Qâˆ’1 k,j (z, Â¯z) = 2(1 âˆ’ zÂ¯z) âˆ‚2 âˆ‚z âˆ‚ Â¯z (1 âˆ’ zÂ¯z)Q1 kâˆ’1,jâˆ’1(z, Â¯z) = (1 âˆ’ zÂ¯z) { 2(1 âˆ’ zÂ¯z) âˆ‚2 âˆ‚z âˆ‚ Â¯z âˆ’ 2 ( z âˆ‚ âˆ‚z + Â¯z âˆ‚ âˆ‚Â¯z ) âˆ’ 2 } Q1 kâˆ’1,jâˆ’1(z, Â¯z) = (1 âˆ’ zÂ¯z) {L1 âˆ’ 2} Q1 kâˆ’1,jâˆ’1(z, Â¯z). From Porposition 5.1, we get Lâˆ’1Qâˆ’1 k,j (z, Â¯z) = ( Î»1 kâˆ’1,jâˆ’1 âˆ’ 2)Qâˆ’1 k,j (z, Â¯z) = Î»âˆ’1 k,j Qâˆ’1 k,j (z, Â¯z), which proves the announced result. â–¡ We remark that it was shown in [12] that if Âµ â©½ âˆ’2 is an integer, then LÂµ is not guaranteed to have a comples polynomial system solution (see also R emark 6.1 in [4]). The basic three term relations for multivariate orthogonal polynom ials ([3]) can be deduced for the complex generalized Zernike polynomials. Proposition 5.3. The complex generalized Zernike polynomials satisfy the follow- ing three term relations (k + j + Âµ + 1)z QÂµ k,j (z, Â¯z) = ( k + Âµ + 1) QÂµ k+1,j (z, Â¯z) + j QÂµ k,jâˆ’1(z, Â¯z),16 MISAEL E. MARRIAGA (k + j + Âµ + 1)Â¯z QÂµ k,j (z, Â¯z) = ( j + Âµ + 1) QÂµ k,j+1(z, Â¯z) + k QÂµ kâˆ’1,j (z, Â¯z). Proof. The result follows from writing (k + j + Âµ + 1)z = { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚z + j Â¯z } âˆ’ { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚z âˆ’ (k + Âµ + 1) Â¯z } , (k + j + Âµ + 1)Â¯z = { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚Â¯z + k z } âˆ’ { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚Â¯z âˆ’ (j + Âµ + 1) z } , and the using (20) and (21). â–¡ We can also deduce the linear relation between families of complex gene ralized Zernike polynomials with parameters Âµ and Âµ + 1. We remark that this linear relation is of ï¬xed length for all values of k and j. Proposition 5.4. The complex generalized Zernike polynomials satisfy (k+j+Âµ+1) QÂµ k,j (z, Â¯z) = (k + Âµ + 1)(j + Âµ + 1) Âµ + 1 QÂµ+1 k,j (z, Â¯z)âˆ’ k j Âµ + 1 QÂµ+1 kâˆ’1,jâˆ’1(z, Â¯z). Proof. Since (k + j + Âµ + 1) QÂµ k,j (z, Â¯z) = { z âˆ‚ âˆ‚z + j + Âµ + 1 } QÂµ k,j (z, Â¯z) âˆ’ { z âˆ‚ âˆ‚z âˆ’ k } QÂµ k,j (z, Â¯z), the result follows from (18) and (22). â–¡ Ladder operators can be combined to obtain the so-called structu re relations, that is, linear relations of ï¬xed length involving the partial derivative s of the poly- nomials. Proposition 5.5. The complex generalized Zernike polynomials satisfy the follow- ing structure relations (k + j + Âµ + 1)(1 âˆ’ zÂ¯z) âˆ‚ âˆ‚z QÂµ k,j (z, Â¯z) = k(j + Âµ + 1) ( QÂµ kâˆ’1,j (z, Â¯z) âˆ’ QÂµ k,j+1(z, Â¯z) ) , and (k + j + Âµ + 1)(1 âˆ’ zÂ¯z) âˆ‚ âˆ‚Â¯z QÂµ k,j (z, Â¯z) = j(k + Âµ + 1) ( QÂµ k,jâˆ’1(z, Â¯z) âˆ’ QÂµ k+1,j (z, Â¯z) ) . Proof. The ï¬rst structure relation follows from writing (k+j+Âµ+1)(1âˆ’zÂ¯z) âˆ‚ âˆ‚z = k { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚z âˆ’ (j + Âµ + 1)Â¯z } +(j+Âµ+1) { âˆ‚ âˆ‚z + kÂ¯z } , and using (20) and (21). The second structure relation follows simila rly. â–¡ From the structure relations and (16), we get the following corollar y. Corollary 5.6. The complex generalized Zernike polynomials satisfy (k + j + Âµ + 2)(1 âˆ’ zÂ¯z)QÂµ+1 k,j (z, Â¯z) = ( Âµ + 1) ( QÂµ k,j (z, Â¯z) âˆ’ QÂµ k+1,j+1(z, Â¯z) ) . Since the right hand side of the ï¬rst equation in (22) and the ï¬rst eq uation in (23) are equal, we readily get the following fundamental relation. Proposition 5.7. The complex generalized Zernike polynomials satisfy{ z âˆ‚ âˆ‚z âˆ’ Â¯z âˆ‚ âˆ‚Â¯z } QÂµ k,j (z, Â¯z) = ( k âˆ’ j) QÂµ k,j (z, Â¯z).LADDER OPERATORS FOR GENERALIZED ZERNIKE OR DISK POLYNOMIA LS 17 6. A note about Sobolev orthogonality Univariate orthogonal polynomials with respect to a Sobolev inner pr oduct (i.e., an inner product involving the derivatives of the polynomials) have be en studied extensively in the past few decades. In contrast to one variable, S obolev orthogonal polynomials of several variables are studied only recently. We refer the interested reader to the survey [7] for a recent presentation of the state o f the art on Sobolev orthogonal polynomials. Several authors have taken an interest on Sobolev or- thogonal polynomials on the unit ball of Rn which, clearly, include the generalized Zernike polynomials when n = 2 (see, for instance, [1, 2, 5, 6, 8, 9, 11, 18, 17]). In this section, we illustrate the use of ladder operators in the stud y of Sobolev orthogonal polynomials on the disk. 6.1. Example 1. Consider the following Sobolev inner product (f, g )1 = Î» Ï€ âˆ« D âˆ‚f âˆ‚z (z, Â¯z) âˆ‚g âˆ‚z (z, Â¯z) dz + 1 Ï€ âˆ« 2Ï€ 0 f(eiÎ¸, eâˆ’iÎ¸) g(eiÎ¸, eâˆ’iÎ¸) dÎ¸, Î» > 0. We remark that this inner product is a complex version of one of the S obolev inner products studied in [18] given by ( Ëœf, Ëœg) = Î» Ï€ âˆ« D âˆ‡ Ëœf(x, y)Â·âˆ‡Ëœg(x, y) dxdy+ 1 Ï€ âˆ« 2Ï€ 0 Ëœf(cos Î¸, sin Î¸) Ëœg(cos Î¸, sin Î¸) dÎ¸, Î» > 0, for real-valued functions Ëœf(x, y) = f(z, Â¯z) and Ëœg(x, y) = g(z, Â¯z) under the change of variable z = x + iy = r eiÎ¸. Now we use ladder operators for complex generalized Zernike polyno mials to show that the polynomials deï¬ned in (24) are orthogonal with respe ct to ( Â·, Â·)1. Lemma 6.1. The polynomials deï¬ned in (24) satisfy âˆ‚ âˆ‚z Qâˆ’1 k,j (z, Â¯z) = âˆ’Q0 kâˆ’1,j (z, Â¯z), k, j â©¾ 1. Proof. We compute, âˆ‚ âˆ‚z Qâˆ’1 k,j (z, Â¯z) = { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚z âˆ’ Â¯z } Q1 kâˆ’1,jâˆ’1(z, Â¯z). Our result follows from (17). â–¡ Proposition 6.2. The polynomials deï¬ned in (24) constitute a mutually orthog- onal polynomial system with respect to ( Â·, Â·)1. Moreover, (Qâˆ’1 k,j , Qâˆ’1 m,â„“)1 = Ëœ hk,j (Î») Î´k,m Î´j,â„“, with Ëœhk,j (Î») = ï£± ï£² ï£³ 2, k = j = 0 , Î» h0 kâˆ’1,j , k, j â©¾ 1, where h0 kâˆ’1,j is given in (6). Proof. Clearly, (Qâˆ’1 0,0, Qâˆ’1 0,0)1 = (1 , 1)1 = 1 Ï€ âˆ« 2Ï€ 0 dÎ¸ = 2 . For k, j â©¾ 1, Qâˆ’1 k,j (eiÎ¸, eâˆ’iÎ¸) = 0 ,18 MISAEL E. MARRIAGA since the factor (1 âˆ’ zÂ¯z) vanishes when z = eiÎ¸, 0 â©½ Î¸ â©½ 2Ï€. Therefore, (Qâˆ’1 k,j , Qâˆ’1 0,0)1 = 0 , k, j â©¾ 0. Using Lemma (6.1), we get (Qâˆ’1 k,j , Qâˆ’1 m,â„“)1 = Î» Ï€ âˆ« D Q0 kâˆ’1,j (z, Â¯z) Q0 mâˆ’1,â„“(z, Â¯z) dz = Î» h0 kâˆ’1,j Î´k,m Î´j,â„“, where the last equality follows from (5). â–¡ 6.2. Example 2. Now consider the following Sobolev inner product (f, g )2 = 1 Ï€ âˆ« D âˆ‚2 âˆ‚zâˆ‚ Â¯z [(1 âˆ’ zÂ¯z)f(z, Â¯z)] âˆ‚2 âˆ‚zâˆ‚ Â¯z [(1 âˆ’ zÂ¯z)g(z, Â¯z)]dz. Recall that the two-dimensional Laplace operator can be represe nted in complex variables as âˆ† = 4 âˆ‚2 âˆ‚zâˆ‚ Â¯z . Hence, for real-valued functions Ëœf(x, y) = f(z, Â¯z) and Ëœg(x, y) = g(z, Â¯z), this inner product is equivalent to ( Ëœf, Ëœg)âˆ† = 1 16Ï€ âˆ« D âˆ†[(1 âˆ’ x2 âˆ’ y2) Ëœf(x, y)] âˆ†[(1 âˆ’ x2 âˆ’ y2)Ëœg(x, y)] dxdy. A mutually orthogonal polynomial system with respect to ( Â·, Â·)âˆ† is constructed and studied in [17]. Here, we study a complex mutually orthogonal polynom ial system relative to ( Â·, Â·)2 via ladder operators. Deï¬ne the polynomials U0,0(z, Â¯z) = 1 , U k,j (z, Â¯z) = (1 âˆ’ zÂ¯z)Q2 kâˆ’1,jâˆ’1(z, Â¯z), k, j â©¾ 1. (25) Lemma 6.3. For k, j â©¾ 0, the polynomials deï¬ned in (25) satisfy âˆ‚2 âˆ‚zâˆ‚ Â¯z [(1 âˆ’ zÂ¯z)Uk,j (z, Â¯z)] = ck,j Q0 k,j (z, Â¯z), with ck,j = { âˆ’1, k = j = 0 , 2, k, j â©¾ 1, and (1 âˆ’ zÂ¯z) âˆ‚2 âˆ‚zâˆ‚ Â¯z Q0 k,j (z, Â¯z) = dk,j Uk,j (z, Â¯z), with dk,j = ï£± ï£´ ï£² ï£´ ï£³ 0, k = j = 0 , 1 2 k j (k + 1) ( j + 1), k, j â©¾ 1. Proof. We compute, âˆ‚ âˆ‚Â¯z [(1 âˆ’ zÂ¯z)U0,0(z, Â¯z)] = âˆ’z. Then, âˆ‚2 âˆ‚zâˆ‚ Â¯z [(1 âˆ’ zÂ¯z)U0,0(z, Â¯z)] = âˆ’1 = âˆ’Q0 0,0(z, Â¯z). For k, j â©¾ 1, we compute âˆ‚ âˆ‚Â¯z [(1 âˆ’ zÂ¯z)Uk,j (z, Â¯z)] = (1 âˆ’ zÂ¯z) { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚Â¯z âˆ’ 2z } Q2 kâˆ’1,jâˆ’1(z, Â¯z).LADDER OPERATORS FOR GENERALIZED ZERNIKE OR DISK POLYNOMIA LS 19 It follows from (16) that âˆ‚ âˆ‚Â¯z [(1 âˆ’ zÂ¯z)Uk,j (z, Â¯z)] = âˆ’2(1 âˆ’ zÂ¯z)Q1 k,jâˆ’1(z, Â¯z). Then, âˆ‚2 âˆ‚zâˆ‚ Â¯z [(1 âˆ’ zÂ¯z)Uk,j (z, Â¯z)] = âˆ’2 { (1 âˆ’ zÂ¯z) âˆ‚ âˆ‚z âˆ’ Â¯z } Q1 k,jâˆ’1(z, Â¯z) = 2 Q0 k,j (z, Â¯z), where the last equality follows from (17). Moreover, using (16) and (17), we obtain (1 âˆ’ zÂ¯z) âˆ‚2 âˆ‚zâˆ‚ Â¯z Q0 k,j (z, Â¯z) = 1 2 k j (k + 1) ( j + 1)(1 âˆ’ zÂ¯z)Q2 kâˆ’1,jâˆ’1(z, Â¯z) = 1 2 k j (k + 1) ( j + 1) Uk,j (z, Â¯z). â–¡ Proposition 6.4. The polynomials deï¬ned in (25) constitute a mutually orthog- onal polynomial system with respect to ( Â·, Â·)2. Moreover, (U0,0, U0,0)2 = 1 , (Uk,j , Um,â„“)2 = 4 k + j + 1 Î´k,m Î´j,â„“, k, j â©¾ 1. Proof. By Lemma (6.3), we have (Uk,j , Um,â„“)2 = c2 k,j 1 Ï€ âˆ« D Q0 k,j (z, Â¯z) Q0 m,â„“(z, Â¯z) dz = c2 k,j h0 k,j Î´k,m Î´j,â„“, where h0 k,j is given in (6). â–¡ References [1] A. M. Delgado, T. E. PÂ´ erez, M. A. PiËœ nar, Sobolev-type orthogonal polynomials on the unit ball, J. Approx. Theory 170 (2013), 94â€“106. [2] A. M. Delgado, L. FernÂ´ andez, T. E. PÂ´ erez, M. A. PiËœ nar, Sobolev orthogoanl polynomials on the unit ball via outward normal derivatives , J. Math. Anal. Appl. 440 (2016), no. 2, 716â€“740. [3] C. F. Dunkl, Y. Xu, Orthogonal polynomials of several variables , 2nd edition, Encyclopedia of Mathematics and its Applications, vol. 155, Cambridge Un iv. Press, Cambridge (2014). [4] J. C. Garc Â´ Ä±a-Ardila, M. E. Marriaga, Sobolev orthogonality of polynomial solutions of second- order partial diï¬€erential equations , Comput. Appl. Math. 42 (2023), no.1, Paper No. 13, 44 pp. [5] H. Li, Y. Xu, Spectral approximation on the unit ball , SIAM J. Numer. Anal. 52 (2014), no. 6, 2647â€“2675. [6] F. Lizarte, T. E. PÂ´ erez, M. A. PiËœ nar, The radial part of a class of Sobolev polynomials on the unit ball , Numer. Algorithms 87 (2021), no. 4, 1369â€“1389. [7] F. MarcellÂ´ an, Y. Xu, On Sobolev orthogonal polynomials , Expo. Math. 33 (2015), no. 3, 308-352. [8] M. E. Marriaga, T. E. PÂ´ erez, M. A. PiËœ nar, M. J. Recarte, Approximation via gradients on the ball. The Zernike case , J. Comput. Appl. Math. 430 (2023), Paper No. 115258, 23. [9] M. E. Marriaga, T. E. PÂ´ erez, M. J. Recarte, Simultaneous approximation via Laplacians on the unit ball , Mediterr. J. Math. 20 (2023), no. 6, Paper No. 316, 22 pp. [10] F. W. J. Olver, D. W. Lozier, R. F. Boisvert, C. W. Clark, ( eds), NIST Handbook ofMathe- matical Functions, Cambridge University Press, Cambridge (2010). [11] T. E. PÂ´ erez, M. A. PiËœ nar, Y. Xu, Weighted Sobolev orthogonal polynomials on the unit ball , J. Approx. Theory, 171 (2013), 84â€“104. [12] M. A. PiËœ nar, Y. Xu, Orthogonal Polynomials and Partial Diï¬€erential Equations on the Unit Ball Proc. Amer. Math. Soc. 137, 2979-2987 (2009).20 MISAEL E. MARRIAGA [13] G. SzegË o, Orthogonal polynomials , 4th edition, vol. 23. Amer. Math. Soc. Colloq. Publ., Amer. Math. Soc., Providence RI, 1975. [14] A. Torre, Generalized Zernike or disc polynomials: an application in quantum optics , J. Comput. Appl. Math. 222 (2008), no. 2, 622â€“644. [15] N. Ja. Vilenkin, A. U. Klimyk, Representation of Lie groups and special functions. Vol. 2.: Class I representations, special functions, and integral transforms. Translated from the Russian by V. A. Groza and A. A. Groza, Math. Appl. (Soviet Ser .), 74, Kluwer Academic Publishers Group, Dordrecht, 1993. xviii+607 pp. [16] A. WÂ¨ unsche, Generalized Zernike or disc polynomials , J. Comput. Appl. Math. 174 (2005), no. 1, 135â€“163. [17] Y. Xu, A family of Sobolev orthogonal polynomials on the unit ball , J. Approx. Theory 138 (2006), no. 2, 232-241. [18] Y. Xu, Sobolev orthogonal polynomials deï¬ned via gradient on the u nit ball , J. Approx. Theory, 152 (2008), 52â€“65. [19] Y. Xu, Complex versus real orthogonal polynomials in two variable s, Integral Transforms Spec. Funct. 26 (2015), no. 2, 134â€“151. [20] Zernike F., Beugungstheorie des schneidenver-fahrens und seiner verb esserten form, der phasenkontrastmethode, Physica Vol. 1, Issue 7-12 (1934), 689-704. (M. E. Marriaga) Departamento de Matem Â´atica Aplicada, Ciencia e Ingenier Â´Ä±a de Ma- teriales y Tecnolog Â´Ä±a ElectrÂ´onica, Universidad Rey Juan Carlos (Spain) Email address : misael.marriaga@urjc.es",
      "references": [
        "Sobolev-type orthogonal polynomials on the unit ball",
        "Sobolev orthogoanl polynomials on the unit ball via outward normal derivatives",
        "Orthogonal polynomials of several variables",
        "Sobolev orthogonality of polynomial solutions of second- order partial diï¬€erential equations",
        "Spectral approximation on the unit ball",
        "The radial part of a class of Sobolev polynomials on the unit ball",
        "On Sobolev orthogonal polynomials",
        "Approximation via gradients on the ball. The Zernike case",
        "Simultaneous approximation via Laplacians on the unit ball",
        "NIST Handbook ofMathe- matical Functions",
        "Weighted Sobolev orthogonal polynomials on the unit ball",
        "Orthogonal Polynomials and Partial Diï¬€erential Equations on the Unit Ball",
        "Orthogonal polynomials",
        "Generalized Zernike or disc polynomials: an application in quantum optics",
        "Representation of Lie groups and special functions. Vol. 2.: Class I representations, special functions, and integral transforms.",
        "Generalized Zernike or disc polynomials",
        "A family of Sobolev orthogonal polynomials on the unit ball",
        "Sobolev orthogonal polynomials deï¬ned via gradient on the u nit ball",
        "Complex versus real orthogonal polynomials in two variable s",
        "Beugungstheorie des schneidenver-fahrens und seiner verb esserten form, der phasenkontrastmethode"
      ],
      "meta_data": {
        "arxiv_id": "2405.02988v1",
        "authors": [
          "Misael E. Marriaga"
        ],
        "published_date": "2024-05-05T16:17:57Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Derives and proves a comprehensive family of ladder (raising/lowering) operators for generalized Zernike (disk) polynomials orthogonal on the unit disk with weight (1-x^2-y^2)^Âµ (Âµ>-1), primarily in a complex-variable formulation Q^Âµ_{k,j}(z,\\bar z). Establishes how these operators shift the indices (k,j) and/or the weight parameter Âµ by at most one unit (and in some cases shift total degree by two). Shows how to combine ladder operators to recover fundamental differential equations, three-term recurrences, structure relations, and fixed-length connection relations between families with parameters Âµ and Âµ+1. Demonstrates, via two examples, that ladder operators provide simpler proofs for orthogonality of certain Sobolev-type polynomial systems on the disk.",
        "methodology": "Starts from Jacobi polynomial representations of disk polynomials and first derives multiple univariate Jacobi ladder operators using operator commutators with the Jacobi differential operator (operational proofs) and reflection symmetry. Translates these univariate operators to the bivariate/complex setting by identifying r=z\\bar z and t=2r-1, and expressing Q^Âµ_{k,j}(z,\\bar z) as powers of z or \\bar z times Jacobi polynomials P^{(Âµ,\\cdot)}(2z\\bar z-1). Constructs complex differential operators in z and \\bar z (e.g., âˆ‚/âˆ‚z, (1-z\\bar z)âˆ‚/âˆ‚\\bar z-Âµ z, etc.) whose actions map Q^Âµ_{k,j} to nearby parameters, exploiting invariance Q^Âµ_{k,j}(z,\\bar z)=Q^Âµ_{j,k}(\\bar z,z). Combines ladder identities algebraically to obtain PDE eigenvalue equations (operator L_Âµ), three-term recurrences, and structure relations; applies these identities to compute Sobolev inner products by reducing derivative terms to standard L^2 disk orthogonality.",
        "experimental_setup": "No empirical/compute experiments; the work is theoretical. Validation is via rigorous symbolic/operator derivations and consistency checks with known Jacobi/Zernike identities and orthogonality. Illustrative â€˜examplesâ€™ are analytic: (i) a Sobolev inner product involving âˆ‚/âˆ‚z over the disk plus a boundary integral on the unit circle, proving mutual orthogonality of a family built from (1-z\\bar z)Q^{1}_{k-1,j-1}; (ii) a Sobolev inner product involving âˆ‚^2/(âˆ‚zâˆ‚\\bar z) applied to (1-z\\bar z)f, proving orthogonality of another constructed family. Orthogonality constants are computed using the known L^2 norms h^Âµ_{k,j}.",
        "limitations": "Results rely on the Jacobi-polynomial representation and thus on rotational symmetry of the disk and the specific radial weight (1-r^2)^Âµ. Many ladder operators require parameter restrictions (typically Âµ>-1 for orthogonality; Âµ>0 for operators involving Âµâˆ’1). The paper focuses on shifts by at most one in parameters and does not develop a full Lie-algebraic closure/representation-theoretic framework for the operators. Discussion of Âµâ‰¤âˆ’1 is limited; while the PDE operator makes sense at Âµ=-1, standard orthogonality fails and for integer Âµâ‰¤âˆ’2 polynomial-system existence is not guaranteed (citing prior work). There are no numerical stability/implementation analyses for using these operators in applications (optics, approximation, spectral methods).",
        "future_research_directions": "Extend the ladder-operator framework to higher dimensions (unit ball in R^n) and other domains/weights (less symmetric, nonradial, or with boundary singularities), and to generalized disk polynomials involving additional parameters. Develop an explicit Lie-algebra/representation-theoretic interpretation (e.g., su(2), su(1,1)) for the full operator set and their commutation relations, linking to quantum-optics models. Use the operators to construct and analyze Sobolev orthogonal polynomials for broader classes of Sobolev inner products (higher-order derivatives, mixed boundary terms), including explicit recurrence/connection formulas and basis-conversion algorithms. Investigate computational schemes (spectral methods on the disk) leveraging the ladder operators for fast differentiation, coefficient transforms, and conditioning/stability. Explore parameter-extension/analytic continuation issues for Âµ near or below -1 and characterize when PDE-defined polynomial systems exist and what orthogonality/biorthogonality structures replace the classical ones.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Compositional Semantic Parsing with Large Language Models",
      "full_text": "CACTUS: A Computational Framework for Generating Realistic White Matter Microstructure Substrates Juan Luis Villarreal-Haro1,âˆ—, Remy Gardier1, Erick J Canales-RodrÂ´Ä±guez1, Elda Fischi Gomez2,3,1, Gabriel Girard1,2,3,4, Jean-Philippe Thiran1,2,3, Jonathan Rafael-PatiËœno3,1 1Signal Processing Laboratory (LTS5), Â´Ecole Polytechnique FÂ´edÂ´erale de Lausanne (EPFL), Lausanne, Switzerland 2CIBM Center for Biomedical Imaging, Switzerland 3Radiology Department, Centre Hospitalier Universitaire Vaudois and University of Lausanne, Lausanne, Switzerland 4Department of Computer Science, University of Sherbrooke, Sherbrooke, QC, Canada Correspondence*: Juan Luis Villarreal-Haro juan.villarrealharo@epfl.ch ABSTRACT Monte-Carlo diffusion simulations are a powerful tool for validating tissue microstructure models by generating synthetic diffusion-weighted magnetic resonance images (DW-MRI) in controlled environments. This is fundamental for understanding the link between micrometre-scale tissue properties and DW-MRI signals measured at the millimetre-scale, optimising acquisition protocols to target microstructure properties of interest, and exploring the robustness and accuracy of estimation methods. However, accurate simulations require substrates that reflect the main microstructural features of the studied tissue. To address this challenge, we introduce a novel computational workflow, CACTUS (Computational Axonal Configurator for Tailored and Ultradense Substrates), for generating synthetic white matter substrates. Our approach allows constructing substrates with higher packing density than existing methods, up to 95 % intra-axonal volume fraction, and larger voxel sizes of up to (500Âµm)3 with rich fibre complexity. CACTUS generates bundles with angular dispersion, bundle crossings, and variations along the fibres of their inner and outer radii and g-ratio. We achieve this by introducing a novel global cost function and a fibre radial growth approach that allows substrates to match predefined targeted characteristics and mirror those reported in histological studies. CACTUS improves the development of complex synthetic substrates, paving the way for future applications in microstructure imaging. keywords: microstructure imaging, diffusion MRI, brain imaging, white matter, Monte Carlo simulations, numerical phantom, synthetic substrates, high packing density. 1 INTRODUCTION Diffusion-weighted magnetic resonance imaging (DW-MRI) is a non-invasive technique used to study the microscopic structure of biological tissues in vivo. It is sensitive to the ensemble of water molecules 1 arXiv:2305.16109v1  [cs.CE]  25 May 2023Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS (wherein each molecule follows a random motion pattern) as they interact with cellular surfaces (Bihan, 1995). (Bihan, 1995). This technique provides a valuable tool to study brain microstructure and its alterations following injury (Parizel et al., 2005; To et al., 2022) and neurological disease (van Gelderen et al., 1994; Budde and Frank, 2010; Narvaez-Delgado et al., 2019). White matter is a crucial component of the brain, composed of highly organised axon bundles that interconnect cortical regions and subcortical regions (BrÂ¨uckner et al., 1996; Sporns, 2011). Various imaging techniques have been considered to characterise the white matter tissue microstructure in different species. For example, axon diameters have been measured in some white matter regions of the macaque monkey brain using histology and DW-MRI (Caminiti et al., 2013), and optical microscopy (Innocenti and Caminiti, 2017). These studies show that the estimated distribution of axon diameters is long-tailed, with a mean of around one micrometre. A recent study using high-resolution three-dimensional (3D) synchrotron X-ray nano-holotomography found that axons are non-cylindrical and exhibit environment-dependent variations in diameter and trajectory (Andersson et al., 2020). Alongside axon diameters, another relevant feature is the intracellular volume the axons occupy in a predetermined region. In histological postmortem data, the white matter intracellular space volume has been estimated as ranging between 60-85% of the brain volume for macaques (Stikov et al., 2015) and human adults (SykovÂ´a and Nicholson, 2008). Interestingly, it goes as high as 70%-95% in mice, as reported by light microscopy (TÃ¸nnesen et al., 2018), and cryo and chemical fixations (Korogod et al., 2015). Given the importance of studying white matter tissue microstructure in vivo, several DW-MRI models have been proposed (e.g. Jelescu and Budde, 2017; Novikov et al., 2018, 2019; Assaf et al., 2004, 2008; Alexander et al., 2010; Dyrby et al., 2011; Veraart et al., 2021; Neuman, 1974; van Gelderen et al., 1994; Murday and Cotts, 1968; SÂ¨oderman and JÂ¨onsson, 1995; Lee et al., 2020). However, validating these non- invasive techniques requires physical and numerical phantoms with a well-known microstructure (Lavdas et al., 2013; Campbell et al., 2005; Fillard et al., 2011; Tournier et al., 2008; Fieremans et al., 2008; Schilling et al., 2019; Zhou et al., 2018; Maier-Hein et al., 2017; Andersson et al., 2020; Lee et al., 2020; Rafael-Patino et al., 2020). Phantoms, in the context of this paper, are geometrical models of brain tissue structures that serve as a proxy or reference for evaluating the performance of imaging techniques. While physical phantoms have been widely used, they are often limited by their high costs and the impracticality of replicating axonsâ€™ sizes and complex spatial arrangement. Therefore, numerical phantoms have emerged as the most popular validation technique for studying the complexities of diffusion phenomena in cases where analytical solutions are unavailable; because they only require a substrate that mimics the tissue of interest to simulate the displacements of water molecules and corresponding DW-MRI signal (Close et al., 2009; Neher et al., 2014; CË†otÂ´e et al., 2013). Nevertheless, the difficulty in Monte-Carlo simulations lies in accurately mimicking the geometry of white matter tissue (KerkelÂ¨a et al., 2021; Rafael-Patino et al., 2020; Hall and Alexander, 2009a; Nilsson et al., 2017; Baxter and Frank, 2013; Plante and Cucinotta, 2013; Nilsson et al., 2012; Truffet et al., 2020). Various studies have attempted to generate numerical phantoms approaching the tissueâ€™s morphological complexity and density. For instance, two popular tools, MEDUSA (Ginsburger et al., 2019) and CONFIG (Callaghan et al., 2020), focus on generating specialised voxel-wise phantoms with microstructural geometries that replicate the properties of white matter. Recently, a tailored modification of Close et al. (2009) framework was used to build challenging substrates for the DiSCo challenge (Rafael- Patino et al., 2021), aimed to test fibre-tracking and connectivity methods on large-scale synthetic datasets from DW-MRI Monte-Carlo simulations. While these methods have provided valuable tools to characterise and simulate DW-MRI signals in numerical substrates, they still have important limitations regarding CACTUS 2Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS the maximum packing density and substrate size achieved. For instance, state-of-the-art frameworks can generate synthetic substrates with packing densities up to 75% (Callaghan et al., 2020; Ginsburger et al., 2019; Rafael-Patino et al., 2021), whereas the density found in histological data goes up to 95% in some regions (Korogod et al., 2015; TÃ¸nnesen et al., 2018). Moreover, they cannot sample substrate beyond (100Âµm)3, which in turn restricts the sampling diversity achieved for morphological features (Rafael-Patino et al., 2020; Romascano et al., 2018). Therefore, the DW-MRI signals generated from these substrates may not accurately mimic the brain signals measured in white matter regions with higher packing densities. To overcome these limitations, we introduce a novel computational workflow, CACTUS (Computational Axonal Configurator for Tailored and Ultradense Substrates), to generate synthetic fibres with rich microstructure characteristics. Expanding on previous methods (Close et al., 2009; Ginsburger et al., 2019; Rafael-Patino et al., 2020), we develop a novel numerical phantom generator for white matter substrates. CACTUS solves the high-density packing problem and achieves up to 95% intracellular volume fractions while efficiently generating substrate sizes up to (500Âµm)3. Furthermore, CACTUS is highly customisable, capable of generating synthetic substrates with a wide range of characteristics, such as single-bundle (Stikov et al., 2015), bundle crossings (Schilling et al., 2017; Tournier et al., 2007; Tuch, 2004; Canales-RodrÂ´Ä±guez et al., 2019), orientation dispersion (Zhang et al., 2012; Daducci et al., 2015), gamma-distributed axon radii (Sepehrband et al., 2016; Assaf et al., 2008), non-constant longitudinal fibre-radii (Andersson et al., 2020), substrates with non-cylindrical fibres and tortuous surfaces (Lee et al., 2019), and myelin compartments (Stikov et al., 2015; Mackay et al., 1994; Canales-RodrÂ´Ä±guez et al., 2021). Through these features, CACTUS expands on the capabilities of existing substrate generation methods, providing a flexible and versatile tool for studying white matter microstructure in controlled environments. 2 METHODS CACTUS generates synthetic substrates in three steps (see Figure 1): a) substrate initialisation, b) joint fibre optimisation, c) fibre radial growth (FRG). Firstly, in the substrate initialisation step, synthetic straight cylindrical fibres are initialised and parameterised inside a cuboid. In CACTUS, a single fibre population (bundle) is a group of fibres arranged cohesively along one main orientation. A bundle has two main properties: the average global dispersion, which is the mean angle between the main orientation of each fibre and the bundle, and the target radii distribution, from which the fibre radii are sampled. In the second step, the joint fibre optimisation , CACTUS extends previously proposed frameworks (Close et al., 2009; Ginsburger et al., 2019) based on local optimisation. In our case, we aim to minimise a cost function that penalises some essential fibre properties such as overlapping, high curvature, increase in length, and promote compactness. Moreover, CACTUS introduces a new fibre parameterisation based on capsules, which reduces the number of parameters needed to characterise fibre trajectories and handles fibre overlapping more efficiently. The resulting optimisation problem is solved via a gradient descent algorithm (Duchi et al., 2011). During optimisation, CACTUS prioritises removing fibre overlapping, while the penalisation of curvature, length and promotion of compactness maintains a coherent fibre structure at all time-points. Finally, the fibre trajectories are used to mesh the fibre surfaces in the fibre radial growth (FRG) step. The FRG also increases the packing density while keeping the correspondent fibreâ€™s parameterisation structure using a discrete grid to seed, to grow, and to rearrange the fibre into the final substrates. The grid A 3Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS Figure 1. Example of the CACTUS method steps to create a synthetic substrate. a) The substrate initialisation orients fibres in a bundle to achieve a predefined mean angular dispersion (e.g., 15 Âº). b) The joint fibre optimisation step removes fibre overlaps by adapting their trajectories and local radii. In the case of bundle crossings, the trajectories are trimmed to the centre of the crossing. c) The fibre radial growth step further increases the fibre-packing density while keeping the predefined target radius distribution. discretisation defines the fibresâ€™ isosurface needed to compute the final surfaces with a marching cube algorithm (Lewiner et al., 2003). 2.1 Substrate initialisation Our substrate initialisation algorithm enhances the circle two-dimensional (2D) packing algorithm proposed by Hall and Alexander (2009b) to create a 3D packing of bundles. The algorithm creates a single bundle by initialising the fibres inside a cuboid of dimensions L Ã— L Ã— H. The endpoints of the fibres are contained within the L Ã— L squared faces, while the orientation of the cuboidâ€™s height H and the bundle are aligned to the Z-axis. The algorithm packs 2D circles in the opposite faces of the cuboid, using radii from a gamma Î“(Î±, Î²) distribution (Sepehrband et al., 2016; Assaf et al., 2008), until the target density is CACTUS 4Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS Figure 2. a) Capsule example, whose parameters are two points and a radius at each control point. b) Example of fibre as a chain of capsules. Two adjacent capsules share a control point (position and radius parameters). met. At the same time, the algorithm packs the two opposite 2D circles to create an initialisation such that the bundle reach the specified mean angular dispersion Î·. In order to create a substrate with two bundles crossing at an inter-bundle angle of Î¸, two different bundles are initialised in their respective cuboids and subsequently rotated and translated are applied. Figure 1 shows examples of a single bundle and a bundle crossing initialisation. Finally, we parameterise each fibreâ€™s skeleton as the trajectory of its centre of mass. This trajectory is defined by several control points connecting the two endpoints sampled during the packing algorithm, where each point has a corresponding radius. 2.2 Joint fibre optimisation Once the substrate is initiated as described in subsection 2.1, fibres may overlap. CACTUS employs an optimisation method to readjust the fibre trajectories and disentangle overlaps by defining several cost functions. These cost functions, inspired by Close et al. (2009) and Ginsburger et al. (2019), help to regularise and obtain coherent fibre structures with the specified target properties. Ordered by priority of penalisation, these target properties are as follows: i) fibre overlapping (see subsection 2.2.2), ii) high curvature, iii) increased fibre length, iv) changes in radii and v) compactness. In the following subsection, we introduce the novel parameterisation and overlapping cost function based on capsules, which is a key contribution of our work. As the remaining cost functions are relatively straightforward and similar to those in previous studies, we have provided their definitions in Supplementary Material (section joint fibre optimisation). 2.2.1 Fibre capsule-parameterisation Fibres are parameterised as skeletons made of 3D control points. In the overlapping cost function, every pair of consecutive points in the skeleton forms acapsule, defined with the set of parameters[p0, p1, r0, r1], where p0, p1 âˆˆ R3 are the initial/ending points of the capsule, and r0, r1 âˆˆ R are their respective radius (see Figure 2 a). In this framework, a fibre parameterisation can be defined as a chain of capsules (see Figure 2 b). The fibreSa, with ma control points, is composed of the capsules determined by the subsequent point pairs as \u0002 xa i , xa i+1, ra i , ra i+1 \u0003 with control points {x0a, x1a, . . .xmaâˆ’1a} âŠ‚R3 and associated radius\b ra 0 , ra 1 , . . . ra maâˆ’1 \t âŠ‚ R. 2.2.2 Overlapping cost function The overlapping cost function handles the fibre collision by identifying overlaps from two capsules from two different fibres. In CACTUS, the detection step of capsule intersection is a generalisation of the cylinder- to-cylinder collision detection (Van Verth and Bishop, 2015). We define the overlapping cost function between two capsules by computing the overlapping of the closest spheres centred in the capsules. Formally, the closest points between two given capsules [p0, p1, rp0 , rp1 ], [q0, q1, rq0 , rq1 ], with pi, qj âˆˆ R3 and A 5Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS , rpi , rqj âˆˆ R, are the points centred in the capsule ((1 âˆ’ tp)p0 + tpp1) and, ((1 âˆ’ tq)q0 + tqq1), where tp, tq are found by the following minimisation problem: g(ta, tb; p0, p1, q0, q1) = âˆ¥(1 âˆ’ ta)p0 + tap1 âˆ’ (1 âˆ’ tb)q0 âˆ’ tbq1âˆ¥2 (1) tp, tq := arg min ta,tb s.t. 0â‰¤ta,tbâ‰¤1 g(ta, tb; p0, p1, q0, q1), (2) which has a closed-form solution. After finding the values tp, tq that define the closest centre points between two capsules of different fibres, their overlapping cost function is defined as: f1 (p0, p1, q0, q1, rp0 , rp1 , rq0 , rq1 ; tp, tq) = ( D2âˆ¥p0 âˆ’ p1âˆ¥âˆ¥q0 âˆ’ q1âˆ¥rprq, if D â‰¥ 0 0, if D <0 (3) where, D := \u0012 1 âˆ’ âˆ¥c1 âˆ’ c2âˆ¥ rp + rq \u0013 , (4) c1 := (1 âˆ’ tp)p0 + tpp1, (5) c2 := (1 âˆ’ tq)q0 + tqq1, (6) rp := (1 âˆ’ tp)rp0 + tprp1 , (7) rq := (1 âˆ’ tq)rq0 + tqrq1 , (8) (9) and tp, tr are the minimal values from the function in Eq. 2. Consequently, the total overlapping cost function in a substrate is computed by adding the evaluated cost of all possible pairwise capsule combinations. If capsules overlap, a penalisation is added; otherwise, it is set to zero. 2.2.3 Implementation details At last, we mention the technical implementation details of the joint-fibre optimization algorithm, including strategies for reducing computational complexity and the use of specific data structures. Firstly, in the total overlapping cost function, the capsule-to-capsule comparison is a O(n2) problem. To improve computational time, we implemented a fixed-radius-cell data-structure (Turau, 1991) for nearest neighbours queries, reducing the problem to O(n). Since all the cost functions are analytical, we calculated their analytical derivatives for the gradient descent algorithm (see Supplementary materials, section joint fibre optimisation). We used the adaptative gradient Adagrad (Duchi et al., 2011), iterating until there were no overlapping fibres. All the cost functions, queries, and gradients calculations were implemented in C++ (Stroustrup, 1999) and parallelised with OpenMP (Chandra et al., 2001). To handle bundle crossings, we trim the optimised fibre trajectories to keep only a subregion with fibres that truly belong to the crossing, as shown in Figure 1. This step eliminates boundary fibres that may not fully represent the crossing characteristics. CACTUS 6Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS 2.3 Fibre radial growth (FRG) 2.3.1 FRG description After completing the substrate initialisation and joint fibre optimisation steps, it follows to compute the fibre mesh. Previous studies have managed to achieve a fibre density up to 75% (Ginsburger et al., 2019; Mingasson et al., 2017; Altendorf and Jeulin, 2011) with cylindrical-shaped fibres and gamma- distributed diameter. In this study, we propose a new method, called Fibre Radial Growth (FRG), to obtain higher packing density and complex axon morphologies beyond the cylindrical shape. The FRG algorithm discretises the 3D space that the fibres occupy to define individual masks for each fibre in it. The FRG algorithm begins to generate the fibre masks by randomly placing seed points within all capsule fibres. These seed points grow iteratively by adding neighbouring points to the fibre mask, employing a breadth-first-search approach through the grid. The seeds grow for a fixed number of iterations as long as they do not interfere with other fibresâ€™ boundaries. The propagation through random initialisations avoids uniform growth and adds irregularities to the fibre shape, allowing tortuous surface reconstructions in the fibre surfaces. Since the seeding is done inside capsules, the final axon radius in the mesh is related to the radii used in the capsules. Once the FRG step is completed, the fibre density of the particular configuration inputted is maximised. We compute the fibreâ€™s outer surface mesh using the fibre masks with the Lewiner marching cubes algorithm (Lewiner et al., 2003; Pedregosa et al., 2011). Then, we applied a Laplacian smoothing (Sorkine et al., 2004; Herrmann, 1976; Sullivan and Kaszynski, 2019) to remove sharp angles, and finally decimate the mesh to reduce the number of triangles without affecting the morphology of the substrates (Shekhar et al., 1996). Subsequently, we generate a new mesh representing the fibreâ€™s inner surface by eroding the previously estimated outer grid and following the same procedure for the meshing. The space between these two surfaces defines the myelin volume. 2.3.2 Implementation details Finally, we would like to elaborate on the technical implementation details of the FRG algorithm to mention the specific design choices we made to ensure its computational efficiency. FRG is implemented in Python (Van Rossum and Drake, 2009), parallelised with its multiprocessing ibraries (McKerns et al., 2012), and compiled with Numba (Lam et al., 2015). Image 3D processing and meshing are done using van der Walt et al. (2014); Sullivan and Kaszynski (2019); Hess (2010). Moreover, the FRG is designed to run a ball-tree structure (Moore et al., 2003) from the Sklearn library (Pedregosa et al., 2011) as a preprocessing to store fibres and their interactions. The fine-tuned FRG algorithmâ€™s design allows for the independent execution of fibre growth and meshing on multiple computers in a distributed manner, eliminating the need for multi-thread or computer synchronisation. 3 EXPERIMENTS To evaluate the performance of CACTUS, we designed a comprehensive set of substrates with specific geometries. Each experiment below involves several metrics essential for quantifying the microstructure properties of the brain white matter. The metrics include the axon volume fractions, the radius distribution per substrate, the radii change along the fibres, the myelin volume, the g-ratio, the orientation dispersion and bundle crossings. A 7Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS 3.1 Maximum fibre volume fraction In our first experiment, we aim to explore the macro-structural parameters of substrates, such as substrate size (i.e. the voxel size in MRI experiments), fibre dispersion, two bundle crossings, and the ability to create high-density packing substrates. We assess the maximum fibre volume fraction that CACTUS achieves in two scenarios: a single bundle and two bundles. In the single bundle case, we generated six substrates with mean angle dispersions of 0â—¦, 5â—¦, 10â—¦, 15â—¦, 20â—¦, and 25â—¦, respectively. In the two bundles case, we generated five crossing substrates with inter-bundle angles of 30â—¦, 45â—¦, 60â—¦, 75â—¦, and 90â—¦, and the fibres of each bundle were initialised with a mean angle dispersion of 5â—¦ around the main bundle orientation. 3.2 Substrates targeting predefined microstructure features The following two paragraphs describe experiments conducted to explore the ability of CACTUS to replicate desired microstructural parameters into its synthetic substrates. These parameters include the axon volume fraction (A VF), myelin volume fraction (MFV), g-ratio, and radii distribution. We compare the reference values taken from previous histological studies and those achieved by CACTUS. In the second experiment, we created a series of synthetic substrates that emulate the histological values reported by Stikov et al. (2015) in various white matter regions. Specifically, the target characteristics are the fibre volume fraction, myelin volume fraction, and aggregated g-ratio,g = p 1 âˆ’ MV F/F V F(Stikov et al., 2015). In our scenario, the axon volume fraction (A VF) is the volume of the fibre inner surface. The myelin volume fraction (MVF) represents the volume of the space between the inner and outer fibre surfaces. The fibre volume fraction (FVF) is the sum of A VF and MVF. In the last experiment, we investigated the effect of substrate size on radii distribution. To measure the radii distribution, for each fibre, we cut the mesh skeleton in an orthogonal plane at regular 1Âµm intervals and calculated the cross-sectional area of the polygon defined by the plane. The equivalent fibre radius is defined as the radius of a circle with the same area as the polygon (Lee et al., 2019). The global radii distribution per substrate was computed using the mean radius for each fibre. 4 RESULTS 4.1 Maximum fibre volume fraction Figure 3 shows the internal morphology of four substrates consisting of a single bundle with a dispersion of 0â—¦, 5â—¦, 10â—¦, and 20â—¦, respectively. All the substrates were generated with dimensions of(500Âµm)3. Table 1 (top panel) reports the substrate characteristics, including the number of fibres, the obtained fibre volume fraction, and the dispersion parameters. We note that the maximum fibre volume fraction decreased from 94.7% to 90.8% as the dispersion increased from 0â—¦ to 25â—¦. Results from the experiment generating bundle crossings with different inter-bundle angles are depicted in Figure 4 and Table 1 (bottom panel). Figure 4 displays a cross-section of the substrates, where each bundle has a distinctive colour for visualisation purposes. Although local perturbations in fibre trajectories (on the order of 5â—¦) may occur in the substrates due to the high fibre packing, the average bundle orientation is sustained. The bottom panel of Table 1 reports the fibre volume fraction of these bundle crossing substrates with inter-bundle angles of 30â—¦, 45â—¦, 60â—¦, 75â—¦, and 90â—¦. For all the evaluated substrates, the fibre volume fraction remains nearly constant at approximately 93% (92.2% âˆ’ 93.9%). CACTUS 8Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS Figure 3. (a-d) Meshâ€™s renders of cross-sections with dimensions of (100Âµm)2 to visualize the internal morphology of four substrates with a single bundle, each with different mean angular dispersion. For all cases, the outer surface volume is coloured black. The inner surface volume is superimposed over the outer volume and coloured grey. White represents the extracellular space, ie. the volume not occupied by any fibre. All bundles are vertically aligned, and the substrates were built to have a mean angular dispersion of a) 0â—¦, b) 5â—¦, c) 10â—¦, and d) 20â—¦, respectively. Figure 4. (a-e) Mesh renders of cross-sections with dimensions of (100Âµm)2 portraying the internal morphology of five substrates consisting of two bundles with different inter-bundle angles. In all cases, the outer surface volume of bundle 1 and bundle 2 is displayed in black. The inner surface volume is superimposed over the outer volume and coloured light grey for bundle 1 and dark grey for bundle 2. The extracellular space, representing the volume not occupied by any fibre, is coloured white. The dark grey fibres are aligned parallel to the X axis, and the light grey fibres crossed at angles of a) 30â—¦, b) 45â—¦, c) 60â—¦, d) 75â—¦, and e) 90â—¦. The outer volume, which is defined by the outer surfaces minus the inner volume, is coloured in black for both bundles. Extra axonal space is coloured in white. A 9Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS Nbr of Bundles Bundle dispersion (Î·) Crossing angle (Î¸) Nbr of fibres FVF 1 0â—¦ - 31,954 94.7% 1 5â—¦ - 31,023 93.4% 1 10â—¦ - 30,241 92.6% 1 15â—¦ - 30,412 92.2% 1 20â—¦ - 31,161 91.8% 1 25â—¦ - 31,863 90.8% 2 5â—¦ 30â—¦ 30,026 93.9% 2 5â—¦ 45â—¦ 30,712 93.3% 2 5â—¦ 60â—¦ 31,023 93.5% 2 5â—¦ 75â—¦ 31,152 92.3% 2 5â—¦ 90â—¦ 30,245, 92.2% Table 1. Substrate characteristics, including the number of bundles, mean dispersion angle, mean inter- bundle crossing angle, number of fibres per substrate, and fibre volume fraction (in per cent), respectively. The top and bottom panels correspond to the substrates with a single bundle and two bundles. Each row represents a different substrate. 4.2 Substrates targeting predefined microstructure features 4.2.1 Axon volume fraction, myelin volume fraction, and g-ratio We simulated various substrates of a single bundle to mimic microstructure properties previously reported in Stikov et al. (2015). The histological values used as a reference are the myelin volume fraction (MVF), fibre volume fraction (FVF), axonal volume fraction (A VF=FVF-MVF), and g-ratio. The values achieved by CACTUS are shown in Table 2. The difference between the target and obtained substrate properties was lower than 2% in all cases. Examples of the generated substrates and histology data are shown in Figure 5. Electron microscopy images were generously provided by Prof. Nikola Stikov and Dr. Jennifer Campbell, and are used to highlight the geometric similarities of synthetic fibre shapes. Target Achieved Substrate A VF MVF FVF g-ratio A VF MVF FVF g-ratio a) 25 35 60 64.5 26.0 36.0 62 64.7 b) 25 43 68 60.6 26.3 43.6 69.9 61.3 c) 31 44 75 64.2 32.2 43.8 76.07 65 d) 39 37 76 71.6 41.2 35.0 76.0 73.5 Table 2. Target microstructure histological properties (left) reported in Stikov et al. (2015), and corresponding properties of the substrates generated by CACTUS (right). The axon volume fraction (A VF) is the volume of the inner axon surface. The myelin volume fraction (MVF) represents the volume of the space between the inner and outer axon surfaces. The fibre volume fraction (FVF) is the sum of A VF and MVF. The aggregated g-ratio,g = p 1 âˆ’ MV F/F V Fis equal to the mean inner and outer axon radius ratio for all the fibres in the substrate. CACTUS 10Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS Figure 5. Cross-sections of the synthetic substrates constructed to match the statistics of the histological values reported in Table 2. The substrate dimension is(300Âµm)3. For visualisation purposes, the axonal space is coloured blue, and the myelin is red, and extra axonal space is coloured white. Panels a) - d) correspond to the same substrates shown in Table 2. The bottom panel shows the representative histological images courtesy of Prof. Nikola Stikov and Dr. Jennifer Campbell. The EM images are used to show the similarities in fibre shape and packing. 4.2.2 Radii distribution and substrate size Figure 6 show the CACTUS substrates with different sizes, ranging from (30Âµm)3 to (500Âµm)3, and the target and empirical radius distributions obtained for each substrate. The empirical radius distributions closely replicated the targeted ones for substrates equal to or bigger than (200Âµm)3. The optimisation algorithm step ran for approximately 4 hours for the largest substrate (right panel) on a node with 64 cores (2.4 GHz) and 400 Mb of RAM. The reconstruction time of the FRG algorithm was approximately one minute per fibre, using one core with 500 Mbs of memory per core. We extracted three representative fibre segments from the substrates shown in Figure 5 and displayed them in Figure 7. The top panel of the figure exhibits the cross-sections of the outer and inner surfaces of the fibre, along with the cross-sections of their diameters. The bottom panel shows the diameter distribution of each axon. We observed that, regardless of the tortuosity of the fibre trajectory, the diameter distribution of both the inner and outer diameters of all three cases was centred around the target diameter. A 11Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS Figure 6. Four 3D substrates of varying sizes: a) 503, b) 1003, c) 2003, and d) 5003 (Âµm)3, with 341, 1316, 4859, and 33478 fibres, respectively. The empirical and target radii distributions are displayed on the bottom of each substrate. The empirical distribution better approximates the target distribution as substrate size increases. 5 DISCUSSION Over the last 20 years, Monte-Carlo diffusion simulations have been used to optimise DW-MRI data acquisition protocols and validate microstructure models. Nevertheless, doubts have been raised regarding the accuracy of the simple geometries used to construct the diffusion substrates. In this work, we introduced CACTUS, a novel framework to produce numerical substrates mimicking white matter tissue with rich microstructural features, closely matching the desired input parameters. Those parameters include the target distribution for inner and outer axon radii, g-ratio, orientation dispersion, fibre crossings, curvature, and packing density. The high versatility of CACTUS is founded on its efficient computational implementation and its mathematical formulation divided into three algorithmic steps (substrate initialisation, joint fibre optimisation, and fibre radial growth) composed of various competing terms controlling different substrate parameters. To generate the substrates, we introduced a new algorithm to initialise fibre bundles with a target mean degree of orientation dispersion. Moreover, we introduced a novel capsule-based parametrisation for optimising fibre structures. Compared to circle parametrisations (Close et al., 2009; Ginsburger et al., 2019), the capsule parameterisation requires fewer parameters, reducing the complexity of the optimisation problem. We adapted the cost functions inspired by Close et al. (2009); Ginsburger et al. (2019) for capsules and provided analytical derivatives, making the optimisation faster and computationally more efficient. Finally, we proposed the fibre radial growth algorithm, which increases the fibre packing density in white matter substrates. CACTUS 12Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS Figure 7. Representative group of three fibres extracted from the substrate shown in Figure 5. In the top panel, we display the fibres with varying diameters and tortuous trajectories. The straightest fibre is presented on the left, while the most tortuous one is displayed on the right. We display each fibreâ€™s outer surface in red and its inner surface in blue. We also show the fibreâ€™s skeleton in black and cross-sections orthogonal to the fibreâ€™s skeleton. The diameter is measured every 1Âµm along its trajectory. The cross- section cut of the outer surface is shown in red, and the cross-section cut of the inner surface is shown in blue. The bottom panel presents the violin plots of the outer and inner diameters measured. The red (blue) dotted line represents the target outer (inner) diameter of the three fibres. CACTUS was able to enhance the complexity of the fibre microstructure. In particular, our results showed CACTUS can produce substrate with fibre volume fraction beyond the 75% previously achieved. CACTUS reached high fibre volume fractions, up to 95% in its substrates (Table 1). Moreover, it consistently reached fibre volume fractions superior to 90% at all the various levels of bundle dispersion and crossing angles (Table 1 and Figure 4). In the single bundle case, the fibre volume fraction was the highest at 94.7% when fibres were aligned and decreased to 90.8% with increasing mean angular dispersion. Conversely, the fibre volume fraction remained consistently around approximately 93% in the two-bundle cases, regardless of the crossing angle. However, we note that the packing complexity of substrates with a single bundle and two bundles crossing differs. The former mimics the spatial arrangement of thousands of fibres with different crossing angles, which may produce more empty pockets between fibres and less densely packed substrates. A 13Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS Another important feature of CACTUS is that it can create substrates with statistical characteristics informed by histological data. Indeed, we can closely adhere to the target statistics of axon volume fraction, myelin volume fraction, and g-ratio reported in histological studies (Stikov et al., 2015) (see Figure 5). In all cases, the difference between the target and obtained substrate properties was lower than 2% (see Table 1). Notably, CACTUS is the first tool incorporating the g-ratio as a target characteristic and successfully matching it for large-scale substrates. Also, CACTUS has the capability to generate substrates with a targeted radii distribution. In our experiments, the approximation of the target distribution improves as substrate size increases, as illustrated in Figure 6, underscoring the importance of generating large substrates. Furthermore, we have the availability to measure fibre geometry accurately. For instance, as seen in Figure 7, the generated fibres have a non-constant longitudinal radius and non-circular cross-sections. Despite the tortuous trajectories of the fibres, the diameter distribution remains centred around the target mean outer (inner) diameter of1.5Âµm (1.1Âµm). Additionally, the diameter distribution presented replicates the diameter variations observed in 3D synchrotron images (Andersson et al., 2020), including longitudinal changes and a lack of skewness. Finally, while previous works were able to achieve substrate sizes between (30Âµm)3 and (100Âµm)3, CACTUS demonstrated a substantial improvement in the generation of larger substrates (Callaghan et al., 2020; Ginsburger et al., 2019). As shown in Figure 6, CACTUS generated substrate sizes ranging from (50Âµm)3 to (500Âµm)3, all with up to a 95% fibre volume fraction. Our toolâ€™s ability to generate larger substrate sizes is advantageous for Monte-Carlo diffusion simulations in DW-MRI as it has been shown in previous studies (Rafael-Patino et al., 2020), that substrate sizes larger than (200Âµm)3 can reduce the sampling bias caused by smaller substrate sizes, potentially leading to more accurate DW-MRI numerical simulations Rafael-Patino et al. (2020); Romascano et al. (2018). In addition, the ability to generate large substrate sizes is advantageous as DW-MRI modelling is moving towards incorporating more microstructure features such as somas, astroglia, and vascularity (Schneider-Mizell et al., 2021; Dyer et al., 2017; Lin et al., 2018). This makes the generation of large substrates essential for capturing these additional features and moving towards more accurate and comprehensive microstructure imaging. 5.1 Limitations and future work Although CACTUS incorporates complex microstructural features required to mimic some of the most relevant white matter geometrical properties, it still requires fibre-modelling assumptions to reduce the computational burden. Also, CACTUS generates substrates with characteristics resembling those from healthy white matter, but generating pathological tissue requires additional work, which we reserve for future studies. Additionally, CACTUS focuses solely on generating white matter fibre structures. However, its capacity to generate large substrate sizes expands the potential for including other tissue components in future studies, such as astrocytes, oligodendrocytes, microglia, and capillaries. Finally, although CACTUS output substrates are suitable for simulators like the MCDC (Rafael-Patino et al., 2020), a thorough analysis is necessary to comprehend the influence of mesh quality, like the number of triangles, on the DW-MRI signals generated by Monte-Carlo simulation. Such analysis is crucial for developing computationally viable simulations. CACTUS 14Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS 5.2 Applications beyond diffusion MR The applications of CACTUS are not limited to studying white matter microstructure using DW-MRI. For instance, it can be applied in DW-MRI studies outside the brain (Adelnia et al., 2019), where muscle fibres are organised into fascicles. The microscopic arrangement of muscle fibres can vary between different muscle groups, regions of the same muscle, and multiple pathological conditions (Berry et al., 2018). Moreover, the fibre meshes generated by CACTUS could be used in other applications, like Polarized Light Imaging (PLI) (Amunts and Axer, 2019; Menzel et al., 2015), a technique used to infer the local fibre orientation in histological brain sections based on the birefringent properties of the myelin sheaths. The limitations of the birefringence PLI model were investigated in (Menzel et al., 2015) by generating synthetic PLI data from a hexagonal bundle of straight parallel cylindrical fibres. Although a more general fibre constructor was recently proposed for validating 3D-PLI techniques (Amunts and Axer, 2019), the white matter substrates generated in our study could provide more realistic geometries for conducting similar studies. 6 CONCLUSION The generation of realistic substrates is critical for validating DW-MRI models, as it allows researchers to simulate and analyse the effect of microstructural changes on the DW-MRI signal. In this work, we introduced CACTUS, a novel framework for generating axonal-like substrates with predefined geometrical features of interest. Our experiments show that CACTUS can generate white matter substrates with the desired spatial dimensions, fibre radii, g-ratio, non-circular cross-sections, tortuous trajectories, smooth surfaces, predefined inter-fibre angles and fibre dispersion. Notably, the generated fibre substrates reached up to 95% fibre volume fraction, the highest density reported in the literature to date, in agreement with previous histology studies. We also generated the large substrates/voxels of up to (500Âµm)3, with dimensions similar to or higher than those used in preclinical MRI scanners, reducing the gap between numerical and real voxel sizes. In conclusion, the CACTUS substrate generator tool presented in this study has the potential to advance white matter microstructure modelling. It provides a versatile and customisable platform for generating fibre substrates with quantifiable geometrical characteristics. It is open-source and accessible to the broader research community at http:/cactus.epfl.ch, facilitating the validation and comparison of current and future DW-MRI models. 7 ACKNOWLEDGMENTS This work is supported by the Swiss National Science Foundation under grants 205320 175974 and 205320 204097. We acknowledge access to the facilities and expertise of the CIBM Center for Biomedical Imaging, a Swiss research centre of excellence founded and supported by Lausanne University Hospital (CHUV), University of Lausanne (UNIL), Ecole Polytechnique Federale de Lausanne (EPFL), University of Geneva (UNIGE) and Geneva University Hospitals (HUG). Erick J. Canales-RodrÂ´Ä±guez was supported by the Swiss National Science Foundation (Ambizione grant P Z00P2 185814). We want to express our gratitude to Prof. Nikola Stikov and Dr. Jennifer Campbell for generously donating the histology images from their previous work ?. We thank Dr. Thomas Yu for his help proofreading this manuscript. A 15Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS 8 DATA AVAILABILITY The substrate meshes generated in this work and the source code are available at http://cactus. epfl.ch. 9 AUTHOR CONTRIBUTION REFERENCES Adelnia, F., Shardell, M., Bergeron, C. M., Fishbein, K. W., Spencer, R. G., Ferrucci, L., et al. (2019). Diffusion-weighted mri with intravoxel incoherent motion modeling for assessment of muscle perfusion in the thigh during post-exercise hyperemia in younger and older adults. NMR in Biomedicine 32, e4072 Alexander, D. C., Hubbard, P. L., Hall, M. G., Moore, E. A., Ptito, M., Parker, G. J., et al. (2010). Orientationally invariant indices of axon diameter and density from diffusion mri. NeuroImage 52, 1374â€“1389. doi:https://doi.org/10.1016/j.neuroimage.2010.05.043 Altendorf, H. and Jeulin, D. (2011). Random-walk-based stochastic modeling of three-dimensional fiber systems. Physical Review E 83, 041804 Amunts, K. and Axer, M. (2019). Dense fiber modeling for 3d-polarized light imaging simulations. Future Trends of HPC in a Disruptive Scenario 34, 240 Andersson, M., Kjer, H. M., Rafael-Patino, J., Pacureanu, A., Pakkenberg, B., Thiran, J.-P., et al. (2020). Axon morphology is modulated by the local environment and impacts the noninvasive investigation of its structureâ€“function relationship. Proceedings of the National Academy of Sciences 117, 33649â€“33659 Assaf, Y ., Blumenfeld-Katzir, T., Yovel, Y ., and Basser, P. J. (2008). Axcaliber: A method for measuring axon diameter distribution from diffusion mri. Magnetic Resonance in Medicine 59, 1347â€“1354. doi:https://doi.org/10.1002/mrm.21577 Assaf, Y ., Freidlin, R. Z., Rohde, G. K., and Basser, P. J. (2004). New modeling and experimental framework to characterize hindered and restricted water diffusion in brain white matter. Magnetic Resonance in Medicine 52, 965â€“978. doi:https://doi.org/10.1002/mrm.20274 Baxter, G. T. and Frank, L. R. (2013). A computational model for diffusion weighted imaging of myelinated white matter. Neuroimage 75, 204â€“212 Berry, D. B., Regner, B., Galinsky, V ., Ward, S. R., and Frank, L. R. (2018). Relationships between tissue microstructure and the diffusion tensor in simulated skeletal muscle. Magnetic resonance in medicine 80, 317â€“329 Bihan, D. L. (1995). Molecular diffusion, tissue microdynamics and microstructure. NMR in Biomedicine 8, 375â€“386 BrÂ¨uckner, G., H Â¨artig, W., Kacza, J., Seeger, J., Welt, K., and Brauer, K. (1996). Extracellular matrix organization in various regions of rat brain grey matter. Journal of neurocytology 25, 333â€“346 Budde, M. D. and Frank, J. A. (2010). Neurite beading is sufficient to decrease the apparent diffusion coefficient after ischemic stroke. Proceedings of the National Academy of Sciences 107, 14472â€“14477 Callaghan, R., Alexander, D. C., Palombo, M., and Zhang, H. (2020). ConFiG: Contextual Fibre Growth to generate realistic axonal packing for diffusion MRI simulation. NeuroImage 220, 117107. doi:10.1016/j.neuroimage.2020.117107 Caminiti, R., Carducci, F., Piervincenzi, C., Battaglia-Mayer, A., Confalone, G., Visco-Comandini, F., et al. (2013). Diameter, length, speed, and conduction delay of callosal axons in macaque monkeys and humans: comparing data from histology and magnetic resonance imaging diffusion tractography. Journal of Neuroscience 33, 14501â€“14511 CACTUS 16Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS Campbell, J. S. W., Siddiqi, K., Rymar, V . V ., Sadikot, A. F., and Pike, G. B. (2005). Flow-based fiber tracking with diffusion tensor and q-ball data: validation and comparison to principal diffusion direction techniques. NeuroImage 27, 725â€“36. doi:10.1016/j.neuroimage.2005.05.014 Canales-RodrÂ´Ä±guez, E. J., Legarreta, J. H., Pizzolato, M., Rensonnet, G., Girard, G., Rafael-Patino, J., et al. (2019). Sparse wars: A survey and comparative study of spherical deconvolution algorithms for diffusion mri. NeuroImage 184, 140â€“160. doi:https://doi.org/10.1016/j.neuroimage.2018.08.071 Canales-RodrÂ´Ä±guez, E. J., Pizzolato, M., Piredda, G. F., Hilbert, T., Kunz, N., Pot, C., et al. (2021). Comparison of non-parametric t2 relaxometry methods for myelin water quantification. Medical Image Analysis 69, 101959. doi:https://doi.org/10.1016/j.media.2021.101959 Chandra, R., Dagum, L., Kohr, D., Menon, R., Maydan, D., and McDonald, J. (2001). Parallel programming in OpenMP (Morgan kaufmann) Close, T. G., Tournier, J. D., Calamante, F., Johnston, L. A., Mareels, I., and Connelly, A. (2009). A software tool to generate simulated white matter structures for the assessment of fibre-tracking algorithms. NeuroImage 47, 1288â€“1300. doi:10.1016/j.neuroimage.2009.03.077 CË†otÂ´e, M.-A., Girard, G., BorÂ´e, A., Garyfallidis, E., Houde, J.-C., and Descoteaux, M. (2013). Tractometer: Towards validation of tractography pipelines. Medical Image Analysis 17, 844 â€“ 857. doi:http: //dx.doi.org/10.1016/j.media.2013.03.009. Special Issue on the 2012 Conference on Medical Image Computing and Computer Assisted Intervention Daducci, A., Canales-Rodr Â´Ä±guez, E. J., Zhang, H., Dyrby, T. B., Alexander, D. C., and Thiran, J.-P. (2015). Accelerated microstructure imaging via convex optimization (amico) from diffusion mri data. NeuroImage 105, 32â€“44. doi:https://doi.org/10.1016/j.neuroimage.2014.10.026 Duchi, J., Hazan, E., and Singer, Y . (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research12 Dyer, E. L., Roncal, W. G., Prasad, J. A., Fernandes, H. L., G Â¨ursoy, D., De Andrade, V ., et al. (2017). Quantifying mesoscale neuroanatomy using x-ray microtomography. eneuro 4 Dyrby, T. B., BaarÂ´e, W. F., Alexander, D. C., Jelsing, J., Garde, E., and SÃ¸gaard, L. V . (2011). An ex vivo imaging pipeline for producing high-quality and high-resolution diffusion-weighted imaging datasets. Human Brain Mapping 32, 544â€“563. doi:https://doi.org/10.1002/hbm.21043 Fieremans, E., De Deene, Y ., Delputte, S.,Â¨Ozdemir, M. S., Dâ€™Asseler, Y ., Vlassenbroeck, J., et al. (2008). Simulation and experimental verification of the diffusion in an anisotropic fiber phantom. Journal of Magnetic Resonance 190, 189â€“199. doi:10.1016/j.jmr.2007.10.014 Fillard, P., Descoteaux, M., Goh, A., Gouttard, S., Jeurissen, B., Malcolm, J., et al. (2011). Quantitative evaluation of 10 tractography algorithms on a realistic diffusion MR phantom. NeuroImage 56, 220â€“234. doi:10.1016/j.neuroimage.2011.01.032 Ginsburger, K., Matuschke, F., Poupon, F., Mangin, J. F., Axer, M., and Poupon, C. (2019). MEDUSA: A GPU-based tool to create realistic phantoms of the brain microstructure using tiny spheres. NeuroImage 193, 10â€“24. doi:10.1016/j.neuroimage.2019.02.055 Hall, M. G. and Alexander, D. C. (2009a). Convergence and parameter choice for monte-carlo simulations of diffusion mri. IEEE transactions on medical imaging 28, 1354â€“1364 Hall, M. G. and Alexander, D. C. (2009b). Convergence and Parameter Choice for Monte-Carlo Simulations of Diffusion MRI. IEEE Transactions on Medical Imaging 28, 1354â€“1364. doi:10.1109/TMI.2009. 2015756 Herrmann, L. R. (1976). Laplacian-isoparametric grid generation scheme. Journal of the Engineering Mechanics Division 102, 749â€“756 Hess, R. (2010). Blender Foundations: The Essential Guide to Learning Blender 2.6 (Focal Press) A 17Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS Innocenti, G. M. and Caminiti, R. (2017). Axon diameter relates to synaptic bouton size: structural properties define computationally different types of cortical connections in primates. Brain Structure and Function 222, 1169â€“1177 Jelescu, I. O. and Budde, M. D. (2017). Design and validation of diffusion mri models of white matter. Frontiers in physics 28. doi:10.3389/fphy.2017.00061 KerkelÂ¨a, L., Nery, F., Callaghan, R., Zhou, F., Gyori, N. G., Szczepankiewicz, F., et al. (2021). Comparative analysis of signal models for microscopic fractional anisotropy estimation using q-space trajectory encoding. NeuroImage 242, 118445 Korogod, N., Petersen, C. C., and Knott, G. W. (2015). Ultrastructural analysis of adult mouse neocortex comparing aldehyde perfusion with cryo fixation. elife 4, e05793 [Dataset] Lam, S. K., Pitrou, A., and Seibert, S. (2015). Numba: A llvm-based python jit compiler. doi:10.1145/2833157.2833162 Lavdas, I., Behan, K. C., Papadaki, A., McRobbie, D. W., and Aboagye, E. O. (2013). A phantom for diffusion-weighted MRI (DW-MRI). Journal of Magnetic Resonance Imaging 38, 173â€“179. doi:10. 1002/jmri.23950 Lee, H.-H., Jespersen, S. N., Fieremans, E., and Novikov, D. S. (2020). The impact of realistic axonal shape on axon diameter estimation using diffusion mri. NeuroImage 223, 117228. doi:https://doi.org/10. 1016/j.neuroimage.2020.117228 Lee, H.-H., Yaros, K., Veraart, J., Pathan, J. L., Liang, F.-X., Kim, S. G., et al. (2019). Along-axon diameter variation and axonal orientation dispersion revealed with 3d electron microscopy: implications for quantifying brain white matter microstructure with histology and diffusion mri. Brain Structure and Function 224, 1469â€“1488 Lewiner, T., Lopes, H., Vieira, A. W., and Tavares, G. (2003). Efficient implementation of marching cubesâ€™ cases with topological guarantees. Journal of graphics tools 8, 1â€“15 Lin, C., Huang, Y ., Quan, T., and Zhang, Y . (2018). Modelling brain-wide neuronal morphology via rooted cayley trees. Scientific reports 8, 1â€“10 Mackay, A., Whittall, K., Adler, J., Li, D., Paty, D., and Graeb, D. (1994). In vivo visualization of myelin water in brain by magnetic resonance. Magnetic Resonance in Medicine 31, 673â€“677. doi:https://doi.org/10.1002/mrm.1910310614 Maier-Hein, K. H., Neher, P. F., Houde, J.-C., CË†otÂ´e, M.-A., Garyfallidis, E., Zhong, J., et al. (2017). The challenge of mapping the human connectome based on diffusion tractography. Nature communications 8, 1349 McKerns, M. M., Strand, L., Sullivan, T., Fang, A., and Aivazis, M. A. (2012). Building a framework for predictive science. arXiv preprint arXiv:1202.1056 Menzel, M., Michielsen, K., De Raedt, H., Reckfort, J., Amunts, K., and Axer, M. (2015). A jones matrix formalism for simulating three-dimensional polarized light imaging of brain tissue. Journal of the Royal Society Interface 12, 20150734 Mingasson, T., Duval, T., Stikov, N., and Cohen-Adad, J. (2017). Axonpacking: an open-source software to simulate arrangements of axons in white matter. Frontiers in neuroinformatics11, 5 Moore, A., Gray, A., et al. (2003). New algorithms for efficient high dimensional non-parametric classification. Advances in Neural Information Processing Systems 16 Murday, J. S. and Cotts, R. M. (1968). Self-diffusion coefficient of liquid lithium. The Journal of Chemical Physics 48, 4938â€“4945. doi:10.1063/1.1668160 CACTUS 18Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS Narvaez-Delgado, O., Rojas-Vite, G., Coronado-Leija, R., Ram Â´Ä±rez-Manzanares, A., MarroquÂ´Ä±n, J. L., Noguez-Imm, R., et al. (2019). Histological and diffusion-weighted magnetic resonance imaging data from normal and degenerated optic nerve and chiasm of the rat. Data in brief 26, 104399 Neher, P. F., Laun, F. B., Stieltjes, B., and Maier-Hein, K. H. (2014). Fiberfox: Facilitating the creation of realistic white matter software phantoms. Magnetic Resonance in Medicine 72, 1460â€“1470. doi:10. 1002/mrm.25045 Neuman, C. H. (1974). Spin echo of spins diffusing in a bounded medium. The Journal of Chemical Physics 60, 4508â€“4511. doi:10.1063/1.1680931 Nilsson, M., LasiË‡c, S., Drobnjak, I., Topgaard, D., and Westin, C.-F. (2017). Resolution limit of cylinder diameter estimation by diffusion mri: The impact of gradient waveform and orientation dispersion. NMR in Biomedicine 30, e3711 Nilsson, M., LÂ¨att, J., StËšahlberg, F., van Westen, D., and HagslÂ¨att, H. (2012). The importance of axonal undulation in diffusion mr measurements: a monte carlo simulation study. NMR in Biomedicine 25, 795â€“805 Novikov, D. S., Fieremans, E., Jespersen, S. N., and Kiselev, V . G. (2019). Quantifying brain microstructure with diffusion mri: Theory and parameter estimation. NMR in Biomedicine 32, e3998. doi:https: //doi.org/10.1002/nbm.3998. E3998 nbm.3998 Novikov, D. S., Kiselev, V . G., and Jespersen, S. N. (2018). On modeling.Magnetic Resonance in Medicine 79, 3172â€“3193. doi:https://doi.org/10.1002/mrm.27101 Parizel, P. M., Van Goethem, J., Â¨Ozsarlak, Â¨O., Maes, M., and Phillips, C. (2005). New developments in the neuroradiological diagnosis of craniocerebral trauma. European radiology 15, 569â€“581 Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V ., Thirion, B., Grisel, O., et al. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12, 2825â€“2830 Plante, I. and Cucinotta, F. A. (2013). Monte-carlo simulation of particle diffusion in various geometries and application to chemistry and biology. Theory and Applications of Monte Carlo Simulations , 193â€“225 Rafael-Patino, J., Girard, G., Truffet, R., Pizzolato, M., Thiran, J.-P., and Caruyer, E. (2021). The microstructural features of the diffusion-simulated connectivity (disco) dataset. In Computational Diffusion MRI: 12th International Workshop, CDMRI 2021, Held in Conjunction with MICCAI 2021, Strasbourg, France, October 1, 2021, Proceedings 12(Springer), 159â€“170 Rafael-Patino, J., Romascano, D., Ramirez-Manzanares, A., Canales-Rodr Â´Ä±guez, E. J., Girard, G., and Thiran, J.-P. (2020). Robust monte-carlo simulations in diffusion-mri: Effect of the substrate complexity and parameter choice on the reproducibility of results. Frontiers in neuroinformatics14, 8 Romascano, D., Rafael-Patino, J., Jelescu, I., Barakovic, M., Tim, B., Jean-Philippe, D., et al. (2018). V oxel size matters: big voxels are required to generate realistic extra-axonal dmri signals from monte carlo simulations. In ISMRM. 1â€“2 Schilling, K., Gao, Y ., Janve, V ., Stepniewska, I., Landman, B. A., and Anderson, A. W. (2017). Can increased spatial resolution solve the crossing fiber problem for diffusion mri? NMR in Biomedicine 30, e3787 Schilling, K. G., Nath, V ., Hansen, C., Parvathaneni, P., Blaber, J., Gao, Y ., et al. (2019). Limits to anatomical accuracy of diffusion tractography using modern approaches. Neuroimage 185, 1â€“11 Schneider-Mizell, C. M., Bodor, A. L., Collman, F., Brittain, D., Bleckert, A., Dorkenwald, S., et al. (2021). Structure and function of axo-axonic inhibition. Elife 10, e73783 A 19Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS Sepehrband, F., Alexander, D. C., Clark, K. A., Kurniawan, N. D., Yang, Z., and Reutens, D. C. (2016). Parametric probability distribution functions for axon diameters of corpus callosum. Frontiers in neuroanatomy 10, 59 Shekhar, R., Fayyad, E., Yagel, R., and Cornhill, J. F. (1996). Octree-based decimation of marching cubes surfaces. In Proceedings of Seventh Annual IEEE Visualizationâ€™96 (IEEE), 335â€“342 SÂ¨oderman, O. and JÂ¨onsson, B. (1995). Restricted Diffusion in Cylindrical Geometry. Journal of Magnetic Resonance, Series A 117, 94â€“97. doi:10.1006/jmra.1995.0014 Sorkine, O., Cohen-Or, D., Lipman, Y ., Alexa, M., RÂ¨ossl, C., and Seidel, H.-P. (2004). Laplacian surface editing. In Proceedings of the 2004 Eurographics/ACM SIGGRAPH symposium on Geometry processing. 175â€“184 Sporns, O. (2011). The non-random brain: efficiency, economy, and complex dynamics. Frontiers in computational neuroscience 5, 5 Stikov, N., Campbell, J. S., Stroh, T., LavelÂ´ee, M., Frey, S., Novek, J., et al. (2015). In vivo histology of the myelin g-ratio with magnetic resonance imaging. Neuroimage 118, 397â€“405 Stroustrup, B. (1999). An overview of the c++ programming language. Handbook of object technology Sullivan, B. and Kaszynski, A. (2019). PyVista: 3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK). Journal of Open Source Software 4, 1450. doi:10.21105/ joss.01450 SykovÂ´a, E. and Nicholson, C. (2008). Diffusion in brain extracellular space. Physiological reviews 88, 1277â€“1340 To, X. V ., Mohamed, A. Z., Cumming, P., and Nasrallah, F. A. (2022). Subacute cytokine changes after a traumatic brain injury predict chronic brain microstructural alterations on advanced diffusion imaging in the male rat. Brain, behavior, and immunity102, 137â€“150 TÃ¸nnesen, J., Inavalli, V . K., and NÂ¨agerl, U. V . (2018). Super-resolution imaging of the extracellular space in living brain tissue. Cell 172, 1108â€“1121 Tournier, J.-D., Calamante, F., and Connelly, A. (2007). Robust determination of the fibre orientation distribution in diffusion mri: Non-negativity constrained super-resolved spherical deconvolution. NeuroImage 35, 1459â€“1472. doi:https://doi.org/10.1016/j.neuroimage.2007.02.016 Tournier, J. D., Yeh, C. H., Calamante, F., Cho, K. H., Connelly, A., and Lin, C. P. (2008). Resolving crossing fibres using constrained spherical deconvolution: Validation using diffusion-weighted imaging phantom data. NeuroImage 42, 617â€“625. doi:10.1016/j.neuroimage.2008.05.002 Truffet, R., Rafael-Patino, J., Girard, G., Pizzolato, M., Barillot, C., Thiran, J.-P., et al. (2020). An evolutionary framework for microstructure-sensitive generalized diffusion gradient waveforms. In International Conference on Medical Image Computing and Computer-Assisted Intervention (Springer), 94â€“103 Tuch, D. S. (2004). Q-ball imaging. Magnetic Resonance in Medicine 52, 1358â€“1372. doi:https: //doi.org/10.1002/mrm.20279 Turau, V . (1991). Fixed-radius near neighbors search.Information Processing Letters 39, 201â€“203 van der Walt, S., SchÂ¨onberger, J. L., Nunez-Iglesias, J., Boulogne, F., Warner, J. D., Yager, N., et al. (2014). scikit-image: image processing in Python. PeerJ 2, e453. doi:10.7717/peerj.453 van Gelderen, P., de Vleeschouwer, M. H. M., DesPres, D., Pekar, J., van Zijl, P. C. M., and Moonen, C. T. W. (1994). Water diffusion and acute stroke. Magnetic Resonance in Medicine 31, 154â€“163. doi:10.1002/mrm.1910310209 Van Rossum, G. and Drake, F. L. (2009). Python 3 Reference Manual (Scotts Valley, CA: CreateSpace) CACTUS 20Villarreal-Haro et al. Generating Realistic White Matter Substrates with CACTUS Van Verth, J. M. and Bishop, L. M. (2015). Essential mathematics for games and interactive applications (CRC Press) Veraart, J., Raven, E. P., Edwards, L. J., Weiskopf, N., and Jones, D. K. (2021). The variability of mr axon radii estimates in the human white matter. Human Brain Mapping 42, 2201â€“2213. doi:https: //doi.org/10.1002/hbm.25359 Zhang, H., Schneider, T., Wheeler-Kingshott, C. A., and Alexander, D. C. (2012). Noddi: practical in vivo neurite orientation dispersion and density imaging of the human brain. Neuroimage 61, 1000â€“1016 Zhou, F.-L., Li, Z., Gough, J. E., Cristinacce, P. L. H., and Parker, G. J. (2018). Axon mimicking hydrophilic hollow polycaprolactone microfibres for diffusion magnetic resonance imaging. Materials & design 137, 394â€“403 A 21",
      "references": [
        "Diffusion-weighted mri with intravoxel incoherent motion modeling for assessment of muscle perfusion in the thigh during post-exercise hyperemia in younger and older adults.",
        "Orientationally invariant indices of axon diameter and density from diffusion mri.",
        "Random-walk-based stochastic modeling of three-dimensional fiber systems.",
        "Dense fiber modeling for 3d-polarized light imaging simulations.",
        "Axon morphology is modulated by the local environment and impacts the noninvasive investigation of its structureâ€“function relationship.",
        "Axcaliber: A method for measuring axon diameter distribution from diffusion mri.",
        "New modeling and experimental framework to characterize hindered and restricted water diffusion in brain white matter.",
        "A computational model for diffusion weighted imaging of myelinated white matter.",
        "Relationships between tissue microstructure and the diffusion tensor in simulated skeletal muscle.",
        "Molecular diffusion, tissue microdynamics and microstructure.",
        "Extracellular matrix organization in various regions of rat brain grey matter.",
        "Neurite beading is sufficient to decrease the apparent diffusion coefficient after ischemic stroke.",
        "ConFiG: Contextual Fibre Growth to generate realistic axonal packing for diffusion MRI simulation.",
        "Diameter, length, speed, and conduction delay of callosal axons in macaque monkeys and humans: comparing data from histology and magnetic resonance imaging diffusion tractography.",
        "Flow-based fiber tracking with diffusion tensor and q-ball data: validation and comparison to principal diffusion direction techniques.",
        "Sparse wars: A survey and comparative study of spherical deconvolution algorithms for diffusion mri.",
        "Comparison of non-parametric t2 relaxometry methods for myelin water quantification.",
        "Parallel programming in OpenMP",
        "A software tool to generate simulated white matter structures for the assessment of fibre-tracking algorithms.",
        "Tractometer: Towards validation of tractography pipelines.",
        "Accelerated microstructure imaging via convex optimization (amico) from diffusion mri data.",
        "Adaptive subgradient methods for online learning and stochastic optimization.",
        "Quantifying mesoscale neuroanatomy using x-ray microtomography.",
        "An ex vivo imaging pipeline for producing high-quality and high-resolution diffusion-weighted imaging datasets.",
        "Simulation and experimental verification of the diffusion in an anisotropic fiber phantom.",
        "Quantitative evaluation of 10 tractography algorithms on a realistic diffusion MR phantom.",
        "MEDUSA: A GPU-based tool to create realistic phantoms of the brain microstructure using tiny spheres.",
        "Convergence and parameter choice for monte-carlo simulations of diffusion mri.",
        "Laplacian-isoparametric grid generation scheme.",
        "Blender Foundations: The Essential Guide to Learning Blender 2.6",
        "Axon diameter relates to synaptic bouton size: structural properties define computationally different types of cortical connections in primates.",
        "Design and validation of diffusion mri models of white matter.",
        "Comparative analysis of signal models for microscopic fractional anisotropy estimation using q-space trajectory encoding.",
        "Ultrastructural analysis of adult mouse neocortex comparing aldehyde perfusion with cryo fixation.",
        "Numba: A llvm-based python jit compiler.",
        "A phantom for diffusion-weighted MRI (DW-MRI).",
        "The impact of realistic axonal shape on axon diameter estimation using diffusion mri.",
        "Along-axon diameter variation and axonal orientation dispersion revealed with 3d electron microscopy: implications for quantifying brain white matter microstructure with histology and diffusion mri.",
        "Efficient implementation of marching cubesâ€™ cases with topological guarantees.",
        "Modelling brain-wide neuronal morphology via rooted cayley trees.",
        "In vivo visualization of myelin water in brain by magnetic resonance.",
        "The challenge of mapping the human connectome based on diffusion tractography.",
        "Building a framework for predictive science.",
        "A jones matrix formalism for simulating three-dimensional polarized light imaging of brain tissue.",
        "Axonpacking: an open-source software to simulate arrangements of axons in white matter.",
        "New algorithms for efficient high dimensional non-parametric classification.",
        "Self-diffusion coefficient of liquid lithium.",
        "Histological and diffusion-weighted magnetic resonance imaging data from normal and degenerated optic nerve and chiasm of the rat.",
        "Fiberfox: Facilitating the creation of realistic white matter software phantoms.",
        "Spin echo of spins diffusing in a bounded medium.",
        "Resolution limit of cylinder diameter estimation by diffusion mri: The impact of gradient waveform and orientation dispersion.",
        "The importance of axonal undulation in diffusion mr measurements: a monte carlo simulation study.",
        "Quantifying brain microstructure with diffusion mri: Theory and parameter estimation.",
        "On modeling.",
        "New developments in the neuroradiological diagnosis of craniocerebral trauma.",
        "Scikit-learn: Machine learning in Python.",
        "Monte-carlo simulation of particle diffusion in various geometries and application to chemistry and biology.",
        "The microstructural features of the diffusion-simulated connectivity (disco) dataset.",
        "Robust monte-carlo simulations in diffusion-mri: Effect of the substrate complexity and parameter choice on the reproducibility of results.",
        "V oxel size matters: big voxels are required to generate realistic extra-axonal dmri signals from monte carlo simulations.",
        "Can increased spatial resolution solve the crossing fiber problem for diffusion mri?",
        "Limits to anatomical accuracy of diffusion tractography using modern approaches.",
        "Structure and function of axo-axonic inhibition.",
        "Parametric probability distribution functions for axon diameters of corpus callosum.",
        "Octree-based decimation of marching cubes surfaces.",
        "Restricted Diffusion in Cylindrical Geometry.",
        "Laplacian surface editing.",
        "The non-random brain: efficiency, economy, and complex dynamics.",
        "In vivo histology of the myelin g-ratio with magnetic resonance imaging.",
        "An overview of the c++ programming language.",
        "PyVista: 3D plotting and mesh analysis through a streamlined interface for the Visualization Toolkit (VTK).",
        "Diffusion in brain extracellular space.",
        "Subacute cytokine changes after a traumatic brain injury predict chronic brain microstructural alterations on advanced diffusion imaging in the male rat.",
        "Super-resolution imaging of the extracellular space in living brain tissue.",
        "Robust determination of the fibre orientation distribution in diffusion mri: Non-negativity constrained super-resolved spherical deconvolution.",
        "Resolving crossing fibres using constrained spherical deconvolution: Validation using diffusion-weighted imaging phantom data.",
        "An evolutionary framework for microstructure-sensitive generalized diffusion gradient waveforms.",
        "Q-ball imaging.",
        "Fixed-radius near neighbors search.",
        "scikit-image: image processing in Python.",
        "Water diffusion and acute stroke.",
        "Python 3 Reference Manual",
        "Essential mathematics for games and interactive applications",
        "The variability of mr axon radii estimates in the human white matter.",
        "Noddi: practical in vivo neurite orientation dispersion and density imaging of the human brain.",
        "Axon mimicking hydrophilic hollow polycaprolactone microfibres for diffusion magnetic resonance imaging."
      ],
      "meta_data": {
        "arxiv_id": "2305.16109v1",
        "authors": [
          "Juan Luis Villarreal-Haro",
          "Remy Gardier",
          "Erick J Canales-Rodriguez",
          "Elda Fischi Gomez",
          "Gabriel Girard",
          "Jean-Philippe Thiran",
          "Jonathan Rafael-Patino"
        ],
        "published_date": "2023-05-25T14:40:44Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "CACTUS introduces an open-source computational workflow to generate highly realistic, customizable synthetic white-matter microstructure substrates for Monte-Carlo diffusion MRI simulations. It addresses two key limitations of prior phantom generators (e.g., MEDUSA/ConFiG): limited packing density (~75%) and small voxel sizes (â‰¤(100Âµm)^3). CACTUS achieves ultradense packing up to ~95% fibre volume fraction and supports much larger substrates up to (500Âµm)^3 while incorporating histology-informed features such as orientation dispersion, bundle crossings, along-axon radius variability, non-circular cross-sections, tortuous surfaces, and explicit inner/outer axon surfaces enabling myelin volume and g-ratio targeting. Core technical contributions include (i) a bundle initialization procedure controlling mean angular dispersion and gamma-distributed radii, (ii) a novel capsule-based fibre parameterization and global cost function for overlap-aware joint optimization with analytical gradients, and (iii) a Fibre Radial Growth (FRG) algorithm that maximizes packing density and produces realistic irregular fibre morphologies and myelin compartments.",
        "methodology": "Pipeline with three stages: (1) Substrate initialization: extend a 2D circle packing strategy to initialize 3D fibre bundles in a cuboid (LÃ—LÃ—H), sampling radii from a Gamma(Î±,Î²) distribution and matching a target mean angular dispersion; crossing substrates are built by initializing two bundles and applying rotation/translation to set inter-bundle angle. Fibre skeletons are defined by control points with associated radii. (2) Joint fibre optimization: formulate a global objective with prioritized penalties for capsule-capsule overlap, curvature, length increase, radius changes, and a compactness term; represent fibres as chains of capsules [p0,p1,r0,r1] to reduce parameters and improve collision handling. Overlap cost is computed via closest-point distance between two capsules (closed-form solution) and penalizes normalized inter-capsule penetration; optimization solved with gradient descent using Adagrad and analytical derivatives; computational acceleration via fixed-radius cell (spatial hashing) to reduce comparisons from O(n^2) to ~O(n), implemented in C++ with OpenMP. Bundle-crossing outputs are trimmed to retain the true crossing region. (3) Fibre Radial Growth (FRG): discretize 3D space into a grid, place random seeds inside capsules, iteratively grow each fibre mask with BFS while preventing boundary interference; random growth induces surface tortuosity and non-cylindrical shapes. Extract outer/inner meshes via marching cubes, Laplacian smoothing, and mesh decimation; inner surface is produced by erosion to define myelin volume and g-ratio. FRG implemented in Python with multiprocessing and Numba; uses ball-tree preprocessing to manage fibre interactions; designed for distributed per-fibre execution.",
        "experimental_setup": "Synthetic evaluation (no in-vivo dataset) across controlled substrate designs and metrics. Experiments: (1) Maximum fibre volume fraction (FVF) for single bundles at mean dispersion Î·âˆˆ{0Â°,5Â°,10Â°,15Â°,20Â°,25Â°} in (500Âµm)^3 voxels, and for two-bundle crossings with inter-bundle angle Î¸âˆˆ{30Â°,45Â°,60Â°,75Â°,90Â°} with Î·=5Â° per bundle; report number of fibres (~30k) and FVF (~90.8â€“94.7% single-bundle; ~92.2â€“93.9% crossings). (2) Targeted microstructure replication using histology-derived targets from Stikov et al. (2015): generate (300Âµm)^3 single-bundle substrates matching axon volume fraction (AVF), myelin volume fraction (MVF), fibre volume fraction (FVF=AVF+MVF), and aggregated g-ratio g = sqrt(1 - MVF/FVF); achieved values within <2% of targets, with qualitative comparison to provided electron microscopy images. (3) Effect of substrate size on radii distribution: generate substrates from (50Âµm)^3, (100Âµm)^3, (200Âµm)^3, to (500Âµm)^3 (with 341, 1316, 4859, 33478 fibres) and compare empirical vs target gamma-distributed radii; agreement improves notably for â‰¥(200Âµm)^3. Validation/measurement: compute radii by slicing each fibre orthogonal to its skeleton every 1Âµm, measuring cross-sectional polygon area and converting to equivalent radius; examine along-axon diameter variability on representative fibres. Computational reporting: optimization ~4 hours for (500Âµm)^3 on 64-core node; FRG ~1 minute per fibre per core (reported memory figures likely intended as GB rather than MB).",
        "limitations": "Relies on simplifying fibre-model assumptions to keep computation tractable (e.g., fibres represented by capsule chains; growth on discrete grids). Focuses on healthy white-matter-like axonal substrates; pathological microstructure is not yet modeled. Current generator includes only fibre/myelin compartments and does not yet incorporate other cellular/vascular components (astrocytes, oligodendrocytes, microglia, capillaries), though large voxel capability could allow it. Mesh-based outputs may impact Monte-Carlo DW-MRI signal accuracy; the paper notes the need for systematic analysis of how mesh quality (e.g., triangle count/decimation/smoothing) affects simulated signals and computational cost. High computational requirements remain substantial for largest substrates (hours for optimization; per-fibre FRG time), and radius-distribution fidelity degrades in small substrates (<200Âµm)^3 due to sampling bias.",
        "future_research_directions": "Extend CACTUS to simulate pathological white matter (e.g., demyelination, axonal loss, swelling/beading, gliosis) with controllable lesion patterns and region-specific statistics. Integrate additional tissue constituents into large substrates (glia, somas, vasculature, extracellular matrix) to support next-generation microstructure models and multi-compartment simulations. Perform rigorous studies linking mesh discretization/quality (smoothing, decimation, triangle counts, grid resolution) to Monte-Carlo diffusion signal bias/variance and runtime, producing best-practice guidelines. Couple CACTUS substrates with acquisition/protocol optimization and model-fitting benchmarks, enabling standardized validation suites for microstructure imaging methods. Expand beyond brain DW-MRI to other fibrous tissues (e.g., skeletal muscle) and to other modalities such as polarized light imaging (PLI) simulation with realistic myelin birefringence geometries.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Improving Coherence and Consistency in Neural Sequence Models with Dual-System, Neuro-Symbolic Reasoning",
      "full_text": "Low-Range-Sidelobe Waveform Design for MIMO-OFDM ISAC Systems Peishi Liâ€ , Zichao Xiao â€ , Ming Li â€ , Rang Liu â€¡, and Qian Liu Â§ â€  School of Information and Communication Engineering Dalian University of Technology, Dalian, Liaoning 116024, China E-mail: {lipeishi, xiaozichao}@mail.dlut.edu.cn, mli@dlut.edu.cn â€¡ Center for Pervasive Communications and Computing University of California, Irvine, CA 92697, USA E-mail: rangl2@uci.edu Â§ School of Computer Science and Technology Dalian University of Technology, Dalian, Liaoning 116024, China E-mail: qianliu@dlut.edu.cn Abstractâ€”Integrated sensing and communication (ISAC) is a promising technology in future wireless systems owing to its efficient hardware and spectrum utilization. In this paper, we con- sider a multi-input multi-output (MIMO) orthogonal frequency division multiplexing (OFDM) ISAC system and propose a novel waveform design to provide better radar ranging performance by taking range sidelobe suppression into consideration. In specific, we aim to design the MIMO-OFDM dual-function waveform to minimize its integrated sidelobe level (ISL) while satisfying the quality of service (QoS) requirements of multi- user communications and the transmit power constraint. To achieve a lower ISL, the symbol-level precoding (SLP) technique is employed to fully exploit the degrees of freedom (DoFs) of the waveform design in both temporal and spatial domains. An efficient algorithm utilizing majorization-minimization (MM) framework is developed to solve the non-convex waveform design problem. Simulation results reveal radar ranging performance improvement and demonstrate the benefits of the proposed SLP- based low-range-sidelobe waveform design in ISAC systems. Index Termsâ€”Integrated sensing and communication (ISAC), waveform design, range sidelobe, symbol-level precoding (SLP). I. I NTRODUCTION Integrated sensing and communication (ISAC) has gained widespread attention from both industry and academia since it has been deemed as a key technique in future wireless networks and autonomous vehicular systems [1]. The crucial challenge in ISAC systems lies in the dual-functional wave- form design that should balance the performance requirements of both communication and sensing [2]. Multi-input multi- output (MIMO) orthogonal frequency division multiplexing (OFDM) has been widely adopted in various wireless systems, owing to its superior ability of providing high-performance wireless communications. Consequently, the utilization of MIMO-OFDM waveform to achieve both communication and sensing functions is one of the most proposing approaches to deploy ISAC in practical networks. Numerous dual-function MIMO-OFDM waveform designs have been proposed by jointly considering different communication and sensing per- formance metrics to achieve a satisfactory performance trade- off. Typical sensing performance metrics include the signal- to-interference-plus-noise ratio (SINR) at the radar receiver [3], the mean squared error (MSE) of the beampattern [4], the CramÂ´er-Rao bound (CRB) [5], etc. The above-mentioned waveform designs mainly focus on the spatial second-order statistics of the transmitted wave- forms, which attempt to improve radar sensing performance while enabling beamforming gains for communications. Iron- ically, the randomness of communication information causes the ISAC dual-function waveform to inherently exhibit a high range sidelobe level, which may severely degrade the radar ranging performance. However, existing dual-function waveform designs ignore the range sidelobe, which is a crucial factor for radar ranging performance. To mitigate range sidelobes, many deterministic sequences that exhibit good aperiodic auto/cross-correlation properties are introduced for radar waveform designs [6]. Therefore, a low-range-sidelobe waveform design is becoming a crucial task for ISAC systems. Fortunately, the recently-emerged symbol-level precoding (SLP) technique, which fully exploits the degrees of free- dom (DoFs) of both spatial and temporal domains, can be utilized for low-range-sidelobe waveform designs in MIMO- OFDM ISAC systems. Particularly, by elaborately designing the waveform in each symbol time-slot, the SLP technique can efficiently suppress the range sidelobes and improve the radar ranging performance. On the other hand, from the perspective of multi-user communications, the SLP technique can utilize the symbol information to convert the harmful multi-user interference (MUI) into constructive interference (CI), thus achieving better communication quality of service (QoS). Therefore, SLP is a promising solution to improve the performance of dual-function waveforms in both communica- tion and sensing functions by fully exploiting the DoFs in both spatial and temporal domains. To the best of our knowledge, SLP-based low-range-sidelobe waveform designs for ISAC systems have not been investigated in the existing literature. In this paper, we investigate the SLP-based low-range- arXiv:2305.18847v2  [eess.SP]  23 Oct 2023BS Communication  Channel TargetTargetTarget Radar  Channel Radar  Channel ... User 1User 1 User 2User 2 User KUser K ... User 1 User 2 User K Fig. 1: An MIMO-OFDM ISAC system. sidelobe waveform design for an MIMO-OFDM ISAC system, where a multi-antenna base station (BS) performs both down- link multi-user communication and radar sensing functions. In particular, we optimize the SLP-based MIMO-OFDM wave- form to minimize the integrated sidelobe level (ISL) of the transmit waveform to suppress its range sidelobe while satis- fying the multi-user communication QoS requirements and the transmit power constraint. We develop an efficient algorithm utilizing the majorization-minimization (MM) framework to solve the resulting non-convex waveform design problem, which exploits the unique signal structures, significantly reduc- ing the computational complexity. Finally, simulation results are provided to reveal radar ranging performance improvement and validate the benefits of the proposed SLP-based low-range- sidelobe waveform design in ISAC systems. II. S YSTEM MODEL AND PROBLEM FORMULATION A. Transmitted Signal Model The considered ISAC system is illustrated in Fig. 1. We assume that the BS is equipped with Nt transmit antennas in a uniform linear array (ULA) and serves K single-antenna communication users while sensing one target with range R in angle Î¸ using OFDM signals with N subcarriers. In specific, the transmitted symbol vector on the n-th subcarrier in the l-th time-slot is denoted as sn[l] â‰œ [sn,1[l], sn,2[l], . . . , sn,K[l]]T âˆˆ CK. The corresponding SLP transmit waveform xn[l] âˆˆ CNt is designed to realize functions of delivering information sn[l] to K users and sensing the target. With âˆ†f denoting the subcarrier spacing, and fc the carrier frequency, the transmitted signal is expressed as x(t) = 1âˆš N Nâˆ’1X n=0 xn[l]ej2Ï€(fc+nâˆ†f)t, (1) where t âˆˆ [(l âˆ’ 1)(Ts + Tcp) + Tcp, l(Ts + Tcp)], âˆ€l, Ts is the OFDM symbol duration, and Tcp is the cyclic prefix (CP) duration. B. Multi-user Communication Model To simplify the description of SLP, we assume that the modulated symbol sn,k[l] is obtained by â„¦-phase-shift-keying Real Imag , []nks lïƒ O A B D ,( [ ])H n k nClhx Constructive Region ï¦ ,,n k n kï³ ï‡ Fig. 2: The idea of CI for a QPSK symbol. (PSK) ( â„¦ = 2 , 4, . . .). The corresponding SLP transmit waveform xn[l] is designed to transfer information sn,k[l] to the communication users. The channel impulse response from the BS to the k-th user can be modeled by a tapped delay line (TDL) with U taps {Â¯h1,k, . . . ,Â¯hu,k} [7], where Â¯hu,k âˆˆ CNt is assumed to be perfectly known at the BS. Then, the received signal on the n-th subcarrier at the k-th user is given by yn,k[l] = hH n,kxn[l] + zn,k, (2) where hn,k âˆˆ CNt denotes the frequency-domain channel, and zn,k âˆ¼ CN(0, Ïƒ2) is the additive white Gaussian noise (AWGN). SLP converts the harmful MUI into CI by elaborately de- signing the transmit waveform xn[l] based on the transmitted symbol sn,k[l] and the channel state information hn,k. To illustrate the concept of CI, we take quadrature-PSK (QPSK) for example, and we assume sn,k[l] = ejÏ€/4 as the transmit symbol without loss of generality. The idea of CI is shown in Fig. 2, where the received noise-free signal is hH n,kxn[l], Ï• = Ï€/â„¦, and the decision boundaries for the symbol ejÏ€/4 are the positive halves of x and y axes. The transmit symbol sn,k[l] can be correctly detected if the received signal yn,k[l] lies in the first quadrant. In multi-user systems, the SLP approach designs xn[l] to ensure that point C lies in the constructive region, where the MUI is converted into CI that pushes the received signals away from the decision boundaries for improving multi-user communication performance. To derive the mathematical formulation for multi-user com- munication QoS requirements, point C is projected onto the direction of âˆ’ â†’OA at point B. We define point D as the intersection of the extension of âˆ’ âˆ’ â†’BC with the boundaries of the constructive region. Then, point C in the constructive region should satisfy |âˆ’ âˆ’ â†’BD| âˆ’ |âˆ’ âˆ’ â†’BC| â‰¥0, which can be written as [8] R \b hH n,kxn[l]eâˆ’jâˆ sn,k[l] âˆ’ Ïƒ p Î“n,k \t tan Ï• âˆ’ \f\f\fI \b hH n,kxn[l]eâˆ’jâˆ sn,k[l]\t\f\f\f â‰¥ 0, âˆ€n, k, (3) where Î“n,k is the signal-to-noise ratio (SNR) threshold for user k on the n-th subcarrier. Due to space limitations, the detailed derivation is omitted and readers can refer to [8]. Weutilize some fundamental linear algebra laws to reformulate (3) into an equivalent concise form: R \behH n,ixn[l] \t â‰¥ Î³n,i, i = 1, 2, . . . ,2K, (4) where ehH n,2k â‰œ hH n,keâˆ’jâˆ sn,k \u0000 sin Ï• + eâˆ’j Ï€ 2 cos Ï• \u0001 , âˆ€n, k, ehH n,2kâˆ’1 â‰œ hH n,keâˆ’jâˆ sn,k \u0000 sin Ï• âˆ’ eâˆ’j Ï€ 2 cos Ï• \u0001 , âˆ€n, k, Î³n,2k = Î³n,2kâˆ’1 â‰œ Ïƒ p Î“n,k sin Ï•, âˆ€n, k. (5) In the remainder of this paper, we will employ (4) as the multi-user communication performance constraint. C. Radar Sensing Model For the stationary target, the radar received echo signal yn[l] can be quantified as [9] yn[l] = Î²eâˆ’j2Ï€fcÏ„ aH(Î¸)xn[l]eâˆ’j2Ï€nâˆ†fÏ„ + zr, (6) where zr is the AWGN at the radar receiver, Î² â‰œ q ÏƒRCSÎ»2 (4Ï€)3R4 is the attenuation factor, ÏƒRCS is the targetâ€™s radar cross section (RCS), Ï„ â‰œ 2R/c0 is the round-trip time delay, c0 is the speed of light, and a(Î¸) â‰œ [1, ej 2Ï€ Î» d sin(Î¸), . . . , ej 2Ï€ Î» (Ntâˆ’1)d sin(Î¸)]T âˆˆ CNt is the transmit steering vector, with d denoting the antenna spacing and Î» the wavelength. Generally, range estimation is performed by a correlator bank. Therefore, the radar ranging performance is featured by the auto-correlation of the transmit waveform: r(Ï„) = Z +âˆ âˆ’âˆ x(t)xâˆ—(t âˆ’ Ï„)dt, (7) where x(t) = aH(Î¸)x(t) is the emitted signal through the Nt antennas. With the CP in OFDM signals, we can approximate the samples of the auto-correlation using the circular auto- correlation [10], which can be written as r[m] = NX p=0 x[p]xâˆ—[mod(p âˆ’ m, N)], (8) where m is range bin index, x[p] = x(pTc), and Tc = Ts/N is the sampling period. It can be seen that the auto-correlation function r[m] is random due to the randomness of dual-function waveform. The random communication waveforms may lead to high range sidelobes, rendering poor ranging performance. In this paper, we employ the SLP technique to suppress the range sidelobes by elaborately designing the transmit waveform. It is widely acknowledged that the auto-correlation with a significant peak at the mainlobe (m = 0) and low sidelobes (m Ì¸= 0) offer superior radar ranging performance. However, due to the stochastic nature of the dual-function waveform, the auto-correlation r[m] is a random function. Consequently, the resulting range profile may exhibit a high range sidelobe level, thereby compromising the radar ranging performance. To overcome this drawback, we employ the SLP technique to suppress the range sidelobes by elaborately designing the transmit waveform. The commonly used metric to quantify the sidelobe level is the ISL: Î¾r = Nâˆ’1X m=âˆ’N+1 mÌ¸=0 |r[m]|2 = 2 Nâˆ’1X m=1 |r[m]|2. (9) However, the explicit relationship between ISL and xn[l] cannot be found from (9). Using the Parseval equality, a more intuitive analytic expression of ISL can be obtained by [11] Î¾r = 2 \u0010 1 N Nâˆ’1X n=0 \f\faHxn \f\f4 âˆ’ \u0000 1 N Nâˆ’1X n=0 \f\faHxn \f\f2 \u00012\u0011 , (10) where the time-slot index [l] and angle Î¸ is omitted for conciseness. Since ISL indicates the self-interference from the sidelobes, the smaller the ISL, the better the radar ranging performance. D. Problem Formulation To facilitate the algorithm development, we first convert the ISL (10) into an equivalent concise form: Î¾r = 2 N exHAHAex âˆ’ 2 N2 (xH eAx)2, (11) where we define x â‰œ \u0002 xT 0 , xT 1 , Â·Â·Â· , xT Nâˆ’1 \u0003T , ex â‰œ vec(xxH), an â‰œ en+1 âŠ— a, An â‰œ anaH n , ËœA â‰œ Nâˆ’1X n=0 An, A â‰œ [vec(A0), vec(A1), Â·Â·Â· , vec(ANâˆ’1)]H, (12) and en âˆˆ RN indicates the n-th column of an N Ã—N identity matrix. We aim to design the SLP transmit waveform x to minimize the ISL while satisfying the multi-user communi- cation QoS requirements and the transmit power constraint. Therefore, the optimization problem for SLP-based low-range- sidelobe waveform design can be formulated as min xn,âˆ€n Î¾r = 2 N exHAHAex âˆ’ 2 N2 (xH eAx)2 (13a) s.t. R \behH n,ixn \t â‰¥ Î³n,i, âˆ€n, i, (13b) Nâˆ’1X n=0 âˆ¥xnâˆ¥2 â‰¤ P0, (13c) where P0 is the transmit power budget. Due to the quartic objective function (13a), the optimization problem (13) cannot be easily solved via existing optimization algorithms. There- fore, we will develop an efficient algorithm utilizing the MM framework to solve the waveform design problem. III. P ROPOSED SLP- BASED LOW-RANGE -SIDELOBE WAVEFORM DESIGN ALGORITHM To solve the resulting non-convex waveform design problem (13) efficiently, we utilize MM framework [12] to transform the original problem into a more tractable optimization prob- lem. In specific, we seek an upper-bound surrogate functionexHAHAex (a) â‰¤ Î»aexH ex + 2R \bexH(AHA âˆ’ Î»aI)ext \t + exH t (Î»aI âˆ’ AHA)ext (14a) (b) â‰¤ 2R \b xHBx \t + Î»aP2 0 + exH t (Î»aI âˆ’ AHA)ext (14b) (c) â‰¤ 2R \b 2Î»bxHx + xH t Bx + xHBxt âˆ’ 2Î»bxHxt + xH t (Î»bI âˆ’ B)xt \t + Î»aP2 0 + exH t (Î»aI âˆ’ AHA)ext (14c) (d) â‰¤ 2R \b xH(B + BH âˆ’ 2Î»bI)xt \t + Î»aP2 0 + exH t (Î»aI âˆ’ AHA)ext + 2Î»bP0 + 2R \b xH t (Î»bI âˆ’ B)xt \t , (14d) (d) = 2R \b xH(B + BH âˆ’ 2Î»bI)xt \t + c1, (14e) that locally approximates the objective function (13a) and min- imize the surrogate function in each iteration. The following describes the procedure of deriving the surrogate functions for exHAHAex and (xH eAx)2, respectively. A. Majorizing exHAHAex The upper-bound surrogate function of f(x) can be derived according to the second-order Taylor expansion at xt as [12]: f(x)â‰¤f(xt)+âˆ‡f(xt)H(xâˆ’xt)+ 1 2(xâˆ’xt)HM(xâˆ’xt), (15) where M âª° âˆ‡2f(x). Inspired by (15), we can transform exHAHAex into a simple linear function based on the second- order Taylor expansion in steps (a) and (c), respectively. The details of the derivation are presented in (14), where B â‰œ PNâˆ’1 n=0 AnxtxH t AT n âˆ’ Î»axtxH t , Î»a = Î»max{AHA} and Î»b = Î»max{B} are the maximum eigenvalues of matrices AHA and B, respectively, and the constant term c1 is irrele- vant to variable x. With the transmit power constraint (13c), step (b) is obtained by exH ex = vecH(xxH)vec(xxH) â‰¤ P2 0 . Similarly, step (d) is derived using xHx â‰¤ P0. However, calculating the maximum eigenvalue Î»a of the matrix AHA with the dimension N2N2 t Ã— N2N2 t is com- putationally challenging. To tackle this issue, we exploit the unique signal structures and find that Î»a is a constant that depends only on the number of transmit antennas. Recalling the definition given in (12), we can obtain that AHA =PNâˆ’1 n=0 An âŠ— An and AH n Anâ€² = 0, âˆ€n Ì¸= nâ€². Since An is a Hermitian matrix, the eigenvectors of An and Anâ€² are orthogonal. Therefore, we can directly obtain the value of Î»a as Î»a = max n=0,1,...,Nâˆ’1 Î»max{An âŠ— An} = max n=0,1,...,Nâˆ’1 Î»2 max{(en+1 âŠ— a)(eH n+1 âŠ— aH)} = max n=0,1,...,Nâˆ’1 Î»2 max{(en+1eH n+1) âŠ— (aaH)} = N2 t . (16) In this way, we can substantially reduce the computational complexity in majorizing exHAHAex. Subsequently, we need to derive the lower-bound surrogate function of (xH eAx)2. B. Minorizing (xH eAx)2 It is easy to prove that (xH eAx)2 is a convex function with respect to x by showing that the Hessian matrix âˆ‡2(xH eAx)2 Algorithm 1 Proposed SLP-based Low-Range-Sidelobe Waveform Design Algorithm Input: hn,k, Î“n,k, sn,k, âˆ€n, k, a(Î¸), L, Ï•, P0, Î´th. Output: xâ‹†. 1: Initialize t := 0, initialize x. 2: Construct an, An, âˆ€n, eA, A, x, ex by (12). 3: Calculate the objective value Î¾r by (13a). 4: Set Î´ := 1. 5: while Î´ â‰¥ Î´th do 6: Ë†Î¾r := Î¾r. 7: Update b by (19). 8: Update xt+1 by solving the problem (20). 9: Calculate the objective value Î¾r by (13a). 10: Î´ := \f\f(Î¾r âˆ’ Ë†Î¾r)/Î¾r \f\f. 11: t := t + 1 12: end while 13: Return xâ‹† = xt. is positive semi-definite. Therefore, the lower-bound surrogate function of (xH eAx)2 can be derived according to the first- order Taylor expansion as (xH eAx)2 â‰¥ (xH t eAxt)2 + 4(xH t eAxt)xH t eA(x âˆ’ xt) + 4(xH t eAxt)xT t eAT (xâˆ— âˆ’ xâˆ— t ) = 8Î±R{xH eAxt} âˆ’7Î±2, (17) where Î± â‰œ xH t eAxt for conciseness. Therefore, substituting (14) and (17) into objective function (13a) leads to the following inequality Î¾r â‰¤ R{xHb} + Îµ, (18) where Îµ is a constant term and b â‰œ 4 N (B + BH âˆ’ 2Î»bI)xt âˆ’ 16Î± N2 ËœAxt. (19) By ignoring the constant term Îµ, the optimization problem in each iteration can be formulated as min xn,âˆ€n R{xHb} (20a) s.t. R \behH n,ixn \t â‰¥ Î³n,i, âˆ€n, i, (20b) Nâˆ’1X n=0 âˆ¥xnâˆ¥2 â‰¤ P0. (20c) It can be seen that the optimization problem (20) is convex, which can be easily solved by various existing convex opti-0 5 10 15 20 25 30 35 40 Range (m) -35 -30 -25 -20 -15 -10 -5 Normalized sidelobe level (dB) Communication-only Proposed (a) Radar range profile for the K = 3 scenario. 0 5 10 15 20 25 30 35 40 Range (m) -35 -30 -25 -20 -15 -10 -5 0 Normalized sidelobe level (dB) Communication-only Proposed (b) Radar range profile for the K = 4 scenario. Fig. 3: Radar range profiles for a target at 20m, Î“ = 6 dB, L = 50. mization algorithm. With the above derivations, the proposed SLP-based low- range-sidelobe waveform design algorithm is summarized in Algorithm 1, where Î´th represents the convergence threshold. In summary, the SLP transmit waveform x is iteratively optimized by solving the problem (20) until convergence. IV. S IMULATION RESULTS In this section, we provide numerical results to evaluate the performance of the proposed SLP-based low-range-sidelobe waveform design for ISAC systems. Without loss of generality, we assume that the BS is equipped with Nt = 8 antennas with antenna spacing d = Î»/2. The number of subcarriers is N = 64, the CP length is NCP = N/4. The communication noise power is Ïƒ2 = 10dBm, and the QoS requirements are identical for each subcarrier and communication user, i.e., Î“ = Î“n,k. In addition, the transmit power is P0 = 0.5W and the convergence threshold is set as Î´th = 10âˆ’5. To validate the performance of the proposed SLP-based low- range-sidelobe waveform design in terms of range sidelobe suppression, we employ the proximate matched filter with (a) Communication-only waveform. (b) Proposed waveform. Fig. 4: The range-Doppler map for two stationary targets at ranges 15m and 20m, respectively, Î“ = 6dB, L = 256. circular correlation (PMF-CC) [10] commonly used in OFDM radar to achieve the range profile. The radar range profiles of different waveforms are shown in Fig. 3. The communication- only waveform is obtained by minimizing the transmitted power under the communication QoS requirements. It can be seen that the proposed waveform achieves a 10dB range sidelobe reduction for the K = 3 scenario and a 7dB range sidelobe reduction for the K = 4 scenario compared with the communication-only waveform, which improves the radar ranging performance while maintaining satisfactory commu- nication QoS. Next, we demonstrate the radar ranging performance of the proposed SLP-based low-range-sidelobe waveform design in a multi-target scenario. The range-Doppler map is also achieved by PMF-CC in [10]. In Fig. 4, a strong target with ÏƒRCS = 20 dBsm (e.g. car) and a weak target with ÏƒRCS = 1dBsm (e.g. pedestrian) are simulated at nearby ranges. For the communication-only waveform, the weak target at 15m is likely to be submerged in the sidelobes of the strong target at 20m, which makes it difficult to be detected. By contrast, for the proposed waveform, the mainlobe level of the weak target0 1 2 3 4 5 6 7 8 9 10 -25 -20 -15 -10 -5 0 Normalized ISL (dB) Communication-only, K=3 Communication-only, K=4 Proposed, K=3 Proposed, K=4 Fig. 5: The ISL of different waveforms versus the communi- cation QoS requirements Î“. is much higher than the sidelobe level so that it can be detected easily. Therefore, the proposed SLP-based low-range-sidelobe waveform design can achieve better radar ranging performance by suppressing the range sidelobes. Furthermore, Fig. 5 evaluates the ranging performance of different waveforms in terms of the ISL. Compared with the communication-only waveform, the proposed SLP-based low- range-sidelobe waveform design can significantly reduce the ISL of transmit waveform by exploiting more DoFs in the temporal domain, improving the radar ranging performance. Besides, the ISL of the proposed waveform increases with larger Î“ and K, which demonstrates the performance trade- off between multi-user communication and radar sensing. Finally, in order to further investigate the impact of range sidelobes on radar ranging performance, we analyze the range estimation root mean square error (RMSE) of the weak target for the case that we attempt to estimate the range of two distinct targets with different RCS. The strong target is located at 20m with ÏƒRCS = 20 dBsm, while the weak target is randomly distributed between 20m to 25m with ÏƒRCS = 1 dBsm. From Fig. 6, as the communication QoS requirements Î“ and the number of communication users K increase, the RMSE of range estimation gradually increases, which further illustrates the trade-off between radar rang- ing performance and communication QoS. Additionally, our proposed waveform always provides better range estimation performance than the communication-only waveform by sup- pressing range sidelobes. This demonstrates the superiority of the proposed SLP-based low-range-sidelobe waveform design for range estimation and the effectiveness of utilizing ISL as the sensing performance metric. V. C ONCLUSIONS In this paper, we proposed an SLP-based low-range-sidelobe waveform design for an MIMO-OFDM ISAC system. The ISL of the transmit waveform was minimized while satisfying the multi-user communication QoS requirements and the transmit 0 1 2 3 4 5 6 7 8 9 10 0 1 2 3 4 5 6 7RMSE (m) Communication-only, K=3 Communication-only, K=4 Proposed, K=3 Proposed, K=4 Fig. 6: Range estimation RMSE of the weak target versus the communication QoS requirements Î“. power constraint. We developed an efficient algorithm utiliz- ing MM framework to solve the waveform design problem. Simulation results were provided to reveal radar ranging performance improvement and demonstrated the benefits of the proposed SLP-based low-range-sidelobe waveform design in ISAC systems. REFERENCES [1] Y . Cui, F. Liu, X. Jing, and J. Mu, â€œIntegrating sensing and communi- cations for ubiquitous IOT: Applications, trends, and challenges,â€ IEEE Network, vol. 35, no. 5, pp. 158-167, Sep. 2021. [2] R. Liu, M. Li, Q. Liu, and A. L. Swindlehurst, â€œDual-functional radar- communication waveform design: A symbol-level precoding approach,â€ IEEE J. Sel. Topics Signal Process., vol. 15, no. 6, pp. 1316-1331, Nov. 2021. [3] L. Chen, Z. Wang, Y . Du, Y . Chen, and F. R. Yu, â€œGeneralized transceiver beamforming for DFRC with MIMO radar and MU-MIMO communication,â€ IEEE J. Sel. Areas Commun., vol. 40, no. 6, pp. 1795- 1808, Jun. 2022. [4] B. Tang, H. Wang, L. Qin, and L. Li, â€œWaveform design for dualfunction MIMO radar-communication systems,â€ in Proc. IEEE Sensor Array Multichannel Signal Process. Workshop (SAM), Hangzhou, China, Jun. 2020. [5] P. Kumari, S. A. V orobyov, and R. W. Heath, â€œAdaptive virtual waveform design for millimeter-wave joint communication-radar,â€ IEEE Trans. Signal Process., vol. 68, pp. 715-730, Nov. 2019. [6] J. Song, P. Babu, and D. P. Palomar, â€œSequence design to minimize the weighted integrated and peak sidelobe levels,â€ IEEE Trans. Signal Process., vol. 64, no. 8, pp. 2051-2064, Apr. 2016. [7] H. Li, W. Cai, Y . Liu, M. Li, Q. Liu, and Q. Wu, â€œIntelligent reflect- ing surface enhanced wideband MIMO-OFDM communications: From practical model to reflection optimization,â€ IEEE Trans. Commun., vol. 69, no. 7, pp. 4807-4820, Jul. 2021. [8] R. Liu, M. Li, Q. Liu, and A. L. Swindlehurst, â€œJoint waveform and filter designs for STAP-SLP-based MIMO-DFRC systems,â€ IEEE J. Sel. Areas Commun., vol. 40, no. 6, pp. 1918-1931, Jun. 2022. [9] C. Sturm and W. Wiesbeck, â€œWaveform design and signal processing aspects for fusion of wireless communications and radar sensing,â€ IEEE Proc., vol. 99, no. 7, pp. 1236-1259, Jul. 2011. [10] S. Mercier, S. Bidon, D. Roque, and C. Enderli, â€œComparison of correlation-based OFDM radar receivers,â€ IEEE Trans. Aerosp. Electron. Syst., vol. 56, no. 6, pp. 4796-4813, Dec. 2020. [11] H. Li, â€œFrequency multiplexing and waveform synthesis in joint com- munications and sensing,â€ in Proc. IEEE Wireless Commun. Netw. Conf. (WCNC), Austin, TX, Apr. 2022. [12] Y . Sun, P. Babu, and D. P. Palomar, â€œMajorization-minimization algo- rithms in signal processing, communications, and machine learning,â€ IEEE Trans. Signal Process., vol. 65, no. 3, pp. 794-816, Feb. 2017.",
      "references": [
        "Integrating sensing and communi- cations for ubiquitous IOT: Applications, trends, and challenges",
        "Dual-functional radar-communication waveform design: A symbol-level precoding approach",
        "Generalized transceiver beamforming for DFRC with MIMO radar and MU-MIMO communication",
        "Waveform design for dualfunction MIMO radar-communication systems",
        "Adaptive virtual waveform design for millimeter-wave joint communication-radar",
        "Sequence design to minimize the weighted integrated and peak sidelobe levels",
        "Intelligent reflect- ing surface enhanced wideband MIMO-OFDM communications: From practical model to reflection optimization",
        "Joint waveform and filter designs for STAP-SLP-based MIMO-DFRC systems",
        "Waveform design and signal processing aspects for fusion of wireless communications and radar sensing",
        "Comparison of correlation-based OFDM radar receivers",
        "Frequency multiplexing and waveform synthesis in joint com- munications and sensing",
        "Majorization-minimization algo- rithms in signal processing, communications, and machine learning"
      ],
      "meta_data": {
        "arxiv_id": "2305.18847v2",
        "authors": [
          "Peishi Li",
          "Zichao Xiao",
          "Ming Li",
          "Rang Liu",
          "Qian Liu"
        ],
        "published_date": "2023-05-30T08:38:11Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes a symbol-level-precoding (SLP) based dual-function MIMO-OFDM waveform design for ISAC that explicitly suppresses radar range sidelobes. Formulates waveform optimization to minimize the integrated sidelobe level (ISL) of the emitted signal while satisfying multi-user communication QoS (constructive-interference SNR constraints per subcarrier/user) and a total transmit power budget. Develops an efficient majorizationâ€“minimization (MM) algorithm with structure-exploiting eigenvalue simplifications, and demonstrates improved radar ranging (lower sidelobes, better weak-target detectability, lower range RMSE) versus a communication-only baseline under the same QoS/power constraints.",
        "methodology": "System model: BS with Nt antennas transmits N-subcarrier OFDM over L time slots to K single-antenna users while sensing a target at angle Î¸; per-subcarrier/time transmit vector x_n[l] is optimized (SLP). Communication constraints: PSK signaling; constructive interference (CI) reformulation yields linear real-part inequalities Re{e_h_{n,i}^H x_n[l]} â‰¥ Î³_{n,i} for i=1..2K. Radar metric: range sidelobes quantified by ISL, expressed via Parseval as a quartic function of {a^H x_n}. Optimization: nonconvex quartic objective with linear QoS constraints and convex power constraint. Solution: MM frameworkâ€”majorize the quartic term vec(xx^H)^H A^H A vec(xx^H) using second-order Taylor upper bounds and power-bound relaxations; minorize the convex term (x^H A x)^2 via first-order Taylor lower bound. Leads to an iterative sequence of convex subproblems with linear objective Re{x^H b} and original constraints. Complexity reduction: shows Î»_max(A^H A)=N_t^2 due to Kronecker/orthogonality structure, avoiding large eigen-decompositions. Convergence via relative objective change threshold Î´_th.",
        "experimental_setup": "Evaluation is simulation-based (no real datasets). Parameters: Nt=8 ULA, d=Î»/2; N=64 subcarriers; CP length N_CP=N/4; transmit power P0=0.5 W; communication noise Ïƒ^2=10 dBm; identical QoS thresholds Î“ across users/subcarriers; PSK modulation (examples use QPSK); MM stopping threshold Î´_th=1e-5. Baseline: 'communication-only' waveform obtained by minimizing transmit power subject to the same QoS constraints. Radar processing/benchmarks: proximate matched filter with circular correlation (PMF-CC) to obtain range profiles and rangeâ€“Doppler maps. Experiments: (1) single-target range profile at 20 m for K=3 and K=4, Î“=6 dB, L=50; (2) two-target scenario (strong car-like target Ïƒ_RCS=20 dBsm at 20 m and weak pedestrian-like target Ïƒ_RCS=1 dBsm at 15 m), L=256, comparing rangeâ€“Doppler maps; (3) ISL vs Î“ for K=3,4; (4) range-estimation RMSE of weak target with strong target fixed at 20 m and weak target uniformly/randomly between 20â€“25 m, varying Î“ and K.",
        "limitations": "Assumes perfect CSI for all user channels and known target steering angle Î¸; focuses on stationary targets and does not model Doppler/velocity coupling in the optimization (Doppler appears only in evaluation maps). ISL objective derived using circular autocorrelation approximation relying on CP; may deviate from true aperiodic correlation and ignores practical impairments (PA nonlinearity, quantization, phase noise, synchronization errors, PAPR/CM constraints, spectral masks). Uses PSK-only CI constraints; extension to QAM/higher-order constellations is nontrivial. Optimizes for a single look direction/target angle; multi-target or wide-angle sensing beampattern control is not included. MM yields a local optimum; initialization and convergence to global optimum are not guaranteed, and per-iteration convex solves may still be computationally heavy for large N, Nt, K, or real-time operation. Power constraint is total average across subcarriers; per-antenna or per-subcarrier power constraints are not considered.",
        "future_research_directions": "Develop robust designs under CSI/angle uncertainty and hardware impairments; incorporate Doppler-aware objectives and joint rangeâ€“Doppler sidelobe suppression. Extend CI/QoS formulations to QAM and coded/modulation-adaptive links, and include per-antenna/per-subcarrier power, PAPR, constant-modulus, and spectral-mask constraints. Generalize sensing objective to multi-target scenarios, multi-angle beampattern shaping, or joint ISL + beampattern/SINR/CRB metrics. Create low-complexity/closed-form or first-order/online solvers for real-time implementation and analyze convergence/optimality bounds. Validate on measured channels and over-the-air experiments, and study integration with ISAC network scheduling/resource allocation (time/frequency/beam/user selection) and multi-cell interference.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment",
      "full_text": "RealFill: Reference-Driven Generation for Authentic Image Completion LUMING TANG, Cornell University, US NATANIEL RUIZ,Google Research, US QINGHAO CHU, Google Research, US YUANZHEN LI, Google Research, US ALEKSANDER HOÅYÅƒSKI, Google Research, US DAVID E. JACOBS,Google Research, US BHARATH HARIHARAN, Cornell University, US YAEL PRITCH, Google Research, Israel NEAL WADHWA,Google Research, US KFIR ABERMAN, Snap Research, US MICHAEL RUBINSTEIN, Google Research, US RealFill (Ours) ? Reference ImagesTarget Image ? Photoshop Generative FillStable Diffusion Inpainting Fig. 1. Given a few reference images that roughly capture the same scene, and a target image with a missing region, RealFill is able to complete the target image with image content that is faithful to the true scene. In contrast, standard prompt-based inpainting methods hallucinate plausible but inauthentic content due to their lack of knowledge of the original scene. Recent advances in generative imagery have brought forth outpainting and inpainting models that can produce high-quality, plausible image content in Authorsâ€™ addresses: Luming Tang, lt453@cornell.edu, Cornell University, US; Nataniel Ruiz, natanielruiz@google.com, Google Research, US; Qinghao Chu, qchu@google.com, Google Research, US; Yuanzhen Li, yzli@google.com, Google Research, US; Aleksander HoÅ‚yÅ„ski, holynski@google.com, Google Research, US; David E. Jacobs, dejacobs@ google.com, Google Research, US; Bharath Hariharan, bharathh@cs.cornell.edu, Cornell University, US; Yael Pritch, yaelp@google.com, Google Research, Israel; Neal Wadhwa, nealw@google.com, Google Research, US; Kfir Aberman, kfiraberman@gmail.com, Snap Research, US; Michael Rubinstein, mrub@google.com, Google Research, US. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). Â© 2024 Copyright held by the owner/author(s). ACM 0730-0301/2024/7-ART135 https://doi.org/10.1145/3658237 unknown regions. However, the content these models hallucinate is neces- sarily inauthentic, since they are unaware of the true scene. In this work, we propose RealFill, a novel generative approach for image completion that fills in missing regions of an image with the content thatshould have been there. RealFill is a generative inpainting model that is personalized using only a few reference images of a scene. These reference images do not have to be aligned with the target image, and can be taken with drastically varying viewpoints, lighting conditions, camera apertures, or image styles. Once personalized, RealFill is able to complete a target image with visually com- pelling contents that are faithful to the original scene. We evaluate RealFill on a new image completion benchmark that covers a set of diverse and challenging scenarios, and find that it outperforms existing approaches by a large margin. Project page: https://realfill.github.io. CCS Concepts: â€¢ Computing methodologies â†’Computational photog- raphy; Image processing; Computer vision. Additional Key Words and Phrases: Image Completion, Diffusion Model ACM Trans. Graph., Vol. 43, No. 4, Article 135. Publication date: July 2024. arXiv:2309.16668v2  [cs.CV]  14 May 2024135:2 â€¢ Tang et al. ACM Reference Format: Luming Tang, Nataniel Ruiz, Qinghao Chu, Yuanzhen Li, Aleksander HoÅ‚yÅ„ski, David E. Jacobs, Bharath Hariharan, Yael Pritch, Neal Wadhwa, Kfir Aberman, and Michael Rubinstein. 2024. RealFill: Reference-Driven Generation for Au- thentic Image Completion. ACM Trans. Graph. 43, 4, Article 135 (July 2024), 13 pages. https://doi.org/10.1145/3658237 1 INTRODUCTION Photographs capture ephemeral and invaluable experiences in our lives, but can sometimes fail to do these memories justice. In many cases, no single shot may have captured the perfect angle, framing, timing, or composition, and unfortunately, just as the experiences themselves cannot be revisited, these elements of the captured im- ages are also unalterable. We show one such example in Fig. 2: imagine having taken a nearly perfect photo of your daughter danc- ing on stage, but her unique and intricate crown is partially cut out of the frame. Of course, there are many other pictures from the per- formance that showcase her crown, but they all fail to capture that precise special moment: her pose mid-dance, her facial expression, and the perfect lighting. Given this collection of imperfect photos, you can certainly imagine the missing parts of this perfect shot, but actually creating a complete, shareable version of this image is much harder. In this paper, we focus on this problem, which we call Authentic Image Completion. Given a few reference images (up to five) and one target image that captures roughly the same scene (but in a different arrangement or appearance), we aim to fill missing regions of the target image with high-quality image content that is faithful to the originally captured scene. Note that for the sake of practical benefit, we focus particularly on the more challenging, unconstrained setting in which the target and reference images may have very different viewpoints, environmental conditions, camera apertures, image styles, or even moving objects. Approaches to solving variants of this problem have been pro- posed using classical geometry-based pipelines [Shan et al . 2014; Zhao et al. 2023a; Zhou et al . 2021] that rely on correspondence matching, depth estimation, and 3D transformations, followed by patch fusion and image harmonization. These methods tend to en- counter catastrophic failure when the sceneâ€™s structure cannot be accurately estimated, e.g., when the scene geometry is too complex or contains dynamic objects. On the other hand, recent generative models [Chang et al. 2023, 2022; Yu et al. 2018], and in particular diffusion models [Ho et al . 2020; Rombach et al. 2022; Song et al. 2021], have demonstrated strong performance on the tasks of image inpainting and outpainting [Ramesh et al. 2022; Stability AI 2022; Wang et al. 2023]. These methods, however, struggle to recover genuine scene structure and fine details, since they are only guided by text prompts, and canâ€™t use reference image content. To this end, we present a simple yet effective reference-driven im- age completion framework calledRealFill. For a given scene, we first create a personalized generative model by finetuning a pretrained inpainting diffusion model [Stability AI 2022] on the reference and target images. This finetuning process is designed such that the adapted model not only maintains a good image prior, but also learns the contents, lighting, and style of the scene in the input images. We then use this finetuned model to fill the missing regions in the target image through a standard diffusion sampling process. Given the stochastic nature of generative inference, we propose Correspondence-Based Seed Selection , to automatically select a small set of high-quality generations by exploiting a special property of our completion task: that there exists true correspondences between generated contents and reference images. Specifically, we filter out samples that have too few keypoint matches with references, which greatly reduces human labor to select high-quality model outputs. As shown in Figs. 1 to 4, RealFill is able to effectively inpaint and outpaint a target image with its genuine scene content. Most importantly, our method is able to handle large differences between reference and target images, e.g., viewpoint, lighting, aperture, style or dynamic deformations â€” differences which are very difficult for previous geometry-based approaches. Existing benchmarks for im- age completion [Zhou et al. 2021] mainly focus on small inpainting tasks and minimal changes between reference and target images. In order to quantitatively evaluate the aforementioned challeng- ing use-case, we collect a dataset containing 10 inpainting and 23 outpainting examples along with corresponding ground-truth, and show that RealFill outperforms baselines by a large margin across multiple image similarity metrics. In summary, our contributions are as follows: â€¢We define a new problem namedAuthentic Image Completion , i.e., given a set of reference images and a target image with missing regions, we seek to complete those missing regions with content that is faithful to the scene as captured in the references. In essence, the goal is to complete the target image with what â€œshould have been thereâ€ rather than what â€œcould have been thereâ€, like in typical generative inpainting. â€¢We introduce RealFill, a method that aims to solve this prob- lem by finetuning an inpainting diffusion model on reference and target images. This model is sampled withCorrespondence- Based Seed Selection to filter outputs with low fidelity to the reference images. RealFill is the first method that expands the expressive power of generative inpainting models by condi- tioning the process beyond text, enabling extra conditioning on reference images. â€¢We proposeRealBench, a dataset for quantitative evaluation of authentic image completion, composed of 33 scenes spanning both inpainting and outpainting tasks. 2 RELATED WORK Adapting Pretrained Diffusion Models . Diffusion models [Dhari- wal and Nichol 2021; Ho et al. 2020; Song et al. 2021] have shown strong performance in text-to-image (T2I) generation [Ramesh et al. 2022; Rombach et al. 2022; Saharia et al. 2022]. Recent works make use of this pretrained image prior by finetuning them for vari- ous tasks. Personalization methods propose to finetune the T2I model [Avrahami et al. 2023; Chen et al. 2023; Ruiz et al. 2023a,b] or text embedding [Gal et al . 2022; Voynov et al. 2023], on a few images to achieve arbitrary text-driven generation of a given object or style. Other techniques instead finetune a T2I model to add new conditioning signals, either for image editing [Brooks et al . 2023; Kawar et al. 2023; Wang et al. 2023] or more controllable genera- tion [Mou et al. 2023; Sohn et al. 2023; Zhang et al. 2023b]. The same ACM Trans. Graph., Vol. 43, No. 4, Article 135. Publication date: July 2024.RealFill: Reference-Driven Generation for Authentic Image Completion â€¢ 135:3 approach can be also applied to specialized tasks [Liu et al. 2023; Raj et al. 2023; Wu et al. 2022; Zhao et al. 2023b] such as converting a T2I model into a 3D or video generation model. Our method shows that a pretrained T2I inpainting diffusion model can be adapted to perform reference-driven image completion. Image Completion . As an enduring challenge in computer vi- sion, image completion aims to fill missing parts of an image with plausible content, i.e., inpainting and outpainting. Traditional ap- proaches [Barnes et al. 2009; Bertalmio et al. 2000; Criminisi et al. 2003; Hays and Efros 2007] rely on handcrafted heuristics while more recent deep learning-based methods [Iizuka et al. 2017; Kim et al. 2022; Liu et al. 2018; Suvorov et al. 2022] instead directly train end-to-end neural networks that take original image and mask as inputs and generate the completed image. Given the challenging nature of this problem [Zheng et al. 2019], many works [Chang et al. 2023, 2022; Lugmayr et al. 2022; Yeh et al. 2017] propose to leverage the image prior from a pretrained generative model for this task. Built upon powerful T2I diffusion models, recent solutions [Adobe Inc. 2023; Ramesh et al. 2022; Stability AI 2022] demonstrate strong text-driven image completion capabilities. However, due to their sole dependence on a text prompt (which has limited descriptive power), generated image content can often be hard to control, re- sulting in tedious prompt engineering, especially when a particular or otherwise true scene content is desired. This is one of the main issues we aim to tackle in our work. Reference-Based Image Inpainting. Existing works for reference- based inpainting [Zhao et al. 2023a; Zhou et al. 2021] or outpaint- ing [Shan et al. 2014] usually make use of carefully tuned pipelines containing many individual components like depth and pose esti- mation, image warping, and harmonization. Each of these modules usually tackles a moderately challenging problem itself and the resulting prediction error can, and often does, propagate and accu- mulate through the pipeline. This can lead to catastrophic failure es- pecially in challenging cases with complex scene geometry, changes in appearance, or scene deformation. Paint-by-Example [Yang et al. 2023] proposes a novel latent diffusion model [Rombach et al. 2022] whose generation is conditioned on both a reference and target image. However, the conditioning is based on CLIP embedding [Rad- ford et al. 2021] of a single reference image, therefore is only able to capture high-level semantics of the reference object. In contrast, our method is the first to demonstrate multiple reference image-driven inpainting and outpainting that is both visually compelling and faithful to the original scene, even in cases where there are large appearance changes between reference and target images. 3 METHOD 3.1 Reference-Based Image Completion Given a set of casually captured reference images (up to five), our goal is to complete (either outpaint or inpaint) a target image of roughly the same scene. The output image is expected to be not only plausible and photorealistic, but also faithful to the reference images, i.e., recovering content and scene detail that were present in the actual scene. In essence, we want to achieve authentic image completion, where we generate what â€œshould have been thereâ€ in- stead of what â€œcould have been thereâ€. We purposefully pose this as a broad and challenging problem with few constraints on the inputs. For example, the images could be taken from very different viewpoints with unknown camera poses. They could also have dif- ferent lighting conditions or styles, and the scene could potentially be non-static and have significantly varying layout across images. In this section, we first provide background knowledge on dif- fusion models and subject-driven generation (Sec. 3.2). Then, we formally define the problem of authentic image completion (Sec. 3.3). Finally, we present RealFill, our method to perform reference-based image completion with a pretrained diffusion image prior (Sec. 3.4). 3.2 Preliminaries Diffusion models are generative models that aim to transform a Normal distribution into an arbitrary target data distribution. During training, different magnitudes of Gaussian noise are added to a data point ğ‘¥0 to obtain noisy ğ‘¥ğ‘¡: ğ‘¥ğ‘¡ = âˆšğ›¼ğ‘¡ğ‘¥0 +(âˆš1 âˆ’ğ›¼ğ‘¡)ğœ– (1) where the noise ğœ– âˆ¼N(0,I), and {ğ›¼ğ‘¡}ğ‘‡ ğ‘¡=1 define a fixed noise sched- ule with larger ğ‘¡ corresponding to more noise. Then, a neural net- work ğœ–ğœƒ is trained to predict the noise using the following loss function: L= Eğ‘¥,ğ‘¡,ğœ– âˆ¥ğœ–ğœƒ(ğ‘¥ğ‘¡,ğ‘¡,ğ‘ )âˆ’ğœ–âˆ¥2 2 (2) where ğœ–ğœƒ is conditioned on some signal ğ‘, e.g., a language prompt for a T2I model, a masked image for an inpainting model. During inference, starting from ğ‘¥ğ‘‡ âˆ¼N( 0,I), ğœ–ğœƒ is used to iteratively re- move noise from ğ‘¥ğ‘¡ to get a less noisy ğ‘¥ğ‘¡âˆ’1, eventually leading to a sample ğ‘¥0 from the target data distribution. DreamBooth [Ruiz et al. 2023a] enables T2I diffusion models to generate images of a specific subject with semantic modifications. The core idea is to finetune the model ğœ–ğœƒ on a few subject images using the loss in Eq. 2. Instead of finetuning all the network weights, it is possible to combine DreamBooth with Low Rank Adaptations (LoRA) [Hu et al . 2022; Ryu 2023], for a more memory-efficient alternative, by injecting learnable residual modules Î”ğ‘Š to each net- work weight matrix ğ‘Š. Î”ğ‘Š is a composition of low-rank matrices, i.e., ğ‘Š +Î”ğ‘Š = ğ‘Š +ğ´ğµ where ğ‘Š âˆˆRğ‘›Ã—ğ‘›, ğ´ âˆˆRğ‘›Ã—ğ‘Ÿ, ğµ âˆˆRğ‘ŸÃ—ğ‘›, ğ‘Ÿ â‰ªğ‘›, and only the added Î”ğ‘Š is being updated during training while modelâ€™s original parameters ğ‘Š stay frozen. 3.3 Problem Setup Formally, the model is given ğ‘› (ğ‘› â‰¤5) reference images Xğ‘Ÿğ‘’ğ‘“ B {ğ¼ğ‘˜ ğ‘Ÿğ‘’ğ‘“ }ğ‘› ğ‘˜=1, a target image ğ¼ğ‘¡ğ‘”ğ‘¡ âˆˆRğ»Ã—ğ‘ŠÃ—3 and its associated binary mask ğ‘€ğ‘¡ğ‘”ğ‘¡ âˆˆ{0,1}ğ»Ã—ğ‘Š, in which 1 denotes the region to fill and 0 denotes the existing area in ğ¼ğ‘¡ğ‘”ğ‘¡. The model is expected to generate a harmonized image ğ¼ğ‘œğ‘¢ğ‘¡ âˆˆ Rğ»Ã—ğ‘ŠÃ—3 whose pixels should stay as similar as possible to ğ¼ğ‘¡ğ‘”ğ‘¡ where ğ‘€ğ‘¡ğ‘”ğ‘¡ equals 0, while staying faithful to the corresponding contents in Xğ‘Ÿğ‘’ğ‘“ where ğ‘€ğ‘¡ğ‘”ğ‘¡ equals 1. We assume there is enough overlap between Xğ‘Ÿğ‘’ğ‘“ and ğ¼ğ‘¡ğ‘”ğ‘¡ such that a human could imagine a plausible ğ¼ğ‘œğ‘¢ğ‘¡. 3.4 RealFill This task is challenging for both geometry-based [Zhao et al. 2023a; Zhou et al . 2021] and reconstruction-based approaches [Milden- hall et al. 2020] because there are barely any geometric constraints ACM Trans. Graph., Vol. 43, No. 4, Article 135. Publication date: July 2024.135:4 â€¢ Tang et al. input image â€œa photo of [V]â€ add noisetextencoder â„ ğŸ”¥ LoRA text-to-image diffusion inpainting model â„ ğŸ”¥ LoRA random maskmasked image â¨€ = predicted clean image reconstructionloss reference & target images randomly sample and augmentTraining Inference â€œa photo of [V]â€ finetuned diffusion modelâ€¦ target image & mask GaussianNoise RealFill output Fig. 2. Training and inference pipelines of RealFill. RealFillâ€™s inputs are a target image to be filled and a few reference images of the same scene. We first finetune LoRA weights of a pretrained inpainting diffusion model on the reference and target images (with random patches masked out). Then, we use the adapted model to fill the desired region of the target image, resulting in a faithful, high-quality output. For example, the girlâ€™s crown is recovered in the target image, despite the girl being in very different poses in the reference images. between Xğ‘Ÿğ‘’ğ‘“ and ğ¼ğ‘¡ğ‘”ğ‘¡, there are only a few images available as inputs, and the reference images may have different styles, lighting conditions, and subject poses from the target. One alternative is to use a controllable inpainting or outpainting model, however, these models are either prompt-based [Adobe Inc. 2023; Rombach et al. 2022] or single-image object-driven [Yang et al. 2023], which makes them hard to use for recovering complex scene-level structure and details. Therefore, we propose to first inject knowledge of the scene into a pretrained generative model by finetuning it on a set of reference images, then use the adapted model to generate ğ¼ğ‘œğ‘¢ğ‘¡ conditioned on ğ¼ğ‘¡ğ‘”ğ‘¡ and ğ‘€ğ‘¡ğ‘”ğ‘¡, such that it is aware of the sceneâ€™s contents. Training. Starting from a state-of-the-art T2I diffusion inpainting model [Rombach et al. 2022], we inject LoRA weights and finetune it on both Xğ‘Ÿğ‘’ğ‘“ and ğ¼ğ‘¡ğ‘”ğ‘¡ with randomly generated binary masks ğ‘š âˆˆ{0,1}ğ»Ã—ğ‘Š. The loss function is L= Eğ‘¥,ğ‘¡,ğœ–,ğ‘š âˆ¥ğœ–ğœƒ(ğ‘¥ğ‘¡,ğ‘¡,ğ‘,ğ‘š, (1 âˆ’ğ‘š)âŠ™ ğ‘¥)âˆ’ğœ–âˆ¥2 2 (3) where ğ‘¥ âˆˆXğ‘Ÿğ‘’ğ‘“ âˆª{ğ¼ğ‘¡ğ‘”ğ‘¡}, ğ‘ is a fixed language prompt, âŠ™denotes the element-wise product and therefore (1 âˆ’ğ‘š)âŠ™ ğ‘¥ is the masked clean image. For ğ¼ğ‘¡ğ‘”ğ‘¡, the loss is only calculated on the existing region, i.e., where ğ‘€ğ‘¡ğ‘”ğ‘¡â€™s entry equals 0. Specifically, we use the open-sourced Stable Diffusion v2 inpainting model [Stability AI 2022] and inject LoRA layers into its text encoder and U-Net for finetuning. Following [Ruiz et al. 2023a], we fix ğ‘ to be a sentence containing a rare token, i.e., â€œa photo of [V]\". For each training example, similar to [Suvorov et al . 2022], we generate multiple random rectangles and take either their union or the complement of the union to get the final random mask ğ‘š. Fig. 2 illustrates the whole pipeline. Inference. After training, we use the DDPM [Ho et al. 2020] sampler to generate an imageğ¼ğ‘”ğ‘’ğ‘›, conditioning the model onğ‘, ğ¼ğ‘¡ğ‘”ğ‘¡ and ğ‘€ğ‘¡ğ‘”ğ‘¡. However, similar to the observation in [Zhu et al. 2023], we notice that the existing region inğ¼ğ‘¡ğ‘”ğ‘¡ is distorted inğ¼ğ‘”ğ‘’ğ‘›. To resolve this, we first feather the mask ğ‘€ğ‘¡ğ‘”ğ‘¡, then use it to alpha composite ğ¼ğ‘”ğ‘’ğ‘› and ğ¼ğ‘¡ğ‘”ğ‘¡, leading to the final ğ¼ğ‘œğ‘¢ğ‘¡ with full recovery on the existing area and a smooth transition at the boundary of the generated region. Correspondence-Based Seed Selection . The diffusion inference process is stochastic, i.e., the same inputs may produce any number of generated images depending on the random seeds of the sampling process. The generation quality can vary due to this stochasticity, thus requiring humans to select high-quality samples. While there is work to identify good samples from a collection of generated outputs [Samuel et al. 2023], this remains an open problem. In our case, the reference images actually provide a grounding signal for the true scene content, thus can be used to help identify high-quality outputs. Specifically, we use the number of image feature correspondences between ğ¼ğ‘œğ‘¢ğ‘¡ and Xğ‘Ÿğ‘’ğ‘“ as a metric to roughly quantify whether the result is faithful to the reference images. During inference, we first generate a batch of outputs, i.e., {ğ¼ğ‘œğ‘¢ğ‘¡}, then extract a set of correspondences (e.g., using LoFTR [Sun et al. 2021]) between Xğ‘Ÿğ‘’ğ‘“ and the filled region of each ğ¼ğ‘œğ‘¢ğ‘¡, and finally rank the generated results {ğ¼ğ‘œğ‘¢ğ‘¡}by the number of correspondences. This allows us to automatically filter generations to a small set of high-quality results. 4 EXPERIMENTS Qualitative Results . In Figs. 3 and 4, we show that RealFill is able to convincingly outpaint and inpaint image content that is faithful to the reference images. Notably, it is able to handle dramatic differences in camera pose, lighting, defocus blur, image style, and even subject pose. This is because RealFill has both a good image prior (from the pretrained diffusion model) and knowledge of the scene (from finetuning on the input images). Evaluation Dataset. Existing benchmarks for reference-based im- age completion [Zhou et al . 2021] primarily focus on inpainting small regions, and assume at most very minor changes between the reference and target images. To better evaluate our target use cases, we create our own dataset, RealBench. It consists of 33 scenes (23 outpainting and 10 inpainting), where each scene has a set of reference images Xğ‘Ÿğ‘’ğ‘“ , a target image ğ¼ğ‘¡ğ‘”ğ‘¡ to fill, a binary mask ğ‘€ğ‘¡ğ‘”ğ‘¡ indicating the missing region, and the ground-truth result ğ¼ğ‘”ğ‘¡. The number of reference images in each scene varies from 1 to 5. ACM Trans. Graph., Vol. 43, No. 4, Article 135. Publication date: July 2024.RealFill: Reference-Driven Generation for Authentic Image Completion â€¢ 135:5 Reference ImagesRealFill Outputs Reference Images RealFill Outputs Fig. 3. Reference-based outpainting with RealFill. Given the reference images on the left, RealFill outpaints the corresponding target images on the right. The region inside the white box is provided to the network as known pixels, and the region outside the white box is generated. RealFill produces high-quality images that are faithful to the references, even when there are dramatic differences between the references and targets such as changes in viewpoint, aperture, lighting, image style, and object motion. ACM Trans. Graph., Vol. 43, No. 4, Article 135. Publication date: July 2024.135:6 â€¢ Tang et al. Target ImageRealFill Output Reference Images  Target ImageRealFill OutputReference Images Fig. 4. Reference-based inpainting with RealFill. Given the references on the left, RealFill can not only remove undesired objects in the target image and reveal the occluded contents faithfully (left column), but also insert objects into the scene despite significant viewpoint changes between reference and target images (right column). In the bottom left example, the reference and target images have different defocus blurs. RealFill not only recovers the buildings behind the mug, but also keeps the same amount of blur as in the target image. The dataset contains diverse, challenging scenarios with significant variations between the reference and target images, such as changes in viewpoint, defocus blur, lighting, style, and subject pose. Evaluation Metrics. We use multiple metrics to evaluate the qual- ity and fidelity of our model outputs. We compare the generated images with the ground-truth target image at multiple levels of im- age similarity, including PSNR, SSIM, LPIPS [Zhang et al. 2018] for low-level, DreamSim [Fu et al. 2023] for mid-level, and DINO [Caron et al. 2021], CLIP [Radford et al. 2021] for high-level. For low-level metrics, we only do calculation inside the filled-in region, i.e., where ğ‘€ğ‘¡ğ‘”ğ‘¡ is 1. For high-level image similarity, we use the cosine dis- tance between the full image embeddings from CLIP and DINO. DreamSim [Fu et al. 2023] is a mid-level similarity between two full images, emphasizing differences in image layouts, object poses, and semantic contents. Baseline Methods. We compare to two groups of baselines: models that take reference image as conditioning input, i.e., TransFill [Zhou et al. 2021] and Paint-by-Example [Yang et al. 2023]; and prompt- based image filling approaches including Stable Diffusion Inpaint- ing [Stability AI 2022] and Photoshop Generative Fill [Adobe Inc. 2023]. Since TransFill and Paint-by-Example can only use one refer- ence image during inference, we randomly sample one fromXğ‘Ÿğ‘’ğ‘“ as reference for each run of them. Choosing an appropriate prompt for prompt-based filling methods is a necessary component of getting a high quality result. So, for a fair comparison, instead of using a generic prompt like â€œa beautiful photo\", for each scene, we manually design a long prompt that describe the true scene in detail with the help of ChatGPT [OpenAI 2023]. Quantitative Comparison. We quantitatively evaluate all methods on RealBench. For each approach, we report average metrics across all 33 scenes. Specifically, for generative methods, each sceneâ€™s metric is itself computed from an average of 64 randomly generated samples (18 for Generative Fill due to Photoshop UI-only limits); for TransFill, the metrics are averaged over different choices of Table 1. Quantitative comparison of RealFill and baselines. On Real- Bench, our evaluation set of 33 diverse challenging scenes, RealFill outper- forms both prompt-based and reference-based baselines by a large margin on all types of metrics. Method low-level mid-level high-level PSNRâ†‘SSIMâ†‘LPIPSâ†“DreamSimâ†“DINOâ†‘CLIPâ†‘ prompt based SD Inpaint 10.63 0.282 0.605 0.213 0.831 0.874 Generative Fill 10.92 0.311 0.598 0.212 0.851 0.898 reference based Paint-by-Example 10.13 0.244 0.642 0.237 0.797 0.859 TransFill 13.28 0.404 0.542 0.192 0.860 0.866 RealFill (ours) 14.78 0.424 0.431 0.077 0.948 0.962 single reference image. As shown in Tab. 1, RealFill outperforms all baselines by a large margin across all metrics. Qualitative Comparison. In Fig. 5, we present a visual comparison between RealFill and the baselines. We also show the ground-truth and input images for each example. In order to better highlight the regions which are being generated, we overlay a semi-transparent white mask on the ground-truth and output images, covering the known regions of the target image. RealFill not only generates high- quality images, but also more faithfully reproduces the scene than the baseline methods. Paint-by-Example relies on the CLIP embed- ding of the reference images as the condition. This poses a challenge when dealing with complex scenes or attempting to restore object details, since CLIP embeddings only capture high-level semantic information. Although the geometric-based TransFill has decent numbers in terms of low-level metrics like PSNR, the outputs have much lower quality due to the lack of a good image prior, espe- cially when the scene structure has complex depth variants beyond a planar surface, which is hard for homography transformations to approximate. The generated results from Generative Fill are plausi- ble on their own. However, because natural language is limited in conveying complex visual information, they often exhibit substan- tial deviations from the original scenes depicted in the references. ACM Trans. Graph., Vol. 43, No. 4, Article 135. Publication date: July 2024.RealFill: Reference-Driven Generation for Authentic Image Completion â€¢ 135:7 Paint-by-ExampleGround-TruthRealFill (Ours)TransFillGenerative Fill Reference Images Fig. 5. Qualitative comparison of RealFill and baselines. Transparent white masks are overlayed on the unaltered known regions of the target images. Paint-by-Example loses fidelity with the reference images because it relies on CLIP embeddings, which only capture high-level semantic information. TransFill outputs low quality images due to the lack of a good image prior and the limitations of its geometry-based pipeline. While Generative Fill produces plausible results, they are inconsistent with the reference images because prompts have limited expressiveness. In contrast, RealFill generates high-quality results that have high fidelity with respect to the reference images. Table 2. Effect of correspondence-based seed selection. It helps RealFill output higher quality results, i.e., filtering out samples with fewer matches results in better quantitative scores. Filtering Rate PSNRâ†‘ SSIMâ†‘ LPIPSâ†“ DreamSimâ†“ DINOâ†‘ CLIPâ†‘ 0% 14.78 0.424 0.431 0.077 0.948 0.962 25% 15.01 0.427 0.421 0.066 0.955 0.967 50% 15.05 0.427 0.418 0.063 0.958 0.969 75% 15.10 0.427 0.417 0.060 0.961 0.970 Table 3. User study results. Here we show the preference percentages for the most realistic and most faithful image completions across 58 scenes by 44 participants. RealFill significantly surpasses baselines, especially on the faithfulness criterion. Method Most Realistic â†‘ Most Faithfulâ†‘ TransFill [Zhou et al. 2021] 2.0% 3.7% Paint-by-Example [Yang et al. 2023] 11.0% 1.9% Generative Fill [Adobe Inc. 2023] 23.4% 7.2% RealFill (ours) 63.7% 87.2% ACM Trans. Graph., Vol. 43, No. 4, Article 135. Publication date: July 2024.135:8 â€¢ Tang et al. Reference Images RealFill OutputsTarget ImageGround-Truth # of matches: 1111# of matches: 692# of matches: 83 # of matches: 97# of matches: 54# of matches: 9 Fig. 6. Correspondence-based seed selection. Given the reference images on the left, we show multiple RealFill outputs on the right along with the number of matched key points. We can see that fewer matches correlate with lower-quality outputs that are more divergent from the ground-truth. RealFill outputs after correspondence-based seed selection reference images target image ground-truth Fig. 7. Top 16 RealFill outputs after correspondence-based selection. Model first generates 64 examples in a batch with different sampled noises, then the top 16 images are automatically selected based on predicted corre- spondences. The outputs are mostly high-quality, so itâ€™s easy for users to pick the final image based on their own preference. Correspondence-Based Seed Selection . We evaluate the effect of our proposed correspondence-based seed selection described in Sec. 3.4. To measure the correlation between our seed selection mechanism and high-quality results, for each scene, we rank Real- Fillâ€™s outputs {ğ¼ğ‘œğ‘¢ğ‘¡}according to the number of matched keypoints, and then filter out a certain percent of the lowest-ranked samples. We then average the evaluation metrics only across the remaining samples. Higher filtering rates like 75% are quantitatively better than no filtering (Tab. 2). In Fig 6, we show multiple RealFill out- puts with the corresponding number of matched keypoints. These demonstrate a clear trend, where fewer matches usually indicate lower-quality results. Therefore, we followed such strategy to select RealFill outputs in Figs. 1 to 5: for each scene, model first generates 64 examples in a batch with different sampled noises, then we only keep the top 16 images based on predicted correspondences, and manually pick one from them. For fair comparison, we also manually pick the best outputs for each baseline as in Figs. 1 and 5. As shown in Fig. 7, after correspondence-based selection, the outputs are mostly high-quality with small variations. Users can make choices based on their own preference, therefore the involved human labor is very light. User study. In our study, 44 users evaluated the realism and faith- fulness of image completions from four different methods across 58 scenes. Participants first review an incomplete target image along- side the outputs of these methods, choosing the one they found most realistic. Then, with reference images provided, they select the most faithful completion. The study includes a mix of Real- Bench and 25 additional challenging scenes. All method outputs are randomly sampled to avoid bias. Overall, 2552 votes per criterion were collected. The results, as summarized in Tab. 3, show Real- Fillâ€™s superiority compared to the baseline methods especially in faithfulness. ACM Trans. Graph., Vol. 43, No. 4, Article 135. Publication date: July 2024.RealFill: Reference-Driven Generation for Authentic Image Completion â€¢ 135:9 Ground-TruthTarget ImageTransFill Output Reference Images RealFill Outputs Fig. 8. Influence of reference image choice on RealFill. For the given target image, we show different RealFill outputs by gradually increasing number of reference images from left to right. We can see outputs are getting better when having more references, e.g., the ghost gets recovered when having all three reference images. Note that all RealFill outputs significantly outperform the TransFill baseline. 5 DISCUSSION How does reference image choice affect RealFill? Empirically, when there are more reference images, or when the references have smaller variations from the target in terms of viewpoint and lighting, RealFill gives better results, as shown in Fig. 8. Would other baselines work? Image Stitching . It is possible to stitch the reference and target images together using correspondences. However, we find that even strong commercial image stitching software doesnâ€™t work when there are dramatic lighting changes or object motion. Taking the two scenes in Fig. 9 for example, multiple commercial software so- lutions produce no output, asserting that there are insufficient cor- respondences. In contrast, RealFill faithfully recovers these scenes. DreamBooth. Instead of finetuning an inpainting model, an al- ternative is to finetune a standard Stable Diffusion model on the reference images, i.e., DreamBooth, then use the finetuned T2I model to inpaint the target image [Lugmayr et al. 2022], as implemented in the popular Diffusers library [von Platen et al. 2022]1. However, be- cause this model is never trained with a masked prediction objective, it is much worse than RealFill, as shown in Fig. 10. What makes RealFill work? To explore why RealFill leads to strong results, especially on complex scenes, we make the following two hypotheses: RealFill relates multiple elements in a scene . If we make the con- ditioning image a blank canvas during inference, i.e., all entries of 1Diffusersâ€™ Stable Diffusion inpainting pipeline code. Reference Images Target ImageRealFill Outputs Fig. 9. RealFill vs. image stitching . Commercial image stitching soft- wares fail to produce any outputs when there are significant variations between reference and target images, such as disparate lighting conditions. In contrast, RealFill excels by producing accurate and high-quality results. It effectively recovers elements like rooftop tanks and balloons, maintain- ing fidelity even under varying lighting scenarios between the compared images. DreamBooth Outputs Reference Images RealFill OutputGround-Truth strength=1.0strength=0.8strength=0.5 Fig. 10. RealFill vs. DreamBooth. Finetuning a standard Stable Diffusion model on reference images and using it to fill missing regions, leads to drastically worse results compared to RealFill. We show samples for various levels of the strength hyper-parameter. ğ‘€ğ‘¡ğ‘”ğ‘¡ equal 1, we can see in Fig. 11 that the finetuned model is able to generate multiple scene variants with different structures, e.g., removing the foreground or background object, or manipulating the object layouts. This suggest that RealFill may understand the scene composition. RealFill captures correspondences among input images . Even if the reference and target images do not depict the same scene, the finetuned model is still able to fuse the corresponding contents of ACM Trans. Graph., Vol. 43, No. 4, Article 135. Publication date: July 2024.135:10 â€¢ Tang et al. Reference & Target ImagesGenerated Scene Variants Fig. 11. RealFill is able to generate multiple scene variants when conditioned on a blank image as input, e.g., people are added or removed in the first and second rows. This suggests that the finetuned model can relate elements inside the scene in a compositional manner. Reference Images RealFill Outputs Fig. 12. When the reference and target images do not depict the same scene, the finetuned model is still able to fuse the reference contents into the target image in a semantically-reasonable way, suggesting that it captures both real or invented correspondences between input images. the reference images into the target area seamlessly, as shown in Fig. 12. This suggests that RealFill is able to capture and utilize real or invented correspondences between reference and target images to do generation. Previous works [Luo et al. 2023; Tang et al. 2023; Reference Images Target ImageRealFill Output Fig. 13. Failure cases of RealFill. (Top) RealFill fails to recover the precise 3D scene structure, e.g., the output husky plush has different pose com- pared to the reference; (Bottom) RealFill fails to handle cases that are also challenging for the base T2I diffusion model, e.g., the words on the store sign are wrongly spelled. Zhang et al. 2023a] also found similar emergent correspondence inside pretrained diffusion models. Limitations. Because RealFill requires a gradient-based finetuning process on input images, it is relatively slow. Empirically, we also find that, when there is a large viewpoint change between reference and target images, RealFill fails to recover the 3D scene faithfully, es- pecially when thereâ€™s only a single reference image, as in Fig. 13 top. In addition, because RealFill relies on the base pretrained modelâ€™s image prior, it also fails to handle cases that are challenging for the base model, e.g., Stable Diffusion is not good at generating fine details, such as text, human faces, or body parts, as in Fig. 13 bottom. Lastly, similar to the user study in Tab. 3, we conducted another study on RealBench where participants compare randomly sampled RealFill output vs. ground-truth image for each scene. Among the collected 396 pairwise comparisons, RealFill only gets 23.7% of the votes for realism and 22.0% for faithfulness, vs. ground-truthâ€™s 76.3% and 78.0% respectively. This is reasonable because itâ€™s easy for hu- man to spot artifacts especially for side-by-side comparison, but it also shows that more future improvements are needed to make RealFill achieve perfect authentic image completion. Societal Impact. This research aims to create a tool that can help users express their creativity and improve the quality of their per- sonal photographs through image generation. However, advanced image generation methods can have complex impacts on society. Our proposed method inherits some of the concerns that are associated with this class of technology, such as the potential to alter sensitive personal characteristics. The open-source pretrained model that we use in our work, Stable Diffusion, exhibits some of these concerns. However, we have not found any evidence that our method is more likely to produce biased or harmful content than previous work. Despite these findings, it is important to continue investigating the potential risks of image generation technology. Future research should focus on developing methods to mitigate bias and harmful ACM Trans. Graph., Vol. 43, No. 4, Article 135. Publication date: July 2024.RealFill: Reference-Driven Generation for Authentic Image Completion â€¢ 135:11 content, and to ensure that image generation tools are used in a responsible manner. 6 CONCLUSION In this work, we introduce the problem of Authentic Image Comple- tion, where given a few reference images, we intend to complete some missing regions of a target image with the content that â€œshould have been thereâ€ rather than â€œwhatcould have been thereâ€. To tackle this problem, we proposed a simple yet effective approach called RealFill, which first finetunes a T2I inpainting diffusion model on the reference and target images, and then uses the adapted model to fill the missing regions. We show that RealFill produces high-quality image completions that are faithful to the content in the reference images, even when there are large differences between reference and target images such as viewpoint, aperture, lighting, image style, and object pose. ACKNOWLEDGMENTS Luming and Bharath are supported by NSF IIS-2144117. We would like to thank Rundi Wu, Qianqian Wang, Viraj Shah, Ethan Weber, Zhengqi Li, Kyle Genova, Richard Tucker, Boyang Deng, Maya Gold- enberg, Noah Snavely, Ben Poole, Ben Mildenhall, Alex Rav-Acha, Pratul Srinivasan, Dor Verbin, Jon Barron and all the anonymous reviewers for their valuable discussion and feedbacks, and thank Zeya Peng, Rundi Wu, Shan Nan for their contribution to the eval- uation dataset. A special thanks to Jason Baldridge, Kihyuk Sohn, Kathy Meier-Hellstern, and Nicole Brichtova for their feedback and support for the project. A IMPLEMENTATION DETAILS A.1 RealFill For each scene, we finetune the Stable Diffusion inpainting model [Sta- bility AI 2022] for 2,000 iterations with a batch size of 16 on a single NVIDIA A100 GPU with LoRA rank 8. With a probability of 0.1, we randomly dropout prompt ğ‘, mask ğ‘šand LoRA layers indepen- dently during training. The learning rate is set to 2e-4 for the U-Net and 4e-5 for the text encoder. The whole finetuning process takes around one hour but usually 20 minutes would give pretty good results already in many cases. Note that these hyper-parameters could be further tuned for each scene to get better performance, e.g., some scenes converge more quickly may overfit if trained for too long. However, for the sake of fair comparison, we use a constant set of hyper-parameters for all results shown in the paper. During inference, we use DDPM [Ho et al. 2020] sampler with step 200 and guidance weight 1.0, i.e., without classifier-free guidance. A.2 User Study In the user study, participants are asked to evaluated the realism and faithfulness of image completions across 58 scenes from four differ- ent methods including RealFill, Paint-by-Example [Yang et al. 2023], TransFill [Zhou et al. 2021], and Photoshop Generative Fill [Adobe Inc. 2023]. All method outputs are randomly sampled without hu- man intervention. For each scene, the placement organization of different method outputs are also randomly shuffled to avoid user bias. We are aiming to complete the image below by filling the gray region: Which completed image looks the best to you? i.e., most photorealistic and visually compelling 1 Fig. 14. An example of the realism question in the user study. Users first review an incomplete target image alongside the shuffled outputs of four methods, then choose the one they find most realistic. 2 Other photos capture the same scene: Given the other photos, which completed image is most faithful to the original scene? i.e., recover the true scene Fig. 15. An example of the faithfulness question in the user study. After the user answered the realism question, the reference images are provided. Then, they are asked to select the most faithful completion. Specifically, participants first review an incomplete target image alongside the outputs of these methods, choosing the one they found most realistic, as shown in Fig. 14. Then, with reference images provided, they select the most faithful completion, as shown in Fig. 15. REFERENCES Adobe Inc. 2023. Adobe Photoshop. https://www.adobe.com/products/photoshop.html Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. 2023. Break-A-Scene: Extracting Multiple Concepts from a Single Image. ArXiv preprint abs/2305.16311 (2023). https://arxiv.org/abs/2305.16311 Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Goldman. 2009. Patch- Match: A randomized correspondence algorithm for structural image editing. ACM Trans. Graph. 28, 3 (2009), 24. Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. 2000. Image inpainting. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques . 417â€“424. Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2023. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 18392â€“18402. ACM Trans. Graph., Vol. 43, No. 4, Article 135. Publication date: July 2024.135:12 â€¢ Tang et al. Mathilde Caron, Hugo Touvron, Ishan Misra, HervÃ© JÃ©gou, Julien Mairal, Piotr Bo- janowski, and Armand Joulin. 2021. Emerging Properties in Self-Supervised Vi- sion Transformers. In 2021 IEEE/CVF International Conference on Computer Vi- sion, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021 . IEEE, 9630â€“9640. https://doi.org/10.1109/ICCV48922.2021.00951 Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming- Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. 2023. Muse: Text-to-image generation via masked generative transformers.ArXiv preprint abs/2301.00704 (2023). https://arxiv.org/abs/2301.00704 Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. 2022. MaskGIT: Masked Generative Image Transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022 . IEEE, 11305â€“11315. https://doi.org/10.1109/CVPR52688.2022.01103 Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, and William W Cohen. 2023. Subject-driven text-to-image generation via apprenticeship learning. ArXiv preprint abs/2304.00186 (2023). https://arxiv.org/abs/2304.00186 Antonio Criminisi, Patrick Perez, and Kentaro Toyama. 2003. Object removal by exemplar-based inpainting. In 2003 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2003. Proceedings. , Vol. 2. IEEE, IIâ€“II. Prafulla Dhariwal and Alexander Quinn Nichol. 2021. Diffusion Models Beat GANs on Image Synthesis. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual , Marcâ€™Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (Eds.). 8780â€“8794. https://proceedings. neurips.cc/paper/2021/hash/49ad23d1ec9fa4bd8d77d02681df5cfa-Abstract.html Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. 2023. DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data. ArXiv preprint abs/2306.09344 (2023). https: //arxiv.org/abs/2306.09344 Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022. An image is worth one word: Personalizing text-to- image generation using textual inversion. ArXiv preprint abs/2208.01618 (2022). https://arxiv.org/abs/2208.01618 James Hays and Alexei A Efros. 2007. Scene completion using millions of photographs. ACM Transactions on Graphics (ToG) 26, 3 (2007), 4â€“es. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising Diffusion Probabilistic Models. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Bal- can, and Hsuan-Tien Lin (Eds.). https://proceedings.neurips.cc/paper/2020/hash/ 4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022 . OpenReview.net. https://openreview.net/forum?id= nZeVKeeFYf9 Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. 2017. Globally and locally consistent image completion. ACM Transactions on Graphics (ToG) 36, 4 (2017), 1â€“14. Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2023. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 6007â€“6017. Soo Ye Kim, Kfir Aberman, Nori Kanazawa, Rahul Garg, Neal Wadhwa, Huiwen Chang, Nikhil Karnad, Munchurl Kim, and Orly Liba. 2022. Zoom-to-Inpaint: Image In- painting with High-Frequency Details. In IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2022, New Orleans, LA, USA, June 19-20, 2022 . IEEE, 476â€“486. https://doi.org/10.1109/CVPRW56347.2022.00063 Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. 2018. Image inpainting for irregular holes using partial convolutions. In Proceedings of the European conference on computer vision (ECCV) . 85â€“100. Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. 2023. Zero-1-to-3: Zero-shot One Image to 3D Object. arXiv:2303.11328 [cs.CV] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. 2022. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 11461â€“11471. Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holynski, and Trevor Darrell. 2023. Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence. ArXiv preprint abs/2305.14334 (2023). https://arxiv.org/abs/2305. 14334 Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra- mamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In ECCV. Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. 2023. T2i-adapter: Learning adapters to dig out more con- trollable ability for text-to-image diffusion models. ArXiv preprint abs/2302.08453 (2023). https://arxiv.org/abs/2302.08453 OpenAI. 2023. ChatGPT: Optimizing Language Models for Dialogue. https://openai. com/blog/chatgpt. Accessed: November 17, 2023. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event (Proceedings of Machine Learning Research, Vol. 139) , Marina Meila and Tong Zhang (Eds.). PMLR, 8748â€“8763. http://proceedings.mlr.press/v139/radford21a.html Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, Yuanzhen Li, and Varun Jampani. 2023. DreamBooth3D: Subject-Driven Text-to-3D Generation. ArXiv preprint abs/2303.13508 (2023). https://arxiv.org/abs/2303.13508 Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. ArXiv preprint abs/2204.06125 (2022). https://arxiv.org/abs/2204.06125 Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjÃ¶rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition . 10684â€“10695. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2023a. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 22500â€“22510. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein, and Kfir Aberman. 2023b. HyperDreamBooth: Hy- perNetworks for Fast Personalization of Text-to-Image Models. ArXiv preprint abs/2307.06949 (2023). https://arxiv.org/abs/2307.06949 Simo Ryu. 2023. Low-rank Adaptation for Fast Text-to-Image Diffusion Fine-tuning. https://github.com/cloneofsimo/lora. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022. Photorealistic text-to-image diffusion models with deep language under- standing. Advances in Neural Information Processing Systems 35 (2022), 36479â€“36494. Dvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan, and Gal Chechik. 2023. It is all about where you start: Text-to-image generation with seed selection. ArXiv preprint abs/2304.14530 (2023). https://arxiv.org/abs/2304.14530 Qi Shan, Brian Curless, Yasutaka Furukawa, Carlos Hernandez, and Steven M Seitz. 2014. Photo uncrop. In Computer Visionâ€“ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VI 13 . Springer, 16â€“31. Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al . 2023. StyleDrop: Text- to-Image Generation in Any Style. ArXiv preprint abs/2306.00983 (2023). https: //arxiv.org/abs/2306.00983 Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2021. Score-Based Generative Modeling through Sto- chastic Differential Equations. In 9th International Conference on Learning Rep- resentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net. https://openreview.net/forum?id=PxTIG12RRHS Stability AI. 2022. Stable-Diffusion-2-Inpainting. https://huggingface.co/stabilityai/ stable-diffusion-2-inpainting. Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. 2021. LoFTR: Detector-Free Local Feature Matching With Transformers. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021 . Com- puter Vision Foundation / IEEE, 8922â€“8931. https://doi.org/10.1109/CVPR46437. 2021.00881 Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. 2022. Resolution-robust large mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF winter conference on applications of computer vision . 2149â€“2159. Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. 2023. Emergent Correspondence from Image Diffusion. In Thirty-seventh Confer- ence on Neural Information Processing Systems . https://openreview.net/forum?id= ypOiXjdfnU Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, and Thomas Wolf. 2022. Diffusers: State-of-the-art diffu- sion models. https://github.com/huggingface/diffusers. Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. 2023.ğ‘ƒ+: Extended Textual Conditioning in Text-to-Image Generation. ArXiv preprint abs/2303.09522 (2023). https://arxiv.org/abs/2303.09522 Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pel- legrini, Yasumasa Onoe, Sarah Laszlo, David J Fleet, Radu Soricut, et al. 2023. Imagen ACM Trans. Graph., Vol. 43, No. 4, Article 135. Publication date: July 2024.RealFill: Reference-Driven Generation for Authentic Image Completion â€¢ 135:13 editor and editbench: Advancing and evaluating text-guided image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 18359â€“18369. Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. 2022. Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation. ArXiv preprint abs/2212.11565 (2022). https://arxiv.org/abs/2212.11565 Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. 2023. Paint by example: Exemplar-based image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 18381â€“18391. Raymond A. Yeh, Chen Chen, Teck-Yian Lim, Alexander G. Schwing, Mark Hasegawa- Johnson, and Minh N. Do. 2017. Semantic Image Inpainting with Deep Generative Models. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017 . IEEE Computer Society, 6882â€“6890. https: //doi.org/10.1109/CVPR.2017.728 Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang. 2018. Generative Image Inpainting With Contextual Attention. In2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018 . IEEE Computer Society, 5505â€“5514. https://doi.org/10.1109/CVPR.2018. 00577 Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. 2023a. A Tale of Two Features: Stable Diffu- sion Complements DINO for Zero-Shot Semantic Correspondence. ArXiv preprint abs/2305.15347 (2023). https://arxiv.org/abs/2305.15347 Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023b. Adding Conditional Control to Text-to-Image Diffusion Models. Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. 2018. The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018 . IEEE Computer Society, 586â€“595. https://doi.org/10. 1109/CVPR.2018.00068 Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. 2023b. Unleashing Text-to-Image Diffusion Models for Visual Perception. ICCV (2023). Yunhan Zhao, Connelly Barnes, Yuqian Zhou, Eli Shechtman, Sohrab Amirghodsi, and Charless Fowlkes. 2023a. Geofill: Reference-based image inpainting with better geometric understanding. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision . 1776â€“1786. Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. 2019. Pluralistic Image Completion. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 . Computer Vision Foundation / IEEE, 1438â€“1447. https://doi.org/10.1109/CVPR.2019.00153 Yuqian Zhou, Connelly Barnes, Eli Shechtman, and Sohrab Amirghodsi. 2021. Trans- Fill: Reference-Guided Image Inpainting by Merging Multiple Color and Spatial Transformations. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021 . Computer Vision Foundation / IEEE, 2266â€“2276. https://doi.org/10.1109/CVPR46437.2021.00230 Zixin Zhu, Xuelu Feng, Dongdong Chen, Jianmin Bao, Le Wang, Yinpeng Chen, Lu Yuan, and Gang Hua. 2023. Designing a Better Asymmetric VQGAN for StableDiffusion. ArXiv preprint abs/2306.04632 (2023). https://arxiv.org/abs/2306.04632 ACM Trans. Graph., Vol. 43, No. 4, Article 135. Publication date: July 2024.",
      "references": [
        "Break-A-Scene: Extracting Multiple Concepts from a Single Image.",
        "Patch- Match: A randomized correspondence algorithm for structural image editing.",
        "Image inpainting.",
        "Instructpix2pix: Learning to follow image editing instructions.",
        "Emerging Properties in Self-Supervised Vi- sion Transformers.",
        "Muse: Text-to-image generation via masked generative transformers.",
        "MaskGIT: Masked Generative Image Transformer.",
        "Subject-driven text-to-image generation via apprenticeship learning.",
        "Object removal by exemplar-based inpainting.",
        "Diffusion Models Beat GANs on Image Synthesis.",
        "DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data.",
        "An image is worth one word: Personalizing text-to- image generation using textual inversion.",
        "Scene completion using millions of photographs.",
        "Denoising Diffusion Probabilistic Models.",
        "LoRA: Low-Rank Adaptation of Large Language Models.",
        "Globally and locally consistent image completion.",
        "Imagic: Text-based real image editing with diffusion models.",
        "Zoom-to-Inpaint: Image In- painting with High-Frequency Details.",
        "Image inpainting for irregular holes using partial convolutions.",
        "Zero-1-to-3: Zero-shot One Image to 3D Object.",
        "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.",
        "T2i-adapter: Learning adapters to dig out more con- trollable ability for text-to-image diffusion models.",
        "Learning Transferable Visual Models From Natural Language Supervision.",
        "DreamBooth3D: Subject-Driven Text-to-3D Generation.",
        "Hierarchical text-conditional image generation with clip latents.",
        "High-resolution image synthesis with latent diffusion models.",
        "Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.",
        "HyperDreamBooth: Hy- perNetworks for Fast Personalization of Text-to-Image Models.",
        "Repaint: Inpainting using denoising diffusion probabilistic models.",
        "Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence.",
        "It is all about where you start: Text-to-image generation with seed selection.",
        "Photo uncrop.",
        "StyleDrop: Text- to-Image Generation in Any Style.",
        "Score-Based Generative Modeling through Sto- chastic Differential Equations.",
        "LoFTR: Detector-Free Local Feature Matching With Transformers.",
        "Resolution-robust large mask inpainting with fourier convolutions.",
        "Emergent Correspondence from Image Diffusion.",
        "Diffusers: State-of-the-art diffu- sion models.",
        "ğ‘ƒ+: Extended Textual Conditioning in Text-to-Image Generation.",
        "Imagen editor and editbench: Advancing and evaluating text-guided image inpainting.",
        "Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation.",
        "Paint by example: Exemplar-based image editing with diffusion models.",
        "Semantic Image Inpainting with Deep Generative Models.",
        "Generative Image Inpainting With Contextual Attention.",
        "A Tale of Two Features: Stable Diffu- sion Complements DINO for Zero-Shot Semantic Correspondence.",
        "Adding Conditional Control to Text-to-Image Diffusion Models.",
        "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric.",
        "Unleashing Text-to-Image Diffusion Models for Visual Perception.",
        "Geofill: Reference-based image inpainting with better geometric understanding.",
        "Pluralistic Image Completion.",
        "Trans- Fill: Reference-Guided Image Inpainting by Merging Multiple Color and Spatial Transformations.",
        "Designing a Better Asymmetric VQGAN for StableDiffusion."
      ],
      "meta_data": {
        "arxiv_id": "2309.16668v2",
        "doi": "10.1145/3658237",
        "authors": [
          "Luming Tang",
          "Nataniel Ruiz",
          "Qinghao Chu",
          "Yuanzhen Li",
          "Aleksander Holynski",
          "David E. Jacobs",
          "Bharath Hariharan",
          "Yael Pritch",
          "Neal Wadhwa",
          "Kfir Aberman",
          "Michael Rubinstein"
        ],
        "published_date": "2023-09-28T17:59:29Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Introduces the task of Authentic Image Completion: filling missing regions in a target photo using a few (â‰¤5) unaligned reference photos of the same scene, aiming to generate what â€œshould have been thereâ€ (scene-faithful) rather than merely plausible content. Proposes RealFill, a reference-driven image completion approach that personalizes a pretrained diffusion inpainting model on the given references + target so it internalizes scene-specific appearance, lighting, and style. Adds Correspondence-Based Seed Selection to automatically rank/filter stochastic diffusion samples using feature-match counts to references, reducing manual curation. Releases RealBench, a benchmark of 33 challenging scenes (23 outpainting, 10 inpainting) with ground truth, and shows large improvements over prompt-only and prior reference-based baselines in both metrics and user studies.",
        "methodology": "Starts from Stable Diffusion v2 inpainting (latent diffusion) and fine-tunes it per scene using LoRA adapters injected into both U-Net and text encoder. Training uses a fixed prompt with a rare token (\"a photo of [V]\") and random rectangle masks; optimization minimizes standard diffusion noise-prediction loss conditioned on prompt, random mask, and the masked clean image. Includes both reference images and the target image in training; for the target, loss is computed only on known (unmasked-by-task) pixels to avoid learning unknown regions. Inference runs a DDPM sampler (200 steps, guidance weight 1.0) conditioned on target + task mask; to prevent distortion of known pixels, the generated result is alpha-composited with the original target using a feathered mask. Correspondence-Based Seed Selection: generate a batch (e.g., 64) with different random seeds, compute dense/feature correspondences (LoFTR) between each outputâ€™s filled region and the reference images, rank by match count, and keep top-k (or filter bottom p%).",
        "experimental_setup": "Creates RealBench: 33 scenes with 1â€“5 reference images, a target image, a binary completion mask, and a ground-truth completed image; includes 23 outpainting and 10 inpainting cases with large viewpoint/lighting/style/defocus/motion changes. Baselines: Stable Diffusion Inpainting and Photoshop Generative Fill (prompt-based); TransFill and Paint-by-Example (reference-based, single reference at inferenceâ€”randomly sample one reference). Prompts for prompt-based baselines are manually engineered per scene (detailed, assisted by ChatGPT) for fairness. Metrics: low-level (PSNR, SSIM, LPIPS) computed only in filled region; mid-level DreamSim on full image; high-level CLIP and DINO cosine similarity on full-image embeddings. For generative methods, report averages over 64 random samples per scene (18 for Photoshop due to UI limits); for TransFill average over different single-reference choices. Also reports correspondence-based filtering ablation at different filtering rates. User study: 44 participants, 58 scenes (RealBench + 25 extra), realism choice without refs then faithfulness choice with refs; 2552 votes per criterion; outputs randomly sampled and shuffled to avoid bias.",
        "limitations": "Requires per-scene gradient-based fine-tuning, making it relatively slow (â‰ˆ1 hour per scene with reported settings; ~20 minutes often sufficient but still nontrivial) and requiring GPU compute; hyperparameters may need tuning to avoid under/overfitting across scenes. Fidelity degrades with large viewpoint changes, especially with only one reference, and it may not recover precise 3D structure (pose/geometry inconsistencies). Inherits failure modes of the base diffusion prior (Stable Diffusion), struggling with fine details like text, faces, and body parts. Despite improvements, side-by-side comparisons show outputs are often distinguishable from ground truth (in a separate study, RealFill loses to ground truth in realism/faithfulness). Assumes sufficient overlap between references and target such that a human could infer the missing content; correspondence-based seed selection depends on successful feature matching and may fail in textureless or extreme condition changes.",
        "future_research_directions": "Speed/efficiency: replace per-scene fine-tuning with faster personalization (hypernetworks, parameter-efficient adapters with fewer steps, distillation, caching) or amortized models trained for reference-driven completion. Stronger geometric/3D consistency: integrate explicit multi-view geometry, depth/NeRF priors, or diffusion-based 3D constraints to better handle large viewpoint changes and dynamic scenes. Better conditioning on multiple references: architectures that ingest sets of reference images directly (cross-attention/transformer set encoders) rather than relying on fine-tuning alone; improve robustness to varying styles/lighting via domain-invariant conditioning. Improve details and text rendering: couple with high-resolution refinement, specialized text/face modules, or better base models. Automated sample selection: learn a faithfulness scorer beyond match counts; combine correspondences with semantic and uncertainty measures to reduce batch size and improve reliability. Broader evaluation and safety: expand RealBench, include harder categories, and study/mitigate potential biases and misuse in photo manipulation.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
      "full_text": "1 Privacy Leakage on DNNs: A Survey of Model Inversion Attacks and Defenses Hao Fang , Yixiang Qiu , Hongyao Yu , Wenbo Yu , Jiawei Kong , Baoli Chong , Bin Chen , Member, IEEE, Xuan Wang , Member, IEEE, Shu-Tao Xia , Member, IEEE, and Ke Xu , Fellow, IEEE Abstractâ€”Deep Neural Networks (DNNs) have revolutionized various domains with their exceptional performance across nu- merous applications. However, Model Inversion (MI) attacks, which disclose private information about the training dataset by abusing access to the trained models, have emerged as a formidable privacy threat. Given a trained network, these attacks enable adversaries to reconstruct high-fidelity data that closely aligns with the private training samples, posing significant privacy concerns. Despite the rapid advances in the field, we lack a comprehensive and systematic overview of existing MI attacks and defenses. To fill this gap, this paper thoroughly investigates this realm and presents a holistic survey. Firstly, our work briefly reviews early MI studies on traditional machine learning scenarios. We then elaborately analyze and compare numerous recent attacks and defenses on Deep Neural Networks (DNNs) across multiple modalities and learning tasks. By meticulously an- alyzing their distinctive features, we summarize and classify these methods into different categories and provide a novel taxonomy. Finally, this paper discusses promising research directions and presents potential solutions to open issues. To facilitate further study on MI attacks and defenses, we have implemented an open- source model inversion toolbox on GitHub 1. Index Termsâ€”Data Leakage, Model Inversion Attack, Model Inversion Defense, Privacy Security. I. I NTRODUCTION W ITH the unprecedented development of Deep Learn- ing (DL), Deep Neural Networks (DNNs) have been widely applied in a variety of fields, including medical re- search [1], [2], financial analysis [3], [4], and personalized recommendations [5]. While many practical applications of DNNs require training on privacy-sensitive datasets, people may mistakenly expect that these private training samples are securely encoded in the weights of the trained network. However, numerous studies [6]â€“[8] have demonstrated that Hao Fang, Yixiang Qiu, and Wenbo Yu are with the Tsinghua Shen- zhen International Graduate School, Tsinghua University, Shenzhen, Guang- dong 518055, China (e-mail: {fang-h23, qiu-yx24 }@mails.tsinghua.edu.cn; wenbo.research@gmail.com). Hongyao Yu, Jiawei Kong, Baoli Chong, Bin Chen, and Xuan Wang are with the School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, Guangdong 518055, China (e-mail: {yuhongyao, kongjiawei, chongbaoli }@stu.hit.edu.cn; chenbin2021@hit.edu.cn; wangx- uan@cs.hitsz.edu.cn). Shu-Tao Xia is with the Tsinghua Shenzhen International Graduate School, Tsinghua University, Shenzhen, Guangdong 518055, China, and also with the Peng Cheng Laboratory, Shenzhen, Guangdong 518055, China (e-mail: xiast@sz.tsinghua.edu.cn). Ke Xu is with the Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China. (e-mail: xuke@tsinghua.edu.cn). Hao Fang and Yixiang Qiu contribute equally to this paper. Corresponding author: Bin Chen (e-mail: chenbin2021@hit.edu.cn). 1https://github.com/ffhibnese/Model-Inversion-Attack-ToolBox ğ‘‹ Train ğ‘“0 ğ‘“Î¸ à· ğ‘‹ Closer MI Attacks TrainingInference Fig. 1: An illustration of model inversion attacks. The service provider trains the model f0(Â·) with private dataset X and obtains fÎ¸(Â·) that is then released for public usage. During the model inference stage, the adversary inverts the available fÎ¸(Â·) to acquire recovered data Ë†X, which highly resembles the inaccessible X. a malicious attacker could utilize the pre-trained models to reveal critical privacy information about the underlying train- ing data. Among different types of attacks, Model Inversion (MI), as presented in Fig. 1, stands out as a potent threat that has raised increasing privacy concerns. Unlike the membership inference attack or the property inference attack which only reveal narrow aspects of privacy associated with training data [9], MI allows an adversary to fully reconstruct the training samples by effectively exploiting the guidance from the given pre-trained models [10]. Model Inversion attack was first introduced by [6] on tabular data in the context of genomic privacy. They demonstrate that when partial input attributes are available to the attacker, the sensitive genomic markers can be accurately recovered by maximizing the posterior probability corresponding to a spe- cific output of the given linear regression model. A following study [11] extends MI attacks to decision trees and shadow face recognition networks and reconstructs certain sensitive personal information based on gradient descent optimization. Subsequent studies [12], [13] further broaden the scope of MI attacks to encompass various Machine Learning (ML) models and more forms of data using diverse optimizing strategies. Additionally, significant advancements have also been made in enhancing the attack efficacy while relaxing the assumptions about the attackerâ€™s capabilities and knowledge. Although these methods have exhibited outstanding performance in traditional ML scenarios for shallow networks and simple data, the effectiveness is significantly reduced when attacking deeper and wider DNNs trained on more complex and higher- arXiv:2402.04013v2  [cs.CV]  11 Sep 20242 dimensional data such as RGB images [14]. To address this challenge, recent studies [14], [15] have made great efforts and achieved remarkable performance im- provements. [14] first achieved RGB image recovery from deeper CNNs by exploiting Generative Adversarial Networks (GANs) [16] as image priors. Specifically, they propose to train a GAN model with publicly available data to generate high-fidelity images for target classes. Benefiting from the rich prior knowledge encoded in the learned GAN model, the quality and fidelity of reconstructed images are significantly improved. Building upon this generative recovery paradigm, a variety of following studies [17]â€“[19] further propose di- verse novel techniques and successfully obtain impressive reconstruction results. Moreover, recent explorations in model inversion on text data [20] and graph data [21] have shown that language models and graph neural networks are also vulnerable to MI threats. Particularly, the serious privacy breach of Large Language Models (e.g., ChatGPT [22]) has drawn growing attention. Users can generate text-based queries and interact with ChatGPT, raising concerns regarding the inadvertent exposure of sensitive information through the modelâ€™s responses [23]. This threat is greatly amplified in this explosion era of AIGC. To protect the private training data from powerful MI attacks, a series of defense strategies [24]â€“[26] have been pro- posed to ensure the security of the publicly released models, which can be uniformly divided into Model Output Processing and Robust Model Learning. In light that MI attacks generally rely on the rich information within the model output, Model Output Processing aims to mislead the attacker by diminish- ing the valid information in model output [27]. Meanwhile, Robust Model learning prefers to incorporate well-designed mechanisms into the model training or fine-tuning process to essentially improve the victimâ€™s MI robustness [25]. Despite the privacy threat to training data raised by MI attacks, a comprehensive overview summarizing their ad- vances is currently not available. Recent related research [28] predominantly focuses on the diverse taxonomies and challenges of MI attacks solely based on some early stud- ies only for the tabular and image data, [29] analyzes the Python code implementation of several attacks, or [9] briefly introduces several representative MI attacks in a subsection among various types of privacy inference attacks. In contrast, we present a systematic and elaborate review of the advanced MI attacks and defenses over multiple data modalities and different learning tasks, thoroughly exploring the landscape. The rest of the paper is organized as follows. Sec. II formally defines model inversion attacks and presents the overall profile of MI attacks with diverse taxonomies. Sec. III briefly reviews early studies on traditional ML scenarios while Sec. IV details attacks on DNNs along with a general framework of methods on classification tasks, based on which we systematically introduce the differences and relationships of these methods. Sec. V includes MI attacks on more modali- ties. Sec. VI then introduces and categorizes numerous defense strategies against MI attacks. Sec. VII discusses the social impact. Finally, Sec. VIII concludes this work and summarizes some remaining challenges and possible future directions. II. M ODEL INVERSION OVERVIEW In this section, we first formally formulate the definition of Model Inversion. Next, we present an overview of these methods from different perspectives based on the taxonomies as summarized in Figure 2. A. Definition of Model Inversion The process of transforming training data into machine- learned systems is not one way but two [30]: the learned models can also be inverted to obtain the training data. Canonical model inversion attacks [10], [14], [21], [31] can be defined as follows: given a pre-trained model fÎ¸(Â·) and certain prior knowledge D (which may be none), the attacker aims to develop an effective algorithm T that inverts the network fÎ¸ to recover as much as possible of the private information about the training dataset X. The inversion algorithm T generally capitalizes on the confidences or feature representations output by the model to achieve satisfactory reconstruction results. A successful model inversion attack should generate realistic reconstructed results Ë†X = T (fÎ¸, D) that closely resemble the target dataset X without having access to it. B. Attackerâ€™s Capability Based on the attackerâ€™s capability and knowledge, existing MI methods can be uniformly categorized into white-box and black-box attacks. Specifically, a white-box scenario implies that the attacker has full access to the weights and outputs of the target model. Benefiting from the available gradient information of the victim model, white-box attacks [14], [15] have demonstrated excellent recovery efficacy through advanced gradient-based optimization strategies. Conversely, black-box settings only allow access to the predicted confi- dence probabilities or hard labels, i.e., an attacker can solely control the input query and obtain the model output. To handle this more challenging situation, researchers employ gradient estimation [32], [33] or heuristic algorithm [18], [34] to navigate the data recovery. C. Reconstructed Data Modalities Given that the attack paradigms are largely contingent upon the target data modality, we present a taxonomy of MI attacks organized based on data modality to provide a concise overview of these attacks. Tabular Data Recovery (Sec. III). Pioneer MI studies [6], [11] investigate attacks on tabular data that contains sensitive personal information (e.g., addresses, telephones, and medical records) in traditional ML scenarios. An adversary maliciously accesses an ML model and infers sensitive information by picking feature values for each individual from all possible feature combinations to minimize the expected error rates calculated by the individualâ€™s label and the model output. Image Data Recovery (Sec. III and IV). Sensitive image data like human faces or medical diagnostic images are also in the scope of MI attacks. Compared with tabular data, an image contains an intractable large set of features to reconstruct,3 Model Inversion Data Modalities Learning Tasks Tabular data Image data Text data Graph data Classification Generation Representation  Learning [11, 19] [6, 11, 19-20,  30, 58] White-box Black-box [21, 47, 105] [48-51, 105] [38, 41] [23, 37, 40, 44, 46,  91-92, 94] [42-43] [20, 39, 42, 93] [10-12, 14-15, 17- 18, 35, 38, 57,  67-68, 71-73] [11-13, 17-19, 32- 34, 36-37, 59, 63,  65-66, 69-70] [12, 20, 23, 39-40,  42, 44-46, 91-94] [12, 31, 41-43, 52] [10-12, 14-15, 17-18,  21, 31, 35, 47, 52, 57,  67-68, 71-73, 105] [6, 11-13, 17-19, 32-34,  36, 45, 48-51, 58-59, 63, 65-66, 69-70, 105] Fig. 2: Threat model of generative model inversion attacks against image classification tasks. which makes the feature-picking strategy impractical. To ad- dress this, [11] proposes an approach against face classifiers that formulates the model inversion as an optimization prob- lem. Subsequent studies [10], [14], [35] generally follow this basic paradigm and further employ numerous techniques (e.g., generative models) to promote the reconstruction of higher di- mension and resolution images. Another roadmap [13], [36] is to construct a set of input-output pairs by repeatedly querying the victim model and train another inversion network to map the output predictions back to the input images. Besides, [37], [38] consider directly inducing the target diffusion models to generate their private training data. Text Data Recovery (Sec. V-A). Examples of text data leakage are analogous to tabular ones. One difference is that in certain scenarios, the user input [39] or prompts preceding the input sentences [40] are also pivotal privacy components targeted by MI attacks. Moreover, the distinctive nature of text data and language models motivates rich forms of attack algorithms, which can be broadly categorized into Embedding Optimization [31], [41], Inverse Mapping [42], [43], Token search [44], [45], and Malicious Prompt Design [23], [46]. Graph Data Recovery (Sec. V-B). Graphs that contain rich sensitive personal information and relational data (e.g., friendship or intimacy) between users are also vulnerable to MI attacks. Researchers delve into the discrete structures of graph data and propose various powerful techniques to achieve successful reconstruction. We further categorize them into: Adjacency Optimization [21], [47], Inverse Mapping [48], [49], and Relationship Reasoning [50], [51]. D. Learning Tasks Given that the attack algorithms are also task-specific, we undertake a concise review of these methods from the perspective of learning tasks. Figure 2 demonstrates that most MI studies concentrate on classification tasks, which typically involve minimizing the classification loss specific to the target label to update the data using diverse optimization algorithms. For generation tasks, attackers usually conduct a heuristic search for the private data based on a pre-defined metric [44], [45], [52] or directly induce the generative models to output their training data [37], [38]. Attacks against representation learning [39], [44] tasks generally target text data and their attack algorithms are highly determined by the specific pre- training tasks of the victim model. Relationship with Data Modality. The victim models trained on image and graph data are primarily used for classification tasks, whereas MI attacks on text data generally target lan- guage models for generation or representation learning. Corre- spondingly, the varying learning tasks of victim models result in disparate attack strategies for different data modalities. III. MI A TTACKS ON TRADITIONAL ML S CENARIOS This section provides a brief review of early MI attacks and defenses on traditional ML models with simple-structure data. Fredrickson et al. [6] propose the first MI attack against tab- ular data in a case study of personalized warfarin dosing [53]. They simply infer the target attribute xâˆ— t (i.e., genotype) from a linear regression model by utilizing maximum a posterior probability (MAP) estimate as follows: xâˆ— t = arg max xt Î£xâˆˆË†X:xt=xt Î 1â‰¤iâ‰¤mpi(xi), (1) where xi is the i-th attribute of the sample x, pi(Â·) denotes the marginal distribution of xi, and m is the number of attributes. Given the access to the k insensitive attributes z = ( x1, . . . , xk) and target label y, the candidates set Ë†X includes samples that match these non-private attributes and are predicted into y, i.e., Ë†X = {Ë† x: Ë† xK = z, f(Ë† x) = y}, where K denotes the corresponding indices. To mitigate the stated privacy threat, [54] proposes the first MI defense based on differential privacy (DP) [55]. They leverage an adaptive strategy by adding more noise for sensitive attributes and less noise for non-sensitive attributes to improve MI robustness and retain the utility of target models. Subsequently, more in- depth MI attack methods emerge. [11] explores attacks on4 decision trees and facial recognition models. They develop novel estimators for decision trees trained on tabular data while applying gradient descent directly on pixels of grayscale images when attacking shallow facial recognition models. Be- sides, [11] presents the first definition of black-box and white- box MI attacks according to the knowledge about the target model. To build a universal MI framework, [56] summarizes previous methods and proposes a more rigorous formulation of MI attacks. Considering attacks in the Federated Learning (FL) system, [57] introduces a simple GAN model to enhance the reconstruction. Regarding the shared global model in FL as the discriminator, the attacker trains a generator to learn the same distribution as the target category. Once the training finishes, the generator can directly produce samples similar to private training data. Apart from image recovery, MI attacks were first extended to text modality by [12], which explores attacks from the lens of a service provider for programming code. In the white-box scenario, they manipulate the training codes to encode the userâ€™s private information into model parameters using a proposed encoding strategy. After the model is released, the privacy can be extracted by carefully analyzing the model parameters. For black-box attacks, model parameters become inaccessible. Hence, they synthesize extra malicious training data whose labels carry the encoded private information. The attacker queries the released model with these synthetic data and then analyzes the returned labels to infer the hidden private information. To relax the assumption about access to some non-sensitive attributes [6], [58] designs a general black-box MI framework against the linear regres- sion model that does not require any knowledge about target samples. By considering the data poisoning mechanism, they divide the attack into three stages: Setup, Poisoning, and Model Inversion. In the initial Setup stage, the attacker estimates the regression coefficients and necessary hyperparameters of the victim model. During the Poisoning, the estimated parameters are utilized to generate poisoned training data that aims to reduce the correlation between non-sensitive attributes and model parameters. Finally, the target model becomes highly correlated with the sensitive attributes and leaks considerable private information in the Model Inversion stage. Inversion Model-Based Image Recovery. This category of MI attack constitutes a series of black-box methods against relatively shallow networks mostly trained on grayscale im- ages. With the assumption that the predicted vectors of private images are known to the attacker, these approaches treat the target model as a fixed encoder and construct an inversion model as the corresponding decoder, which maps these private confidence vectors of the target model back into the recon- structed images. Formally, the inversion model M(Â·) is trained by minimizing the following equation: Mâˆ— = arg min M Exâˆ¼XpubL(M(fÎ¸(x)), x), (2) where L(Â·, Â·) is the distance loss for reconstructed images, Xpub represents the auxiliary public dataset and fÎ¸(Â·) denotes the target model. Once the inversion model is trained, the ad- versary can reconstruct the target private images with the given corresponding prediction vectors. [13] first proposes this inver- sion paradigm in scenarios where only corrupted predictions of private images are available (i.e., only partial confidence scores are accessible). Specifically, the attacker collects public data as an auxiliary training dataset for the inversion model. To simulate the corruption in the confidence vectors, the adversary truncates the predictions of public images with top-k selection, which is then fed into the inversion model M(Â·) to perform optimization via Eq. (2). When the training is completed, the target images are reconstructed directly from the given specified prediction through the inversion model. [59] explores the privacy risk in Explainable Artificial Intelligence (XAI) and presents a novel inversion model structure to enhance MI attacks. To better leverage the explanations (e.g., Gradients [60] and LRP [61]) produced by the target model, they design a multi-modal inversion model to receive the processed explanations as a second input. Besides, an additional network (e.g., Flatten and U-Net [62]) is introduced in front of the inversion model to preprocess the explanations. With the rich information contained in the explanations, they first conquer the challenge of reconstructing RGB images in the context of XAI learning scenarios. To improve the inversion model training, [63] generates adversarial samples with the untar- geted attack SimBA [64] as data augmentation to expand the auxiliary training set. In addition to the reconstruction loss in Eq. (2), a novel semantic loss is introduced to ensure the semantic information in the reconstructed samples. A surrogate model is first trained under the guidance of the target model using the knowledge distillation technique. The semantic loss is then computed as the classification loss of the target label provided by the surrogate model. Previous methods usually attach great importance to the confidence of the target class while neglecting the informa- tion from other classesâ€™ confidences [65] that measure the differences between each predicted vector of the same target class. To utilize these differences to strengthen sample-wise reconstruction, they insert a nonlinear amplification layer ahead of the inversion model to enlarge minor entries within the prediction vector, amplifying the differences between pre- dicted vectors with the same class and further promoting the inversion model to achieve better sample-wise recovery. Re- cent work [66] considers the domain divergence issue between the auxiliary and the private dataset. Inspired by the principle of generative adversarial learning, they regard a pre-trained auxiliary classifier as the generator, while jointly adversarially training a domain discriminator to distinguish the generated and the real private confidence vectors. Under the guidance of the domain discriminator, the attacker fine-tunes the generator to produce prediction vectors whose domain aligns with the private ones, Correspondingly, the inversion model M(Â·) is trained simultaneously with the generated auxiliary prediction vectors as input. Moreover, they also adopt the additional nonlinear amplification layer [65] and achieve the recovery of the RGB images on shallow networks for classification tasks. In addition, they [66] further explore the scenario where attackers have access to private features extracted by the target model. The attacker then removes the nonlinear amplification layer and adopts a specially designed feature alignment model as the generator to utilize the extracted features better.5 Ground Truth  PPA [15]  PLGMI [35]  GMI+LOM MA [67] KDMI+LO MMA [67] IF-GMI [68]  BREPMI [32]  RLBMI [69]  C2FMI [34]  LOKT [70] Fig. 3: Visual comparison of representative SOTA attacks against image classification. It can be observed that these attacks (e.g., IF-GMI [68]) cause serious privacy leakage about the semantic features in human faces. Note that the last four columns display black-box attacks. â€¦ Latent Vectors Generated Images Generator Target Model â„’????? â„’???????? Discriminator z ~ ?(0, ?) Sample Optimize â„’????? â€¦ Target Labels Fig. 4: Threat model of generative model inversion attacks against image classification tasks. IV. M ALICIOUS MI ATTACKS ON DNN S This section provides detailed introductions to current main- stream model inversion attacks on DNNs. A. Attacks against Image Classification Attacks on image classifiers, particularly those for human face recognition, have become a significant focus of model in- version attacks. Figure 3 shows the visual results reconstructed by several representative state-of-the-art (SOTA) attacks. By reconstructing training data for a given target class, an attacker can access an individualâ€™s private facial features, which poses a serious privacy breach threat. We begin by elucidating the general threat model of MI on DNNs. Then, to better clarify and compare these MI approaches, we disassemble the optimization process and summarize the major characteristics of current attacks in Table I, based on which we systematically analyze the attack paradigm across four perspectives: Generative Models , Data Initialization, Attack Process and Additional Generic Tech- niques. Finally, we review multiple commonly used metrics to evaluate the effectiveness of MI attacks. 1) Threat Model: Due to the remarkable improvement fa- cilitated by the generative priors, generative models have been incorporated into the basic paradigm for subsequent research of MI on DNNs [10], [14], [35]. Figure 4 outlines the threat model for a typical GAN-based model inversion attack. Given the target image classifier fÎ¸(Â·) parameterized with weights Î¸ and the trained generator G(Â·), attackers attempt to recover private images xâˆ— from the GANâ€™s latent vectors z that are initialized by Gaussian distribution: zâˆ— = arg min z Liden(fÎ¸(G(z)), c) + Î»Lprior(z; G), (3) where c is the target class, Liden(Â·, Â·) typically denotes the identity loss (i.e., the classification loss of the target label), Î» is a hyperparameter, and Lprior(Â·) is the prior knowledge regularization, e.g., the discriminatorâ€™s classification loss used to improve the reality of G(z). Intuitively, the first term attempts to drive the generated images G(z) to be classified as the desired category by the victim classifier, which essentially endeavors to reduce the generationsâ€™ deviation from the private data based on the perspective of the target model fÎ¸(Â·). The latter term serves as image priors to enhance the quality of the reconstructed samples. Before launching an attack, the adversary needs to acquire a generative model G(Â·) as prior information about the target images. This can be achieved through either manual training of a specialized generator [14], [35] or direct utilization of a pre-trained generator [15]. Subse- quently, the generator is leveraged to generate dummy images6 Method Generative Models Data Initialization Optimization Extra Features Resolution Preprocess Optim-algorithm Loss-fn Search Space White-Box Attacks GMI [14] WGAN-GP 64Ã—64 % Momentum SGD LCE+Î»LDis Z space - KEDMI [71] Inversion-specific GANâ€  64Ã—64 % Adam LCE+Î»LDis Zâˆ— space Distributional recovery VMI [10] DCGAN StyleGAN2-Ada 64Ã—64 128Ã—128 % Adam LCE+Î»LKL Z space Variational inference Î±-GAN-MI [72] Î±-GAN 64Ã—64 \" - LCE Z space - SecretGen [17] WGAN-GP 64Ã—64 \" Momentum SGD LCE+Î»LDis Z space - Mirror [18] StyleGAN 224Ã—224 \" Adam LCE P space pvector Clipping PPA [15] BigGAN StyleGAN2-Ada 224Ã—224 \" Adam LPoincarÂ´e Wspace Results selection PLGMI [35] Conditional GAN 64Ã—64 % Adam LMM Z space Pseudo labels guidance LOMMA [67] âˆ¼ âˆ¼ âˆ¼ âˆ¼ L Logit+Î»LFeature âˆ¼ Model augmentation DMMIA [73] StyleGAN2-Ada 224Ã—224 % Adam LCE+LMemory - Dynamic memory bank IF-GMI [68] StyleGAN2-Ada 224Ã—224 \" Adam LPoincarÂ´e Wspace Intermediate feature optimization Black-Box Attacks SecretGen [17] WGAN-GP 64Ã—64 \" Momentum SGD LDis Z space - BREPMI [32] WGAN-GP 64Ã—64 \" Gradient estimation - Z space - Mirror [18] StyleGAN 224Ã—224 \" Genetic algorithm - P space pvector Clipping BMI-S [33] StyleGAN2-Ada 224Ã—224 % Gradient estimation - Wspace Style vectors selection C2FMI [34] StyleGAN2 128Ã—128 \" Genetic algorithm - Wspace Initial coarse optimization RLBMI [69] WGAN-GP 64Ã—64 % SAC - Z space Reinforcement learning LOKT [70] T-ACGAN 128Ã—128 % Adam LMaxâˆ’margin Z space Invert surrogate models DiffusionMI [19] Conditional Diffusion64Ã—64 % Optimization-free - - Results selection P2I-MI [36] StyleGAN2-Ada 64Ã—64 % Optimization-free - - Prediction alignment âˆ—The latent vectors are sampled from a learnable Gaussian Distribution. â€ This customized GAN structure is first proposed by KEDMI [71]. âˆ¼LOMMA [67] is a plug-and-play technique that can seamlessly combine with existing generative model inversion attacks. TABLE I: Summary of existing MI attacks for visual tasks. Note that SecretGen and Mirror consider both white-box and black-box scenarios. that are fed into the target classifier. During the recovery, the attacker performs iterative updates of the latent vectors z by minimizing the loss function in Eq. (3). Once obtain the optimal zâˆ—, the reconstructed images that closely align with the private images can be generated by xâˆ— = G(zâˆ—). 2) Generative Models: As introduced in Sec. III, most pre- vious studies on traditional ML scenarios directly optimize the random noises and neglect the benefits of powerful generative priors, resulting in poor reconstruction outcomes when faced with high-dimensional images [14]. To overcome this diffi- culty, GMI [14] proposes to employ powerful GAN models to guide image generation and achieve outstanding improvements on deeper CNNs with RGB images. They manually train a GAN model on a large public dataset Xpub, which contains images with similar structures to the private data. Subsequent studies follow this GAN training paradigm [32], [69] and design special loss functions [14], [17] for effective GAN learning or utilize various GAN architectures tailored to the MI scenario, as summarized in Table I. Moreover, some methods further exploit information from victim models to strengthen the generatorâ€™s capability. A widely used technique involves feeding the GANâ€™s training images into the victim classifier to obtain pseudo labels, which then serve as class-condition inputs to the generator [35], [70] and the discriminator [35], [70], [71] in the generative adversarial training. However, constrained by the limited data volume and poor quality of auxiliary datasets for GAN training, the trained generator is only capable of recovering low-resolution images at 64 Ã— 64. Additionally, performing generative adversarial learning incurs expensive computational overheads and con- sumes a significant amount of time. One viable resolution is to utilize publicly available pre-trained GANs that have been well-trained on large image datasets [10], [18], [34], [36]. Compared with GANs trained manually with low-quality auxiliary datasets, these pre-trained generators typically have more intricate structures and hold richer semantic information, enabling higher dimensional reconstruction up to 224 Ã— 224 resolution [15], [33], [68], [73]. Furthermore, the recent work DiffusionMI [19] substitutes GANs with the prevalent condi- tional diffusion model [74]. Specifically, they leverage pseudo- labels classified by the target model as input conditions to train a denoising model with better guidance. This approach7 demonstrates notable improvements in generation quality over former GAN-based methods, particularly in terms of human perception distance (e.g., LPIPS [75]). 3) Data Initialization: a) Resolution: We summarize the image resolutions em- ployed by different algorithms in Table I. Given that more image pixels imply more parameters to optimize, the difficulty of MI attacks is highly determined by the resolution of recovered images. Early MI methods resort to reconstruct- ing low-resolution images for improved performance, e.g., GMI [14] and KEDMI [71]. To enable attacks on higher- resolution images, subsequent studies introduce stronger pre- trained GAN models (e.g., StyleGAN [76]) and manage to recover images with a resolution of 224 Ã— 224. b) Preprocess: While [14], [71] randomly sample ini- tial latent vectors from certain data distributions, [15], [17] highlight that a batch of appropriately initialized latent vec- tors tends to yield better reconstruction results. A common approach is to use the classification confidence to assist the selection process [17], [18]. To obtain those latent vectors, the attacker first samples a large number of latent vectors and gen- erates the corresponding images. These images then undergo a series of data augmentation transformations and are fed into the target classifier to obtain their corresponding confidence scores. The latent vectors with the highest confidence scores as the target class are therefore selected for the later optimization process [15], [68]. C2FMI [34] further proposes a coarse inversion strategy that additionally trains an inverse network M(Â·) to improve the initialization process. The attacker first feeds the images x into the target model fÎ¸(Â·) and a pre- trained feature extractor Î¾(Â·) to acquire predicted confidences and extracted features respectively. The obtained confidence- feature pairs are leveraged to train the inverse network M(Â·) that maps the confidence scores back to the feature space of Î¾(Â·), which then serves as a surrogate model for attackers to perform white-box MI attacks toward the target label. Finally, these optimized latent vectors that can synthesize images close to the target category are subsequently used as the initial latent vectors. [72] conducts this process from a different perspective, which initializes the latent vectors based on real facial features with the image encoder of the Î±-GAN [77]. In practice, the attacker averages several randomly sampled public images and then inputs the averaged image into the Î±-GAN [77]â€™s image encoder to obtain the initial latent vector. To obtain extra knowledge gains, GMI [14] and SecretGen [17] additionally consider the scenario where attackers can access the blurred or masked versions of private images and fully leverage them as auxiliary information to enhance the initialization of latent vectors. Specifically, the extracted features of corrupted images are concatenated with randomly initialized noises as input latent vectors to the generator, enhancing their underlying semantic information. 4) Attack Process: a) White-Box Attacks: In the white-box scenario, the attackers have full access to the weights and outputs of the target model, where they primarily conduct the inversion by employing gradient optimization with Momentum SGD or Adam optimizer to minimize the loss function in Eq. (3). This involves computing the identity loss Liden and the prior loss Lprior. As summarized in Table I, the majority of white- box methods adopt cross-entropy (CE) loss as the metric to compute the identity loss. However, PPA [15] proves that the cross-entropy loss is susceptible to gradient vanishing issues, which makes it challenging to obtain the optimal latent vectors during the reconstruction. To solve this problem, PPA moves the optimization to hyperbolic spaces with constant negative curvature by employing the Poincar Â´e loss function which can guarantee sufficient gradient information to the iteration process. Specifically, the l1 normalized output confidences Ë†yâ€² and the target one-hot vector y are viewed as two points in a Poincar Â´e ball, and the loss is formulated as the hyperbolic distance between two points: LPoincarÂ´e = arccosh(1 + 2||Ë†yâ€² âˆ’ y||2 2 (1 âˆ’ ||Ë†yâ€²||2 2)(1 âˆ’ ||y||2 2)). (4) where PPA adopts an edited version of the typical Poincar Â´e loss by replacing 1 with 0.9999 for better performance. Another effective solution proposed by PLGMI [35] is to substitute CE loss with the max-margin (MM) loss function LMM , which maximizes the output logit of the target class while minimizing that of an additional class with the largest logit: LMM = âˆ’lc(x) + max jÌ¸=c lj(x), (5) where c is the target class and li represents the output logit of the class i. The resultant derivative of max-margin loss concerning the logits contains constant values that can help avoid the gradient vanishing problem. Moreover, PLGMI points out that the max-margin loss facilitates searching for samples that closely resemble the target class while distinctly different from other classes, which aligns with the basic goal of MI attacks. Nevertheless, LOMMA [67] rethinks the optimization objective and proposes a different viewpoint. They emphasize that the fundamental goal of MI attacks is to reconstruct images that highly resemble those in the target class, rather than deviating from non-target classes. Since CE loss inherently combines both objectives, they suggest bypassing the softmax function in the cross-entropy loss and directly maximizing the logit of the target class. As mentioned in Section III, various regularization terms are introduced as prior knowledge to ensure the quality and fidelity of the generated images. One representative attempt is the use of the discrimination classification loss LDis, i.e., the realism penalization from GANâ€™s discriminator [14], [17]. However, [15], [18] note that the GAN is trained on a public dataset and thus the LDis might impair the inversion performance by matching the generated images with the distributions of the public data rather than the private ones. Consequently, subsequent MI methods no longer employ this term to avoid the distribution shift. Apart from the constraint on the con- fidence scores, LOMMA [67] analyzes that successful MI attacks are expected to require the feature similarity between the reconstructed and private images given by the target model. Therefore, an LFeature term is utilized to minimize the distance between the penultimate layer representations of8 reconstructed samples and public images, further boosting the performance of MI attacks. Unlike the above methods, DMMIA [73] focuses on the issue in the GAN model itself, i.e., the catastrophic forgetting problem where the generated images from early reconstruction contain more diverse characteristics, but some disappear as the optimization goes further, decreasing the diversity of inverted images. To solve this issue, DMMIA uses the generated sam- ples to build two types of memory banks, the learnable intra- class multicentric representation (IMR) and the non-parametric inter-class discriminative representation (IDR). IMR learns multiple distinctive features of intra-class images to represent the target class while IDR stores the historical knowledge into prototypes for each class. Based on the two dynamically maintained memory banks, they propose a novel regularizer LMemory = Î»1Limr + Î»2Lidr, where Limr leverages IMR to prevent overfitting to specific image features and Lidr encouraging the reconstruction of images with more class- distinguishable characteristics. Instead of instance-level data reconstruction, KEDMI [71] aims to directly recover the private data distribution for a given label with a learnable Gaussian distribution N(Âµ, Ïƒ2). They adopt the reparameterization trick to make the loss differentiable and optimize the learnable parameters Âµ and Ïƒ. Once finished modeling the private data distribution, attackers can directly sample z from the learned distribution to generate sensitive images. VMI [10] also discusses the distribution- level recovery of the training data. To establish theoretical understanding, VMI formulates the MI attack as a variational inference that attempts to estimate the target posterior dis- tribution p(x|y) with a variational distribution q(x). Based on the theoretical analysis, an extra KL-divergence regularizer is introduced to constrain the distance between a learnable distribution q(z) estimated by deep flow models [78] and the latent distribution pAUX (z) of public auxiliary images in the GANâ€™s latent space. By optimizing the flow model using the proposed loss function, the attacker gradually optimizes the latent distribution q(z) to reach the latent area of target data, further estimating the target data distribution q(x) through the GAN model G(Â·), i.e., q(z) G âˆ’â†’q(x). b) Black-Box Attacks: In contrast to white-box settings, this type of attack has no access to any information within the target model. Consequently, the gradient information is unavailable for performing the back-propagation operation. SecretGen [17] proposes a straightforward solution to the problem of gradient inaccessibility. The attacker first samples numerous latent vectors from random noise and selects the ones that produce images predicted as the correct labels. These vectors are then optimized exclusively with the discrimina- tor loss to improve the reconstruction quality. RLBMI [69] introduces a reinforcement learning-based model inversion attack that leverages confidence scores to provide rewards and optimizes the agent model with the Soft Actor-Critic (SAC) [79] algorithm. This algorithm employs an actor-critic architecture where both states and actions are represented as the latent vectors of the generator, and the next state is computed as a weighted combination of the current state and the action. In this actor-critic framework, the actor network generates an action based on the current state, while the critic network estimates the corresponding reward. To achieve the goal of MI, RLBMI designs the reward with three components: state reward, action reward, and distinguishability reward. The state and action rewards are defined as the confidence scores of the target label with respect to the next state and the action respectively, and distinguishability reward measures the disparity between the confidence score of the target class and the highest confidence score of other classes. By updating the actor-critic networks, the actor network learns to produce actions that can yield higher rewards, significantly enhancing MI attack effectiveness. Label-only scenarios. BREPMI [32] proposes a boundary- repelling strategy to tackle label-only scenarios where only the predicted hard labels are available. Their intuition is that the farther image from the class decision boundary tends to be more representative of the class. Consequently, BREPMI utilizes zero-order optimization to urge the latent vector to gradually move away from the decision boundary. In light that images that are not predicted as the target class represent incorrect optimization directions, the attacker estimates the gradients Ë†gz of latent vectors z opposite to these misclassified images by randomly sampling N unit vectors u and calculat- ing the loss with sampled points on a sphere at radius r: Ë†gz = 1 N NX i=1 Î¦c(z + rui)ui, (6) where Î¦c(Â·) denotes a function that equals zero if the gen- erated image is classified as the target class c, otherwise âˆ’1. The attacker calculates the gradient as an average over those misclassified points and optimizes the latent vector z using gradient ascent to move them in the direction opposite to this unintended average. During the optimization, BREPMI increases the value of r when all sampled points are classified into the target label to better estimate the gradients. A follow- ing study BMI-S [33] adopts a similar pipeline with BREPMI [32] but assumes the soft labels are available. The research replaces the Î¦c(Â·) with CE loss in Eq. 6 and fixes the value of r during the optimization. LOKT [70] also considers label- only scenarios and introduces a Target model-assisted ACGAN (T-ACGAN). During the T-ACGAN training, LOKT employs the target model to tag public auxiliary images and synthesized images generated by generator G with pseudo labels as class supervision to improve the training of ACGANâ€™s discriminator D(Â·) and classification head C(Â·) (where D(Â·) is used to penalize the image realism and C â—¦ D(Â·) is an auxiliary classifier to provide identity loss for better GAN learning). With the training of ACGAN finished, the attacker could directly use C â—¦ D(Â·) as the surrogate model or additionally train multiple surrogate models with sufficient image-label pairs generated by G(Â·) and labeled by the victim model fÎ¸(Â·). Finally, the attacker executes SOTA white-box MI methods on the target model replaced by the obtained surrogate models, which achieves satisfying attack results. Gradient-free optimizer. Orthogonal to the above methods, Mirror [18] and C2FMI [34] explore gradient-free optimiza- tion techniques, which utilize genetic algorithms instead of9 gradients for optimization. Specifically, the population of k candidates in the genetic algorithm represents the latent vectors of GANs, and the fitness score is defined as the confidence score for the target label. To obtain an adequate initial population, Mirror [18] employs a simple initial selec- tion strategy while C2FMI [34] proposes a more complicated coarse optimization method as detailed in Section IV-A3b. In each iteration of the genetic algorithm, a subset of candidates is selected as parents and undergoes crossover to generate new candidates. Additionally, some candidates are mutated either by adding random noise (Mirror [18]) or by crossing over with the candidate that has the highest score (C2FMI [34]) to create more high-quality candidates. The top k candidates with the highest scores are retained for the next iteration. This process repeats over multiple iterations and continually refines the candidates toward achieving higher confidence scores for the target label. Finally, the reconstructions are generated from the candidates with the highest scores via the GAN model. P2I-MI [36] claims that optimization-based methods suffer from high cost and low efficiency. Thus, P2I-MI proposes an optimization-free training-based strategy to overcome this issue. The attacker trains a prediction alignment encoder (PAE) to map the output prediction to the style vector w in the W space of StyleGAN2-Ada. During the attack stage, the research proposes an ensemble scheme that samples a batch of public images and gets their corresponding predicted confidences to be mapped into style vectors with the learned PAE. The final style vector w is calculated as a weighted average of these style vectors, which serves as the condition input for StyleGAN2-Ada to generate the recovered image. In addition, the prevalent denoising diffusion models are also introduced to MI attacks. DiffusionMI [19] uses an auxiliary dataset labeled with pseudo labels to train a class-conditional diffusion model, which is directly used to generate target images without any further optimization during the attack phase. c) Search Space: Most previous studies focus on finding the optimal vectors in the GANâ€™s latent space Z. However, [18] shows that as the resolution increases, this optimization becomes under-constrained due to the large space sparsity. Feature entanglement in Z space is another challenge that further impedes the optimization process since images sharing similar features may not correspond to analogous latent vec- tors. StyleGANs [76] transform the latent space into the well- disentangled W space through a mapping network Gmapping : Z â†’ W. Therefore, searching the W space alleviates the aforementioned issues, which has been adopted by a series of following studies [15], [34]. Previous methods perform the attack within a fixed search space. In contrast, IF-GMI [68] innovatively operates opti- mization across multiple feature spaces to fully exploit the hidden information in GANâ€™s intermediate layers. To perform the intermediate layer optimization, a StyleGAN2-Ada G(Â·) is divided into L + 1 blocks as follows: G(Â·) = GL+1 â—¦ GL â—¦ Â·Â·Â· â—¦G2 â—¦ G1(Â·). (7) During the attack process, the attacker first optimizes the inputs of G1(Â·) and continues the optimization layer by layer until reaching GL(Â·). Besides, some methods propose to constrain the search space to avoid unrealistic image gen- eration. Mirror [18] suggests a P-space clipping strategy to constrain the final w vectors within the target distribution. The P space is defined as the feature space located before the final LeakyReLU function in the mapping network. Numerous vectors are first sampled from Z-space to compute the mean Âµ and variance Ïƒ of the activation values in P-space. Given an optimized w in W-space, attackers first obtain p by projecting w to P-space. Then p is clipped within the range [Âµâˆ’Ïƒ, Âµ+Ïƒ] and projected back to the W-space. This operation preserves the style control capabilities of the W space while ensuring these controls are within the desired distribution. Moreover, IF-GMI [68] employs an l1 ball to limit the optimization space of each feature space during the intermediate search. 5) Additional Generic Techniques: Various innovative tech- niques have been explored and incorporated into numerous studies. Next, we supplement a detailed review of several generic mechanisms. a) Pseudo Label Guidance: As introduced before, pseudo labels have been utilized to guide the training of the generator [19], [35], [70], the discriminator [35], [70], [71] and surrogate models [67], [70]. Specifically, the target model is leveraged to reclassify unlabeled images (e.g. the images generated from the generator and public images) with labels from the private dataset. Since the target model is trained on the private dataset, the pseudo labels contribute to exploiting extra information within the private training data, thus enhanc- ing the capabilities of the generator or discriminator trained on these auxiliary labeled images. b) Augmentation: Many studies have integrated various augmentation techniques into the MI workflow to improve the attack effects. [17] employ sequential cutout as a data augmentation for images to improve the initial latent vector se- lection. [15], [19] process the reconstructed images with image transformation and select results with higher confidence from the output of the target model. Furthermore, [35] performs random augmentations on the generated images before feeding them to the target model to provide more stable convergence to realistic images during the GAN training. In addition to traditional data augmentations, [67] presents a novel approach called model augmentation. This involves training several auxiliary models from the target model using model distillation techniques. During the MI process, the adversary utilizes an ensemble of the original target model and the trained auxiliary models to calculate the loss function. By augmenting with surrogate models, this strategy mitigates overfitting to the target model and encourages the recovered images to capture more distinctive features of the private data. c) Results Selection: [15] notes that DNNs often exhibit overconfidence in their predictions, leading to low transferabil- ity in attack results. More concretely, while some reconstructed images are labeled with high confidence by the target model, they receive low scores when evaluated by another model. To overcome this challenge, [15], [19] apply data augmentations to the generated images before classifying them with the target classifier. By selecting results with the highest confidence score after the augmentation, this approach achieves recovery with enhanced attack accuracy and superior transferability.10 6) Evaluation Metrics: Assessing the degree of privacy leakage raised by MI attacks is another pivotal open issue. Generally, researchers evaluate the performance based on the similarity between the reconstructed images and target class images. In addition to the common distance measurement (e.g., PSNR [80], LPIPS [75] or FID [81]), novel metrics tailored for MI evaluation are introduced and widely adopted. We detailedly summarize these metrics as follows. â€¢ Accuracy. This metric is calculated as the classification accuracy of an auxiliary evaluation classifier (trained on the same private dataset as the victim model but with a different architecture) when applied to these reconstructed samples. It serves as a pivotal criterion for how well the generated samples resemble the target class. The higher accuracy the synthesized samples achieve on the evaluation model, the more private information in the training dataset is exposed [14]. â€¢ Feature Distance. The feature distance is defined as the L2 distance between the feature vector of the recon- structed image and the feature centroid of the private samples specific to the target class. Note that the feature vector is the output from the penultimate layer of the evaluation classifier. â€¢ K-nearest neighbor (KNN) distance. The KNN dis- tance represents the minimum distance from the recon- structed image to the images within the target class [14]. In practical terms, the K usually equals 1, i.e., the nearest training image within the target category to the synthesized sample is first identified, and their feature distance is computed as the KNN distance metric. â€¢ Sample Diversity. [10] proposes this metric that focuses on the intra-class diversity of the reconstructed images. It first computes the improved Precision-Recall [82] and Density-Coverage [83] scores regarding the classification results of the reconstructed images on a given evaluation model. The average of the two quantities is the sample diversity, whose higher values indicate greater intra-class diversity of the synthesized samples. B. Attacks against Image Generation While the majority of existing algorithms are designed to attack image classification tasks, recent research also explores the potential for inverting the generative model to reconstruct their private training data. Previous research [84] demonstrates that GAN models [16], [85] are susceptible to model inversion attacks. By repeatedly sampling images from the generatorâ€™s output distribution, itâ€™s highly likely to generate raw samples from the GANâ€™s training set. Compared with GAN models, the denoising diffusion model [86] is able to produce images of higher quality and realism. However, [37] indicates that they are also more susceptible to being inverted to reconstruct private data, which leaks nearly more than twice the training data leaked by GAN models. Specifically, they propose a generation and filtering mechanism and target a range of prevalent text- conditioned diffusion models, such as Stable Diffusion [87] and Imagen [88]. The extraction process consists of two steps: (1) repeatedly query the diffusion model with the selected text prompt to obtain a large number of images, and (2) construct a graph using these generations by establishing an edge between the generated images if they are considered to be sufficiently similar based on a pre-defined distance metric. Ultimately, the largest clique in the graph which comprises at least 10 nodes is identified as a memorized image. In essence, this serious data leakage issue is a production of diffusionâ€™s significant data memorization phenomenon, which allows them to regenerate individual raw training samples under certain guidance. A subsequent study [89] further examines the influencing factors on data memorization and confirms that the condi- tioning input of diffusion models greatly increases the data memorization and thus renders them more vulnerable to MI at- tacks. Building upon this line, SIDE [38] delves into the more challenging attacks on unconditional diffusion models. To establish a universal theoretical analysis, SIDE first proposes a novel and effective memorization metric to provide theoretical support for enhanced attacks, which also corroborates that the input conditions can enhance the data memorization in diffusion models and further result in more information leak- age. To address the difficulty of the missing input conditions in unconditional diffusion models, they propose an effective solution that additionally trains a time-dependent classifier and uses its classification information as surrogate conditions to guide the denoising process for better private data generation. V. D ATA MODALITIES While existing model inversion attacks on DNN primarily focus on image recovery, recent progress has extended the attacks to encompass text and graph data, which also garners significant attention and concerns. A. Text Data DNN models that process natural-language text also suffer from MI attacks. Due to the language modelsâ€™ unintended memorization of training data [20], an adversary can invert the victim models and incur serious privacy leakage. This has been amplified by the thrives of LLMs as these models are trained on massive text corpora that often contain considerable privacy-sensitive information. 1) Embedding Optimization: These attacks mainly take advantage of the full access to the victim model by formulating the reconstruction as an optimization process that utilizes the back-propagated gradients. [31] attacks the LSTM model fÎ¸(Â·) by conducting a discrete optimization, where each token xi of the sentence s is represented as a logit vector zi to get the embedding Es of the dummy sentence s. Given the target label y, the vectors z1, . . . , zn are repeatedly optimized with the gradients computed from the cross-entropy loss L(fÎ¸(Es), y). Subsequently, [41] adopts the paradigm of prompt learning that freezes the parameter of the victim GPT-Neo and tunes the pre-sentence soft prompt embedding S by forcing the model to output the target suffix b for a given prefix a. The learned prompt S elicits the memorization of the target language model and induces it to respond with private training data. Additionally, it introduces a smoothing regularizer to make the loss distribution of the suffix sequence smoother.11 2) Inverse mapping: The rapid development of general- purpose language models (e.g., BERT [90]) makes them widely used feature extractors to encode sentences into dense embeddings. In privacy-sensitive learning with service providers, users may expect that it would be a safe way to submit text features encoded using the language model from service providers. However, research has demonstrated that the submitted sentence embeddings can also incur the leakage of usersâ€™ private data. To be specific, [42] constructs two types of attacks, namely pattern reconstruct attacks and keywords inference attacks. It collects an external corpus Dext = {si}N i=1 to extract sensitive information {T(si)}N i=1 as labels and query the language model to obtain the sentence embedding {fÎ¸(si)}N i=1. An inversion model consisting of a linear SVM and a 3-layer MLP with 80 hidden units is trained on the (fÎ¸(si), T(si)) pairs to invert the sentence embedding to the sensitive information for attack purpose. However, [43] claims that the direct inversion from output to input with only one step is a highly non-convex optimization issue and limits the recovery accuracy, especially as the victim model grows deeper. Therefore, it divides the inversion into two stages: (1) learn a mapping function M(Â·) to map the higher layer embedding fÎ¸(si) to a shallow layer features (rather than the input words), and (2) infer the private words by matching their corresponding embedding with the obtained shallow layer rep- resentations M(fÎ¸(si)). Nevertheless, one common drawback of these two attacks is they only reconstruct orderless sets of words that lack meaningful semantic information. To address this problem, [39] reformulates the attack as a generation task, which trains a generative attacker model, i.e. GPT-2, to iteratively map the sentence embedding fÎ¸(s) with previous contexts x1, . . . , xiâˆ’1 into the private word xi. The capability of LLMs can be greatly enhanced through well-crafted prompts, which has promoted the continued de- sign of high-quality instructional prompts by service providers. These prompts have also become a valuable asset within their intellectual property rights (IPR). However, malicious users can cause a serious infringement of their IPR by recovering the invisible prompt via the model output [40]. [40] trains an inversion model M(Â·) with an encoder-decoder backbone, to map the modelâ€™s next-token probabilities v back to initial tokens x1:N . During the recovery, the output probability vector of the next token is first unrolled into a sequence of pseudo- embeddings, which are then encoded to provide conditions for the decoder to recover the target prompt. Since LLMs are usually available via API, the output confidence vectors may be inaccessible. Thanks to the typical setting of LLMs that provides the argmax of the confidences and allows users to add a logit bias to adjust the output distribution, the probability of each token can be estimated based on its difference with the most probable word. This is calculated as the smallest logit bias required to make that word most likely. Instead of estimating the confidence distribution, Output2prompt [91] directly uses LLM output sequences as model input to train the inversion model M(Â·). Besides, a sparse encoder is employed to reduce the memory costs of training M(Â·). 3) Token Search: In the black-box scenario, only model outputs are accessible and the gradient back-propagation Carol â–ˆâ–ˆâ–ˆâ–ˆ (name) â–ˆâ–ˆâ–ˆâ–ˆ-3989  (phone) â–ˆâ–ˆâ–ˆâ–ˆ-8537  (fax) â–ˆâ–ˆâ–ˆâ–ˆ251506 (pager) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ-8862 (home fax) carol. â–ˆâ–ˆâ–ˆâ–ˆ. @ â–ˆâ–ˆâ–ˆ.com Malicious Prompts â€”â€”Original Messageâ€”â€” From: Heard, Marie...  Mail To: _________ API LLM Fig. 5: An example of privacy leakage on LLMs. becomes impractical. Researchers choose optimization-free search-based strategies to handle this challenge. Given the sentenceâ€™s prefix tokens a = x1, x2, . . . , xi, attackers attempt to search the most probable pathway for generating the re- maining portion of the sentence, i.e., b = xi+1, xi+2, . . . , xn. Based on the search objective for the subsequent tokens, these methods are further divided into three parts as follows. a) Perplexity-metric Search: Carlini et al. [20] use per- plexity to measures the probability of data sequences. Specifi- cally, given a sequence s = x1 . . . xn and the target generative sequence model fÎ¸ (i.e., LSTM, qRNN), the perplexity PÎ¸(s) can be expressed as: PÎ¸(s) = exp   âˆ’1 n nX i=1 log fÎ¸ (xi|x1, . . . , xiâˆ’1) ! , (8) where PÎ¸(s) quantifies the â€œsurpriseâ€ of a given sequence, and a lower perplexity value indicates a higher probability that the language model has seen the sequence. In this manner, the problem is transformed to find the sequence with minimal per- plexity. However, directly enumerating all possible sequences and computing their perplexity leads to exponentially growing search space. A series of mechanisms have been proposed to improve search efficiency. [20] uses a modification of the Di- jkstra algorithm which efficiently reduces the search range by avoiding visiting unnecessary sequence paths. The subsequent method [44] adopts a greedy-based algorithm that only retains tokens with top-k confidence during each iteration of generat- ing the next token. Moreover, they provide several techniques for improved generated sentence diversity and enhanced attack accuracy. [92] adopts a look-ahead mechanism to improve the quality of generated tokens. Instead of only depending on the next tokenâ€™s probability for the top- k selection, they use the posterior probability of the subsequent sequence to inform the generation of the next token xt. b) Confidence-metric Search: Considering the sentiment classification task, [45] searches for the next token that max- imizes the confidence score of target class y as predicted by a fine-tuned BERT classifier. To counteract the modelâ€™s bias towards high-frequency tokens, a regularizer is introduced to penalize the frequent occurrence of certain tokens. However, this method also faces the challenge of exponentially growing search space. To tackle this, [52] first uses BERT with the original generation head to generate candidate tokens with12 top-k confidence, which are further selected by maximizing the probability of label y with the classification head. c) Difference-metric Search: The pre-train and fine-tune paradigm has gained growing popularity, where models are first trained on a large, public dataset and then fine-tuned on a small, private dataset. Accordingly, several attacks appear to invert the fine-tuning dataset by exploiting the difference between pre-trained model fÎ¸ and fine-tuned model fÎ¸â€². Con- sidering the token sequence s = x1 . . . xn, [93] defines the difference score DSfÎ¸â€² fÎ¸ = Pn i=1 fÎ¸â€²(xi|x<i) âˆ’ fÎ¸(xi|x<i), which measures the difference in the output probabilities between fÎ¸ and fÎ¸â€². Intuitively, a larger difference score value indicates a higher probability that the fine-tuned model fÎ¸â€² has seen sequence s while the original fÎ¸ does not, i.e., s belongs to the fine-tuning dataset. By performing a beam search, they select the next tokens with the highest differential scores and achieve a remarkable attack success rate. Subsequently, [94] defines a novel metric scorer, which additionally considers the distribution-level difference between the target sentence and the current set of reconstructed sentences to more accurately quantify the difference. They first query the fine-tuned GPT-2 model fÎ¸â€² with an empty prompt. Then a complete sentence Sf is generated by iteratively querying fÎ¸â€² until it outputs EOS token. After iteratively querying fÎ¸ with the first half of Sf as the initial prompt, they obtain Sg[l/2 :] generated by the initial pre-trained model. As the larger difference between Sg[l/2 :] and Sf [l/2 :] indicates the greater possibility that Sf comes from the fine-tuning dataset, the attacker calculate the scorer based on Sg[l/2 :] and Sf [l/2 :] to determine whether the generated sentence belongs to the private fine-tuning dataset, thus achieving the aim of data extraction. 4) Malicious Prompt Design: The conversational LLMs have demonstrated powerful capabilities and great potential. However, several studies have shown that some elaborately designed prompts can invert the model to output sensitive training data. As Figure 5 depicts, feeding malicious prompts into language models such as GPT-Neo can incur serious privacy information leakage [46] in both zero-shot and few- shot settings (i.e., providing several demonstrations as prior knowledge to guide LLMâ€™s predictions). More surprisingly, [23] suggests that data can also be extracted by simply asking ChatGPT to repeat a word multiple times. One possible reason is that this instruction causes the model to â€œescapeâ€ its aligning training and revert to its original language modeling objective. B. Graph Data Graph data [95]â€“[99] illustrates the linkage situation be- tween any pair of nodes and indicates how they interact with each other. Correspondingly, Graph Neural Networks (GNNs) [100]â€“[104] exhibit remarkable efficacy in processing complex graph-structured data and have achieved superior performance in non-Euclidean domains. As shown in Figure 6, MI attacks on this modality leverage open access to GNNs to reconstruct the topology of the private graph data. Given the target GNN fÎ¸(Â·) pre-trained on the private graph G, the adversary aims to infer the adjacency matrix Ë†A, which is then converted into graph Ë†G to reconstruct the private training dataset. 1) Adjacency Optimization: These attacks reconstruct the private graph by directly optimizing the adjacency matrix. Since back-propagated gradients are needed for updating, they generally require white-box settings. Denote the prediction vector or the embeddings output by the target GNN model as H, i.e., H = fÎ¸(G). An adjacency matrix Ë†A0 is first ini- tialized to obtain the dummy graph Ë†G0 and the dummy output Ë†H0 = fÎ¸( Ë†G0). By minimizing the distance Lrec between Ë†H0 and H with gradient descent, an attacker iteratively updates Ë†A to find the optimal solution: Ë†At+1 = Ë†At âˆ’ Î·tâˆ‡ Ë†At Lrec, (9) where t is the number of iteration, Ë†At is the adjusted adjacency matrix after each iteration, and Î·t is the corresponding learning rate. Based on this, [21] proposes GraphMI that uses a pro- jected gradient module to tackle the discreteness of graph data while introducing the feature smoothness term Ls and the F- norm term || Ë†A||F for the regularization of feature smoothness and sparsity. RL-GraphMI [105] extends GraphMI to the more challenging hard-label black-box settings by adopting gradient estimation and reinforcement learning methods [106]â€“[110]. From the perspective of information theory, [47] proposes to regard fÎ¸(Â·) as a Markov chain and iteratively optimizes Ë†A via a flexible chain approximation, which can further achieve excellent reconstruction results. Specifically, they adapt the original iterative optimization problem of (9) into a chain- based formulation as: Ë†Aâˆ— = arg max Ë†A Î»pI(HA; Hi Ë†A) | {z } propagation approximation âˆ’Î»cS( Ë†A)| {z } complexity + Î»oI(YA; Y Ë†A) + Î»sI(Y ; Y Ë†A)| {z } outputs approximation , (10) where I(Â·; Â·) is the mutual information, S(Â·) is the information entropy, HA denotes the embeddings of the original adjacency matrix A, Hi Ë†A denotes the representations of the dummy adjacency matrix Ë†A output by the i-th layer ( 1 â‰¤ i â‰¤ L) from the L-layer GNN target model fÎ¸(Â·), YA denotes the classification outputs transformed from HA by the downstream linear layer with activation, Y Ë†A denotes the classification outputs transformed from HL Ë†A by the downstream linear layer with activation, Y denotes the labels for the classification task, and Î»p, Î»c, Î»o, Î»s are the weight factors. By optimizing Ë†A based on (10), a series of intermediate representations are also taken into consideration in addition to the final model output. Thus, the attackers can maximize the approximation of encoding and decoding processes of the target model to enhance the correlation between the ground-truth A and the estimated Ë†A, and minimize the complexity to avoid non- optimal solutions by reducing the graph density. 2) Inverse Mapping: Different from the adjacency opti- mization methods that iteratively optimize Ë†A, these attacks attempt to construct an inverse mapping that reverses the original input-to-output mapping of the target model fÎ¸(Â·) to recover the private graph. Most of them are intended for black-box settings where only the GNN output H is available.13 â„‹ï¿½ï¿½ï¿½ â„‹ï¿½ â„’í‘Ÿï¿½\u0000 ï¿½ï¿½+1 = ï¿½ï¿½ âˆ’ ï¿½ï¿½ï¿½ï¿½ï¿½ â„’í‘Ÿï¿½\u0000 Adjacency Optimization Relationship Reasoning Inverse Mapping Train ï¿½í‘ï¿½\u0000 ï¿½í‘ï¿½\u0000 ï¿½ â„±í‘‘ï¿½\u0000â„‹ â„’í‘Ÿï¿½\u0000 Inference Optimize ï¿½â„±ï¿½ï¿½\u0000 ï¿½ ï¿½ï¿½ â„±í‘‘ï¿½\u0000â„‹í‘–ï¿½ï¿½ ï¿½ ï¿½ ï¿½ ï¿½ ï¿½ ï¿½ or ï¿½ resembles ï¿½ enough? ï¿½ impacts ï¿½ enough? Fig. 6: Different types of MI attacks on graph data. Given the difficulty of reconstruction with limited information, some additional auxiliary knowledge is introduced. Specifi- cally, they first train an autoencoder Fauto(Â·) on an auxiliary graph dataset Gaux by minimizing the reconstruction loss between Ë†Gaux and Gaux, so that mutual conversion between graph data and embeddings can be adequately learned. Once Fauto(Â·) has been fully trained, they disassemble the encoder Fenc(Â·) and decoder Fdec(Â·) from Fauto(Â·), and utilize Fdec(Â·) to map H back to Ë†G, i.e., Ë†G = Fdec(H). [48] leverages such inverse mapping methods and reconstructs Ë†G with high accuracy to quantify privacy leakage in graph embeddings. However, the distribution of latent features Hint within the autoencoder Fauto(Â·) may differ from the original distribution of H. To alleviate this issue, [49] proposes to query fÎ¸(Â·) with Gaux to acquire Haux, i.e., Haux = fÎ¸(Gaux), and fine-tune Fdec(Â·) with the constructed (Haux, Gaux) pair for enhanced reconstruction performance. 3) Relationship Reasoning: These attacks are also designed for black-box scenarios. To successfully conduct the more challenging black-box attacks, they generally require some extra prior knowledge in addition to the GNN output H, such as node attributes in the training data. Unlike the above two types of methods that constantly update Ë†A or disassemble Fdec(Â·) from Fauto(Â·), they reconstruct Ë†G by reasoning about the relationships among pairs of nodes using the introduced auxiliary knowledge. [50] considers three dimensions of auxil- iary knowledge (nodes attributes, partial graph, or an auxiliary dataset), and evaluates 23 = 8 settings based on whether each of the three dimensions is available. [50] presumes that two nodes u and v are linked if they share more similar attributes or predictions. They add an edge between u and v in the reconstructed graph when the distance of feature vectors between them is below a given threshold, and various metrics tailed for each of the 8 auxiliary knowledge settings are designed to calculate such distances. Contrary to the above settings, [51] assumes that the attackers only have access to node attributes in the training data. They suggest that if there is an edge between u and v, the information of u would be propagated to v during training. Therefore, they assume that u and v are linked if changing the feature vector of u can impact the prediction of v to a certain extent. VI. D EFENSES AGAINST MI ATTACKS A rich quantity of defense strategies have been proposed to alleviate MI threats and achieved impressive resistant effects. Basically, these methods focus on improving MI robustness by eliminating the valid information stemming from the pre- trained model that an MI adversary might maliciously utilize. As presented in Figure 7, we primarily divide them into two categories: Model Output Processing and Robust Model Learning. Next, we illustrate these methods in accordance with the proposed taxonomy. A. Model Output Processing Since MI attacks fundamentally exploit the redundant infor- mation contained in the victim modelâ€™s output [25], a viable defense mechanism is to reduce this redundancy by obscuring the model output with various techniques. 1) Additive noise: Intuitively, [24] proposes perturbing the confidence scores with well-designed noise crafted through adversarial learning with an inversion model. The inversion model serves as an MI adversary for the defender, which aims to recover the input data from the confidence vector. Meanwhile, the adversarial noise is updated to resist the inversion model by maximizing the MSE between the original images and images reconstructed from the inversion model. To maintain the main task accuracy, they impose a constraint on the noise that assures the predicted label unchanged. Con- sidering the Differential Privacy (DP) [111]â€“[115] guarantee, [116] designs an approach that divides the score vector into multiple sub-ranges and applies an exponential mechanism to replace and normalize the values in each sub-range. This mechanism assures the target modelâ€™s MI robustness with rigorous theoretical proof of differential privacy. [117] further examines the scenario of split learning, where a global model is split into several components and distributed to disparate clients. During the training, each client calculates the features output from its partial model and then uploads them to the central server for aggregation. Considering the uploaded features also face the threat of being converted into private images by an attack model, [117] suggests a simple yet efficient resolution that incorporates Laplacian noise into the intermediate representations before transmitting them to the computing server, thereby obscuring the potential MI attacker. To protect graph data, [49] also applies Laplacian noise to the model output H. They release a noisy but usable version of output embeddings eH = eH + âˆ†, where âˆ† denotes the Lapla- cian noise. [51] also develops a differentially private GNN model and adds noise to the adjacency matrix to guarantee DP. [118] perturbs feature explanations using a randomized response mechanism and reduces the attack to a random guess. Since applying noise may result in performance degradation, it is essential to choose a decent level of noise and achieve a trade-off between utility and privacy.14 MI Defenses Model Output  Processing Robust Model  Learning Additive noise Purification Detection and  Filtering Secure Training  Mechanism Robust  Fine-tuning Input Data  Preprocessing [24, 49, 51, 54,  116-118] [27, 50, 119- 120] [46] [21, 25-26, 47,  126-129] [130-132] [105, 133] Fig. 7: Taxonomy of defense strategies against MI attacks. 2) Purification: This type of defense aims to purify the confidence scores by reducing the redundant information as- sociated with privacy leakage, which significantly increases the difficulty of MI attacks. In light that a major mechanism of MI attacks is that the outputs are distinguishable for diverse input images, [27] seeks to defend by rendering model predictions less distinguishable and less sensitive to input data changes. Concretely, they propose an autoencoder-based purifier P(Â·) that encourages different input images to be mapped into the same or similar output by reducing the dispersion of the confidence scores from target classifier fÎ¸(Â·). To enhance the purifier P(Â·), they employ an adversarial learning technique, which involves jointly learning an adversarial inversion model M(Â·) via minimizing the inversion loss: Linv = ||M(P(fÎ¸(x))) âˆ’ x||2 2, (11) where x denotes the input images. To train the purifier P(Â·), they additionally introduce the confidence reconstruction loss Lrec and the classification loss Liden as: Lrec = ||P(fÎ¸(x)) âˆ’ fÎ¸(x))||2 2, Liden = CE(P(fÎ¸(x)), arg maxfÎ¸(x)). (12) Finally, the purifierâ€™s optimization objective can be formulated as Ltotal = Lrec + Î»1Liden âˆ’ Î»2Linv, where Î»1 and Î»2 are hyperparameters. [119] follows this pipeline and substitutes the autoencoder with a variational autoencoder with the pre- dicted label as conditioning input, which enhances the output indistinguishability and thus ensures the modelâ€™s MI resiliency. A novel defense framework is to intentionally mislead the attackers into reconstructing public samples [120]. They con- sider the edge computing scenario with similar settings to the split learning [117], where a user uploads the intermediate features Enc(x) encoded from its private image x to the central service provider. To protect the Enc(x) from the threat of an MI attacker, the edge device carefully optimizes these vulnerable features to be closer to the features of the public sample, hence misleading the attacker into recovering insensitive public images. Besides, an Euclidean distance is adopted to restrict the deviation from the original features to further preserve the utility of perturbed features. [50] considers guarding graph data by restricting the GNN model to output only the k largest posterior probabilities, which purifies the confidences to diminish unnecessary infor- mation and thus decreases the potential risk under MI threats. 3) Detection and Filtering: A distinctive attribute of lan- guage models is that the modelâ€™s output contents possess meaningful semantics and can be further analyzed for security concerns. Therefore, [46] proposes an effective defense ap- proach that detects and filters the privacy-sensitive information within the prediction. This includes a detection module to examine whether the output text contains sensitive information and a filtering module to take appropriate measures based on the detection moduleâ€™s action, such as refusing to answer or masking the information for privacy guarantees. B. Robust Model Learning Despite the considerable success of these model output pro- cessing methods, they are constrained to black-box scenarios, where an attacker is merely capable of making queries and obtaining the corresponding output. In contrast, in white-box scenarios, attackers gain complete access to the actual model output before the post-processing. To address this challenge, researchers seek to enhance the modelâ€™s inherent robustness. 1) Secure Training Mechanism: This series of algorithms modifies the model training process to boost the MI robustness. The pioneering research MID [25] is a classic approach to exploring robust training and analyzes that the inversion attack greatly depends on the correlation between the input X and output Ë†Y of the target model fÎ¸, thereby proposing to penalize the mutual information between X and Y during model training. The training objective can be expressed as: min Î¸ E(x,y)âˆ¼pX,Y (x,y)L(fÎ¸(x), y) + Î»I(X, Ë†Y ), (13) where y is the ground truth label, L(Â·, Â·) denotes the loss for the main task, I(Â·, Â·) denotes the mutual information, and Î» is a coefficient balancing privacy and utility. However, it is imprac- ticable to directly compute the I(X, Ë†Y ) due to the difficulty of modeling the joint distribution of model input and output. Inspired by the work on information bottleneck, they substitute it with its upper bound I(X, Z) [121], where Z represents a15 stochastic encoding of the input X at some intermediate layers. Then a variational method [122] is employed to approximate the I(X, Ë†Y ) term. Nonetheless, MID meets the dilemma between data privacy and model utility as the regularizer conflicts with the main task loss. [26] proposes a bilateral dependency optimization (BiDO) to solve this problem. Instead of directly diminishing the correlation between inputs and outputs, BiDO minimizes the dependency d(X, Z) between inputs X and latent rep- resentations Z while maximizing the dependency d(Z, Ë†Y ) between latent representations Z and outputs Ë†Y . The first term limits the propagation of redundant information from inputs to the latent representations, preventing misuse by the adversary. The latter term facilitates the latent layers to learn discriminative representations, ensuring model utility for classification. The overall training loss can be formulated as: LBiDO = LCE ( Ë†Y , Y) + Î»x mX i=1 d(X, Zi) âˆ’ Î»y mX i=1 d(Zi, Ë†Y ), (14) where Î»x and Î»y denote the hyperparameters controlling the intensity of loss items, and Zi, i= 1, . . . , mdenotes the i-th layerâ€™s latent representations specific to the input X. Besides, the study tests both Constrained Covariance (COCO) [123] and Hilbert-Schmidt Independence Criterion (HSIC) [124] as the distance metric d(Â·, Â·) to measure the dependency and finds that HSIC distance turns out to achieve better defense results. Label smoothing [125], initially proposed to improve model generalization or calibration, is investigated by [126] to defend model inversion attacks. Given the initial one-hot label y, the class number C, and the smoothing factor Î±, the smoothed label is defined as: yLS = (1 âˆ’ Î±) Â· y + Î± C . (15) Correspondingly, the training loss function that incorporates the label smoothing technique can be presented as: LLS = (1 âˆ’ Î±)LCE (Ë†y, y) + Î± C CX k=1 LCE (Ë†y, 1 ), (16) where Ë†y and y represent the predicted confidences and the ground truth, and 1 denotes an all one vector with C entries. LS conducts sufficient experiments to analyze the influence of the smoothing factor Î± and reveals that larger values of Î± lead to greater vulnerability to MI attacks. In contrast to the standard training which adopts Î± greater than or equal to zero, LS encourages negative values of Î± to reduce the privacy threat brought by MI attacks. ResSFL [127] considers the split federated learning (SFL), where a central server might misuse the clientsâ€™ uploaded intermediate features to reconstruct private data. A novel two- step framework is developed to deal with the MI threat. The first step adopts a publicly available dataset to pre-train a model with both high accuracy and strong MI resiliency. Similar to [27], this stage jointly trains a powerful inversion model as an adversary to achieve the MI robustness of the pre-trained model. Besides, ResSFL analyzes that higher di- mensions of uploaded features indicate better MI vulnerability. Hence, they modify the client model structure to reduce the dimension of output features. In the second step, the pre- trained model serves as the initial model for subsequent split learning. In addition to the regular main task loss, a weaker inversion model is incorporated to calculate the inversion loss that maximizes the distance between the private images and the images reconstructed by the inversion model. This allows for the optimization of the client-side model to achieve high accuracy while preserving MI resistance. For language models, [128] reveals that language models trained with variants of the differential privacy-based optimizer (i.e, scaling up the batch size to millions to improve the utility of the DP-SGD step) exhibit improved MI resistance while at the expense of performance degradation and increased computation. Besides, [129] demonstrates the memorization of language models essentially has a positive connection to the model overfitting. Therefore, adequate regularization and early stopping are also reliable alternative defense strategies. In graph learning, [21] considers differential privacy (DP) into the model training process by adding Gaussian noise to the clipped gradients in each training iteration for enhanced MI robustness. Similar to [25] and [26], [47] proposes to diminish the correlation between the GNNâ€™s inputs and outputs by injecting stochasticity into adjacent matrix A and regu- larizing the mutual dependency among graph representations, adjacency, and predicted labels during the model training. 2) Robust Fine-tuning: As noted in [130], the above ap- proaches to directly modify model training inevitably result in either a degradation in model utility or great efforts in hyperparameter tuning, given the modelâ€™s sensitivity to minor alterations in hyperparameters. To solve this challenge, another research thread proposes to fine-tune the trained model instead of directly operating the model training phase. [131] discusses the potential of fine-tuning a trained model to intentionally fool an attacker into inverting insensitive public samples. Apart from the classifier fÎ¸(Â·), the defender trains an additional public classifier fp(Â·) and a GAN model Gp(Â·) using a publicly available dataset. Then, the defender uses the obtained GAN Gp(Â·) as image priors and conducts MI attacks on both the victim model fÎ¸(Â·) and the public fp(Â·) to reconstruct private and public images respectively. Afterwards, the defender fine-tunes the victim model with the generated samples, i.e., minimizing the classification loss of insensitive public samples while maximizing that of private ones. This operation injects misleading information into the victim modelâ€™s predictions, hence deceiving the attacker into inverting samples that are significantly different from the private ones. TL [130] presents a straightforward and feasible strategy based on the transfer learning technique [132]. The fisher information is first adopted to measure the influence of each intermediate layer on the ultimate MI attack performance and classification accuracy. The findings indicate that the initial layers of a model exhibit a notable correlation with MI attacks, whereas the last several layers contribute more to the main task accuracy. Therefore, TL introduces the transfer learning mechanism since it typically fine-tunes the last few layers of a target model that are not relevant to MI attacks. Specifically, the algorithm involves pre-training the victim16 model on public datasets and then fine-tuning the last few layers on a private dataset, yielding excellent defense effects. 3) Input Data Preprocessing: These methods elaborately modify the training data to improve the robust model learning. [133] demonstrates that the data duplication strengthens the memorization of the victim language model, thereby im- proving its risk of MI attacks. Therefore, training data de- duplication is an efficient approach to reduce the privacy breaches brought by MI attacks. In graph learning, [105] proves that ensuring DP cannot effectively prevent MI at- tacks and proposes an input data processing technique, which conceals real edges by pre-processing the training graph such as randomly flipping and rewriting. This operation tricks the attacker into reconstructing dummy data that significantly differs from the ground truth. VII. S OCIAL IMPACT Given their effectiveness and practicality, model inversion attacks have become a significant privacy threat within the AI community. Case studies [23], [37] on production models (e.g., ChatGPT and Stable Diffusion) have intensified concerns about personal privacy and the urgent necessity for AI gov- ernance. Furthermore, the privacy risk is greatly amplified by the explosive development of AI generative content (AIGC), where numerous large models are trained on massive datasets that lack ownership verification. Currently, individuals may not be aware of which models have been trained on their personal data and cannot confirm whether model owners have adequately protected their privacy, which could be considered a continuous infringement of their rights to some extent. Therefore, model-level privacy protection and its interplay with official lawmaking is becoming a heated topic. The study by [30] presents an interesting viewpoint: if a model is vulnerable to MI attacks, the model itself should also be regarded as a form of personal data, and correspondingly trigger a series of rights and obligations as defined by existing data protection laws (e.g., the EU General Data Protection Regulation). Nevertheless, it is a considerably complicated task to quantify a modelâ€™s vulnerability to MI attacks, and directly categorizing models as personal data could disrupt the power relationship between model holders and individual data owners, which could have a detrimental impact on technical development. A more cautious approach for data users is to adopt robust and reliable defense strategies or establish access permissions to build responsible AI systems, thereby alleviating potential contradictions with individual data sub- jects. Besides, enacting new privacy legislation specific to model-level protection is indispensable, and introducing more regulatory policies is also strongly encouraged. VIII. C ONCLUSION AND RESEARCH DIRECTIONS This survey presents an exhaustive review of the powerful model inversion attacks. First, we comprehensively categorize existing MI methods into several macroscopic taxonomies and present a systematic overview. We then briefly introduce early MI methods on traditional ML scenarios. Next, we characterize the mainstream attacks on DNNs from multiple perspectives, based on which we deeply illustrate their features and distinctions. Moreover, we investigate MI studies on more data modalities and provide detailed taxonomies. To further facilitate the development of this field, we present a summary of several unsolved challenges and suggest some promising directions for future research. a) Stronger Generative Prior for MI attacks: In visual tasks, previous approaches leveraged various GANs as image priors for reconstruction guidance. A notable contribution by [19] introduced diffusion models into MI attacks. They manually trained a conditional diffusion model [74] to generate 64 Ã— 64 resolution images from the target distribution. To solve higher-resolution image recovery, future research could explore pre-trained diffusion models with rich image priors or alternative techniques to enhance generative model utilization. b) Defense From a Data Perspective: Existing defenses for image data primarily operate on the model [25], [26] or its outputs [25], while little attention is paid to the training data. One potential defense strategy is specially processing the data by applying well-designed masks or perturbations to cre- ate confusing reconstruction results [134]. Additionally, data synthesis [135] techniques have recently garnered significant attention. Since the synthesized images usually disguise the sensitive information, using them for training emerges as a promising defensive approach against MI attacks. c) Certified Robustness against MI Attacks: Despite ex- isting intensive studies on MI robustness, current defense approaches lack certified robustness guarantees. Certifiably robust approaches for DNNs have been developed against adversarial attacks [136] and backdoor attacks [137]. It is also imperative to establish a robust theoretical framework that can provide certified guarantees in the context of MI attacks. Additionally, exploring ways to integrate user feedback and preferences into the certification defense pipeline can enhance the practicality of ensuring certified robustness in conversa- tional AI models like ChatGPT. d) Multi-Modal MI Attacks and Defenses: Recently, ad- vanced multi-modal models trained on cross-modal data pairs have shown sophisticated capabilities in handling complex and diverse data. While current MI methods have made great progress on unimodal models, there is a lack of research on multi-modal foundation models, e.g., the CLIP [138] and SAM [139]. Exploring potential vulnerabilities during the cross- modal inference process can be a promising and necessary direction for model inversion attacks and defenses. e) Model Inversion for Good: Given the ability to re- cover diverse data from the target modelâ€™s training set dis- tribution, MI attacks can naturally be applied to various data-free learning scenarios [140], [141]. For instance, data reconstructed from a white-box teacher model can serve as a reliable dataset for training student models in data-free knowledge transfer and continual learning scenarios. Since ad- vanced MI methodologies can generate data of higher quality, they are anticipated to enhance the efficacy and versatility of their application. In addition, future research could consider utilizing the synthesized data for a broader range of learning tasks and further improve their application performance.17 REFERENCES [1] D. Shen, G. Wu, and H.-I. Suk, â€œDeep learning in medical image analysis,â€ Annual review of biomedical engineering , vol. 19, no. 1, pp. 221â€“248, 2017. [2] H.-Y . Zhou, C. Lu, C. Chen, S. Yang, and Y . Yu, â€œA unified visual information preservation framework for self-supervised pre-training in medical image analysis,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 7, pp. 8020â€“8035, 2023. [3] D. Shah, W. Campbell, and F. H. Zulkernine, â€œA comparative study of lstm and dnn for stock market forecasting,â€ in 2018 IEEE international conference on big data (big data) , pp. 4148â€“4155, IEEE, 2018. [4] A. M. Ozbayoglu, M. U. Gudelek, and O. B. Sezer, â€œDeep learning for financial applications: A survey,â€ Applied soft computing , vol. 93, p. 106384, 2020. [5] S. Zhang, L. Yao, A. Sun, and Y . Tay, â€œDeep learning based rec- ommender system: A survey and new perspectives,â€ ACM computing surveys (CSUR), vol. 52, no. 1, pp. 1â€“38, 2019. [6] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and T. Ristenpart, â€œPrivacy in pharmacogenetics: An {End-to-End} case study of person- alized warfarin dosing,â€ in USENIX Security Symposium , pp. 17â€“32, 2014. [7] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and N. Borisov, â€œProperty inference attacks on fully connected neural networks using permutation invariant representations,â€ in ACM SIGSAC Conference on Computer and Communications Security , pp. 619â€“633, 2018. [8] H. Hu, Z. Salcic, L. Sun, G. Dobbie, P. S. Yu, and X. Zhang, â€œMembership inference attacks on machine learning: A survey,â€ ACM Computing Surveys (CSUR) , vol. 54, no. 11s, pp. 1â€“37, 2022. [9] M. Jegorova, C. Kaul, C. Mayor, A. Q. Oâ€™Neil, A. Weir, R. Murray- Smith, and S. A. Tsaftaris, â€œSurvey: Leakage and privacy at inference time,â€ IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 2022. [10] K.-C. Wang, Y . Fu, K. Li, A. Khisti, R. Zemel, and A. Makhzani, â€œVariational model inversion attacks,â€ in Advances in neural informa- tion processing systems , 2021. [11] M. Fredrikson, S. Jha, and T. Ristenpart, â€œModel inversion attacks that exploit confidence information and basic countermeasures,â€ in ACM SIGSAC Conference on Computer and Communications Security, pp. 1322â€“1333, 2015. [12] C. Song, T. Ristenpart, and V . Shmatikov, â€œMachine learning models that remember too much,â€ in ACM SIGSAC Conference on Computer and Communications Security , pp. 587â€“601, 2017. [13] Z. Yang, J. Zhang, E.-C. Chang, and Z. Liang, â€œNeural network inver- sion in adversarial setting via background knowledge alignment,â€ in ACM SIGSAC Conference on Computer and Communications Security, 2019. [14] Y . Zhang, R. Jia, H. Pei, W. Wang, B. Li, and D. Song, â€œThe secret revealer: Generative model-inversion attacks against deep neural networks,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2020. [15] L. Struppek, D. Hintersdorf, A. D. A. Correira, A. Adler, and K. Kerst- ing, â€œPlug & play attacks: Towards robust and flexible model inversion attacks,â€ in International Conference on Machine Learning , 2022. [16] I. Goodfellow, â€œNips 2016 tutorial: Generative adversarial networks,â€ arXiv preprint arXiv:1701.00160 , 2016. [17] Z. Yuan, F. Wu, Y . Long, C. Xiao, and B. Li, â€œSecretgen: Privacy recovery on pre-trained models via distribution discrimination,â€ in European Conference on Computer Vision , 2022. [18] S. An, G. Tao, Q. Xu, Y . Liu, G. Shen, Y . Yao, J. Xu, and X. Zhang, â€œMirror: Model inversion for deep learning network with high fidelity,â€ in Network and Distributed System Security Symposium , 2022. [19] R. Liu, â€œUnstoppable attack: Label-only model inversion via condi- tional diffusion model,â€ arXiv preprint arXiv:2307.08424 , 2023. [20] N. Carlini, C. Liu, Â´U. Erlingsson, J. Kos, and D. Song, â€œThe secret sharer: Evaluating and testing unintended memorization in neural networks,â€ in USENIX Security Symposium , 2019. [21] Z. Zhang, Q. Liu, Z. Huang, H. Wang, C. Lu, C. Liu, and E. Chen, â€œGraphmi: Extracting private graph data from graph neural networks,â€ in International Joint Conference on Artificial Intelligence , 2021. [22] T. B. Brown, â€œLanguage models are few-shot learners,â€ arXiv preprint arXiv:2005.14165, 2020. [23] M. Nasr, N. Carlini, J. Hayase, M. Jagielski, A. F. Cooper, D. Ippolito, C. A. Choquette-Choo, E. Wallace, F. Tram `er, and K. Lee, â€œScalable extraction of training data from (production) language models,â€ arXiv preprint arXiv:2311.17035, 2023. [24] J. Wen, S.-M. Yiu, and L. C. Hui, â€œDefending against model inversion attack by adversarial examples,â€ in 2021 IEEE International Confer- ence on Cyber Security and Resilience , 2021. [25] T. Wang, Y . Zhang, and R. Jia, â€œImproving robustness to model inversion attacks via mutual information regularization,â€ inProceedings of the AAAI Conference on Artificial Intelligence , 2021. [26] X. Peng, F. Liu, J. Zhang, L. Lan, J. Ye, T. Liu, and B. Han, â€œBilateral dependency optimization: Defending against model-inversion attacks,â€ in ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2022. [27] Z. Yang, B. Shao, B. Xuan, E.-C. Chang, and F. Zhang, â€œDefending model inversion and membership inference attacks via prediction purification,â€ arXiv preprint arXiv:2005.03915 , 2020. [28] S. V . Dibbo, â€œSok: Model inversion attack landscape: Taxonomy, challenges, and future roadmap,â€ in 2023 IEEE 36th Computer Security Foundations Symposium (CSF), pp. 439â€“456, 2023. [29] J. Song and D. Namiot, â€œA survey of the implementations of model in- version attacks,â€ in International Conference on Distributed Computer and Communication Networks , pp. 3â€“16, Springer, 2022. [30] M. Veale, R. Binns, and L. Edwards, â€œAlgorithms that remember: model inversion attacks and data protection law,â€ Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, vol. 376, no. 2133, p. 20180083, 2018. [31] R. Parikh, C. Dupuy, and R. Gupta, â€œCanary extraction in natural language understanding models,â€ Annual Meeting of the Association for Computational Linguistics (ACL) , 2022. [32] M. Kahla, S. Chen, H. A. Just, and R. Jia, â€œLabel-only model inversion attacks via boundary repulsion,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2022. [33] Y . Xu, X. Liu, T. Hu, B. Xin, and R. Yang, â€œSparse black-box inversion attack with limited information,â€ in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , 2023. [34] Z. Ye, W. Luo, M. L. Naseem, X. Yang, Y . Shi, and Y . Jia, â€œC2fmi: Corse-to-fine black-box model inversion attack,â€ IEEE Transactions on Dependable and Secure Computing , 2023. [35] X. Yuan, K. Chen, J. Zhang, W. Zhang, N. Yu, and Y . Zhang, â€œPseudo label-guided model inversion attack via conditional generative adver- sarial network,â€ in Proceedings of the AAAI Conference on Artificial Intelligence, 2023. [36] Y . Liu, W. Zhang, D. Wu, Z. Lin, J. Gu, and W. Wang, â€œPrediction ex- poses your face: Black-box model inversion via prediction alignment,â€ arXiv preprint arXiv:2407.08127 , 2024. [37] N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V . Sehwag, F. Tramer, B. Balle, D. Ippolito, and E. Wallace, â€œExtracting training data from diffusion models,â€ in USENIX Security Symposium , pp. 5253â€“5270, 2023. [38] Y . Chen, X. Ma, D. Zou, and Y .-G. Jiang, â€œExtracting training data from unconditional diffusion models,â€ arXiv preprint arXiv:2406.12752 , 2024. [39] H. Li, M. Xu, and Y . Song, â€œSentence embedding leaks more in- formation than you expect: Generative embedding inversion attack to recover the whole sentence,â€ in Annual Meeting of the Association for Computational Linguistics (ACL) , pp. 14022â€“14040, 2023. [40] J. X. Morris, W. Zhao, J. T. Chiu, V . Shmatikov, and A. M. Rush, â€œLanguage model inversion,â€ arXiv preprint arXiv:2311.13647 , 2023. [41] Z. Zhang, J. Wen, and M. Huang, â€œEthicist: Targeted training data extraction through loss smoothed soft prompting and calibrated confi- dence estimation,â€ in Annual Meeting of the Association for Computa- tional Linguistics (ACL) , 2023. [42] X. Pan, M. Zhang, S. Ji, and M. Yang, â€œPrivacy risks of general-purpose language models,â€ in 2020 IEEE Symposium on Security and Privacy (SP), pp. 1314â€“1331, IEEE, 2020. [43] C. Song and A. Raghunathan, â€œInformation leakage in embedding mod- els,â€ in ACM SIGSAC Conference on Computer and Communications Security, pp. 377â€“390, 2020. [44] N. Carlini, F. Tram `er, E. Wallace, M. Jagielski, A. Herbert-V oss, K. Lee, A. Roberts, T. Brown, D. Song, Â´U. Erlingsson, A. Oprea, and C. Raffel, â€œExtracting training data from large language models,â€ in USENIX Security Symposium , pp. 2633â€“2650, Aug. 2021. [45] A. Elmahdy, H. A. Inan, and R. Sim, â€œPrivacy leakage in text classifi- cation: A data extraction approach,â€ arXiv preprint arXiv:2206.04591, 2022. [46] J. Huang, H. Shao, and K. C.-C. Chang, â€œAre large pre-trained language models leaking your personal information?,â€ arXiv preprint arXiv:2205.12628, 2022.18 [47] Z. Zhou, C. Zhou, X. Li, J. Yao, Q. Yao, and B. Han, â€œOn strengthening and defending graph reconstruction attack with markov chain approx- imation,â€ in International Conference on Machine Learning , 2023. [48] V . Duddu, A. Boutet, and V . Shejwalkar, â€œQuantifying privacy leakage in graph embedding,â€ in International Conference on Mobile and Ubiq- uitous Systems: Computing, Networking and Services (MobiQuitous) , 2020. [49] Z. Zhang, M. Chen, M. Backes, Y . Shen, and Y . Zhang, â€œInference at- tacks against graph neural networks,â€ in USENIX Security Symposium, 2022. [50] X. He, J. Jia, M. Backes, N. Z. Gong, and Y . Zhang, â€œStealing links from graph neural networks,â€ in USENIX Security Symposium , 2021. [51] F. Wu, Y . Long, C. Zhang, and B. Li, â€œLinkteller: Recovering private edges from graph neural networks via influence analysis,â€ in2022 IEEE Symposium on Security and Privacy , 2022. [52] A. Elmahdy and A. Salem, â€œDeconstructing classifiers: Towards a data reconstruction attack against text classification models,â€ arXiv preprint arXiv:2306.13789, 2023. [53] F. Kamali and H. Wynne, â€œPharmacogenetics of warfarin,â€ Annual review of medicine , vol. 61, no. 1, pp. 63â€“75, 2010. [54] Y . Wang, C. Si, and X. Wu, â€œRegression model fitting under differential privacy and model inversion attack,â€ in International Joint Conference on Artificial Intelligence , 2015. [55] J. Zhang, Z. Zhang, X. Xiao, Y . Yang, and M. Winslett, â€œFunctional mechanism: Regression analysis under differential privacy,â€ arXiv preprint arXiv:1208.0219, 2012. [56] X. Wu, M. Fredrikson, S. Jha, and J. F. Naughton, â€œA methodology for formalizing model-inversion attacks,â€ in 2016 IEEE 29th computer security foundations symposium (CSF) , pp. 355â€“370, IEEE, 2016. [57] B. Hitaj, G. Ateniese, and F. Perez-Cruz, â€œDeep models under the gan: information leakage from collaborative deep learning,â€ inACM SIGSAC Conference on Computer and Communications Security , pp. 603â€“618, 2017. [58] S. Hidano, T. Murakami, S. Katsumata, S. Kiyomoto, and G. Hanaoka, â€œModel inversion attacks for online prediction systems: Without knowl- edge of non-sensitive attributes,â€ IEICE Transactions on Information and Systems, vol. 101, no. 11, pp. 2665â€“2676, 2018. [59] X. Zhao, W. Zhang, X. Xiao, and B. Lim, â€œExploiting explanations for model inversion attacks,â€ in Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision , 2021. [60] K. Simonyan, A. Vedaldi, and A. Zisserman, â€œDeep inside convolu- tional networks: Visualising image classification models and saliency maps,â€ arXiv preprint arXiv:1312.6034 , 2013. [61] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. M Â¨uller, and W. Samek, â€œOn pixel-wise explanations for non-linear classifier deci- sions by layer-wise relevance propagation,â€ PloS one , vol. 10, no. 7, p. e0130140, 2015. [62] O. Ronneberger, P. Fischer, and T. Brox, â€œU-net: Convolutional net- works for biomedical image segmentation,â€ in Medical image comput- ing and computer-assisted interventionâ€“MICCAI 2015: 18th interna- tional conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18 , pp. 234â€“241, Springer, 2015. [63] S. Zhou, T. Zhu, D. Ye, X. Yu, and W. Zhou, â€œBoosting model inversion attacks with adversarial examples,â€ IEEE Transactions on Dependable and Secure Computing , vol. 21, no. 3, pp. 1451â€“1468, 2023. [64] C. Guo, J. Gardner, Y . You, A. G. Wilson, and K. Weinberger, â€œSimple black-box adversarial attacks,â€ in International Conference on Machine Learning, pp. 2484â€“2493, PMLR, 2019. [65] Z. Zhang, X. Wang, J. Huang, and S. Zhang, â€œAnalysis and utilization of hidden information in model inversion attacks,â€ IEEE Transactions on Information Forensics and Security , 2023. [66] Z. Zhang and J. Huang, â€œAligning the domains in cross domain model inversion attack,â€ Neural Networks, vol. 178, p. 106490, 2024. [67] N.-B. Nguyen, K. Chandrasegaran, M. Abdollahzadeh, and N.-M. Cheung, â€œRe-thinking model inversion attacks against deep neural networks,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 16384â€“16393, 2023. [68] Y . Qiu, H. Fang, H. Yu, B. Chen, M. Qiu, and S.-T. Xia, â€œA closer look at gan priors: Exploiting intermediate features for enhanced model inversion attacks,â€ in European Conference on Computer Vision, 2024. [69] G. Han, J. Choi, H. Lee, and J. Kim, â€œReinforcement learning-based black-box model inversion attacks,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2023. [70] N.-B. Nguyen, K. Chandrasegaran, M. Abdollahzadeh, and N. man Cheung, â€œLabel-only model inversion attacks via knowledge transfer,â€ in Advances in neural information processing systems , 2023. [71] S. Chen, M. Kahla, R. Jia, and G.-J. Qi, â€œKnowledge-enriched dis- tributional model inversion attacks,â€ in Proceedings of the IEEE/CVF International Conference on Computer Vision , 2021. [72] M. Khosravy, K. Nakamura, Y . Hirose, N. Nitta, and N. Babaguchi, â€œModel inversion attack by integration of deep generative models: Privacy-sensitive face generation from a face recognition system,â€IEEE Transactions on Information Forensics and Security , vol. 17, pp. 357â€“ 372, 2022. [73] G. Qi, Y . Chen, X. Mao, B. Hui, X. Li, R. Zhang, and H. Xue, â€œModel inversion attack via dynamic memory learning,â€ in ACM International Conference on Multimedia , pp. 5614â€“5622, 2023. [74] J. Ho and T. Salimans, â€œClassifier-free diffusion guidance,â€ arXiv preprint arXiv:2207.12598, 2022. [75] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, â€œThe unreasonable effectiveness of deep features as a perceptual metric,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018. [76] T. Karras, S. Laine, and T. Aila, â€œA style-based generator architecture for generative adversarial networks,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2019. [77] M. Rosca, B. Lakshminarayanan, D. Warde-Farley, and S. Mohamed, â€œVariational approaches for auto-encoding generative adversarial net- works,â€ arXiv preprint arXiv:1706.04987 , 2017. [78] D. P. Kingma and P. Dhariwal, â€œGlow: Generative flow with invertible 1x1 convolutions,â€ in Advances in neural information processing systems (S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa- Bianchi, and R. Garnett, eds.), vol. 31, Curran Associates, Inc., 2018. [79] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, â€œSoft actor-critic: Off- policy maximum entropy deep reinforcement learning with a stochastic actor,â€ in International Conference on Machine Learning , pp. 1861â€“ 1870, PMLR, 2018. [80] H. R. Sheikh, M. F. Sabir, and A. C. Bovik, â€œA statistical evaluation of recent full reference image quality assessment algorithms,â€ IEEE Transactions on image processing , vol. 15, no. 11, pp. 3440â€“3451, 2006. [81] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, â€œGans trained by a two time-scale update rule converge to a local nash equilibrium,â€ in Advances in neural information processing systems , 2017. [82] T. Kynk Â¨aÂ¨anniemi, T. Karras, S. Laine, J. Lehtinen, and T. Aila, â€œImproved precision and recall metric for assessing generative models,â€ Advances in neural information processing systems , vol. 32, 2019. [83] M. F. Naeem, S. J. Oh, Y . Uh, Y . Choi, and J. Yoo, â€œReliable fidelity and diversity metrics for generative models,â€ in International Conference on Machine Learning , pp. 7176â€“7185, PMLR, 2020. [84] C. Xu, J. Ren, D. Zhang, Y . Zhang, Z. Qin, and K. Ren, â€œGanobfusca- tor: Mitigating information leakage under gan via differential privacy,â€ IEEE Transactions on Information Forensics and Security , vol. 14, no. 9, pp. 2358â€“2371, 2019. [85] M. Arjovsky, S. Chintala, and L. Bottou, â€œWasserstein generative ad- versarial networks,â€ in International Conference on Machine Learning, pp. 214â€“223, PMLR, 2017. [86] F.-A. Croitoru, V . Hondru, R. T. Ionescu, and M. Shah, â€œDiffusion models in vision: A survey,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 45, no. 9, pp. 10850â€“10869, 2023. [87] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, â€œHigh-resolution image synthesis with latent diffusion models,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684â€“10695, 2022. [88] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, et al. , â€œPhotorealistic text-to-image diffusion models with deep lan- guage understanding,â€ Advances in neural information processing systems, vol. 35, pp. 36479â€“36494, 2022. [89] X. Gu, C. Du, T. Pang, C. Li, M. Lin, and Y . Wang, â€œOn memorization in diffusion models,â€ arXiv preprint arXiv:2310.02664 , 2023. [90] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, â€œBERT: Pre- training of deep bidirectional transformers for language understanding,â€ in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , June 2019. [91] C. Zhang, J. X. Morris, and V . Shmatikov, â€œExtracting prompts by inverting llm outputs,â€ arXiv preprint arXiv:2405.15012 , 2024. [92] W. Yu, T. Pang, Q. Liu, C. Du, B. Kang, Y . Huang, M. Lin, and S. Yan, â€œBag of tricks for training data extraction from language models,â€arXiv preprint arXiv:2302.04460, 2023.19 [93] S. Zanella-B Â´eguelin, L. Wutschitz, S. Tople, V . R Â¨uhle, A. Paverd, O. Ohrimenko, B. KÂ¨opf, and M. Brockschmidt, â€œAnalyzing information leakage of updates to natural language models,â€ in ACM SIGSAC Conference on Computer and Communications Security , 2020. [94] R. Panchendrarajan and S. Bhoi, â€œDataset reconstruction attack against language models,â€ in CEUR Workshop, 2021. [95] F. Xia, K. Sun, S. Yu, A. Aziz, L. Wan, S. Pan, and H. Liu, â€œGraph learning: A survey,â€IEEE Transactions on Artificial Intelligence, vol. 2, no. 2, pp. 109â€“127, 2021. [96] Y . Rong, T. Xu, J. Huang, W. Huang, H. Cheng, Y . Ma, Y . Wang, T. Derr, L. Wu, and T. Ma, â€œDeep graph learning: Foundations, advances and applications,â€ in Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining , pp. 3555â€“3556, 2020. [97] L. Qiao, L. Zhang, S. Chen, and D. Shen, â€œData-driven graph con- struction and graph learning: A review,â€ Neurocomputing, vol. 312, pp. 336â€“351, 2018. [98] I. Chami, S. Abu-El-Haija, B. Perozzi, C. R Â´e, and K. Murphy, â€œMa- chine learning on graphs: A model and comprehensive taxonomy,â€ Journal of Machine Learning Research, vol. 23, no. 89, pp. 1â€“64, 2022. [99] C. C. Aggarwal and H. Wang, â€œGraph data management and mining: A survey of algorithms and applications,â€ Managing and mining graph data, pp. 13â€“68, 2010. [100] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y . Philip, â€œA comprehensive survey on graph neural networks,â€ IEEE Transactions on Neural Networks and Learning Systems , vol. 32, no. 1, pp. 4â€“24, 2020. [101] E. Dai, T. Zhao, H. Zhu, J. Xu, Z. Guo, H. Liu, J. Tang, and S. Wang, â€œA comprehensive survey on trustworthy graph neural net- works: Privacy, robustness, fairness, and explainability,â€ arXiv preprint arXiv:2204.08570, 2022. [102] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, â€œThe graph neural network model,â€ IEEE transactions on neural networks, vol. 20, no. 1, pp. 61â€“80, 2008. [103] A. Gupta, P. Matta, and B. Pant, â€œGraph neural network: Current state of art, challenges and applications,â€ Materials Today: Proceedings , vol. 46, pp. 10927â€“10932, 2021. [104] Y . Xie, Z. Xu, J. Zhang, Z. Wang, and S. Ji, â€œSelf-supervised learning of graph neural networks: A unified review,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence , vol. 45, no. 2, pp. 2412â€“ 2429, 2022. [105] Z. Zhang, Q. Liu, Z. Huang, H. Wang, C.-K. Lee, and E. Chen, â€œModel inversion attacks against graph neural networks,â€ IEEE Transactions on Knowledge and Data Engineering, vol. 35, no. 9, pp. 8729â€“8741, 2023. [106] L. P. Kaelbling, M. L. Littman, and A. W. Moore, â€œReinforcement learning: A survey,â€ Journal of artificial intelligence research , vol. 4, pp. 237â€“285, 1996. [107] Y . Li, â€œDeep reinforcement learning: An overview,â€ arXiv preprint arXiv:1701.07274, 2017. [108] M. A. Wiering and M. Van Otterlo, â€œReinforcement learning,â€ Adap- tation, learning, and optimization , vol. 12, no. 3, p. 729, 2012. [109] K. Arulkumaran, M. P. Deisenroth, M. Brundage, and A. A. Bharath, â€œDeep reinforcement learning: A brief survey,â€ IEEE Signal Processing Magazine, vol. 34, no. 6, pp. 26â€“38, 2017. [110] D. Ernst and A. Louette, â€œIntroduction to reinforcement learning,â€ Feuerriegel, S., Hartmann, J., Janiesch, C., and Zschech, P .(2024). Generative ai. Business & Information Systems Engineering , vol. 66, no. 1, pp. 111â€“126, 2024. [111] C. Dwork, â€œDifferential privacy,â€ in International colloquium on au- tomata, languages, and programming , pp. 1â€“12, Springer, 2006. [112] C. Dwork, â€œDifferential privacy: A survey of results,â€ in International conference on theory and applications of models of computation, pp. 1â€“ 19, Springer, 2008. [113] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang, â€œDeep learning with differential privacy,â€ in Proceedings of the 2016 ACM SIGSAC conference on computer and communications security, pp. 308â€“318, 2016. [114] Z. Ji, Z. C. Lipton, and C. Elkan, â€œDifferential privacy and machine learning: a survey and review,â€ arXiv preprint arXiv:1412.7584, 2014. [115] T. Ha, T. K. Dang, T. T. Dang, T. A. Truong, and M. T. Nguyen, â€œDif- ferential privacy in deep learning: an overview,â€ in 2019 International Conference on Advanced Computing and Applications (ACOMP) , pp. 97â€“102, IEEE, 2019. [116] D. Ye, S. Shen, T. Zhu, B. Liu, and W. Zhou, â€œOne parameter defenseâ€”defending against data inference attacks via differential privacy,â€ IEEE Transactions on Information Forensics and Security , vol. 17, pp. 1466â€“1480, 2022. [117] T. Titcombe, A. J. Hall, P. Papadopoulos, and D. Romanini, â€œPractical defences against model inversion attacks for split neural networks,â€ arXiv preprint arXiv:2104.05743 , 2021. [118] I. E. Olatunji, M. Rathee, T. Funke, and M. Khosla, â€œPrivate graph extraction via feature explanations,â€Proceedings on Privacy Enhancing Technologies, vol. 2, pp. 59â€“78, 2023. [119] Z. Yang, L. Wang, D. Yang, J. Wan, Z. Zhao, E.-C. Chang, F. Zhang, and K. Ren, â€œPurifier: Defending data inference attacks via transform- ing confidence scores,â€ in Proceedings of the AAAI Conference on Artificial Intelligence, pp. 10871â€“10879, 2023. [120] S. Wang, Z. Ji, L. Xiang, H. Zhang, X. Wang, C. Zhou, and B. Li, â€œCrafter: Facial feature crafting against inversion-based identity theft on deep models,â€ arXiv preprint arXiv:2401.07205 , 2024. [121] A. Kolchinsky, B. D. Tracey, and D. H. Wolpert, â€œNonlinear informa- tion bottleneck,â€ Entropy, vol. 21, no. 12, p. 1181, 2019. [122] A. A. Alemi, I. Fischer, J. V . Dillon, and K. Murphy, â€œDeep variational information bottleneck,â€ arXiv preprint arXiv:1612.00410 , 2016. [123] A. Gretton, R. Herbrich, A. Smola, O. Bousquet, B. Sch Â¨olkopf, and A. HyvÂ¨arinen, â€œKernel methods for measuring independence.,â€ Journal of Machine Learning Research , vol. 6, no. 12, 2005. [124] A. Gretton, O. Bousquet, A. Smola, and B. Sch Â¨olkopf, â€œMeasuring statistical dependence with hilbert-schmidt norms,â€ in International conference on algorithmic learning theory , pp. 63â€“77, Springer, 2005. [125] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, â€œRe- thinking the inception architecture for computer vision,â€ in Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 2818â€“2826, 2016. [126] L. Struppek, D. Hintersdorf, and K. Kersting, â€œBe careful what you smooth for: Label smoothing can be a privacy shield but also a catalyst for model inversion attacks,â€ in International Conference on Learning Representations, 2024. [127] J. Li, A. S. Rakin, X. Chen, Z. He, D. Fan, and C. Chakrabarti, â€œRessfl: A resistance transfer framework for defending model inversion attack in split federated learning,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2022. [128] R. Anil, B. Ghazi, V . Gupta, R. Kumar, and P. Manurangsi, â€œLarge- scale differentially private bert,â€ arXiv preprint arXiv:2108.01624 , 2021. [129] S. Ishihara, â€œTraining data extraction from pre-trained language mod- els: A survey,â€ arXiv preprint arXiv:2305.16157 , 2023. [130] S.-T. Ho, K. J. Hao, K. Chandrasegaran, N.-B. Nguyen, and N.-M. Cheung, â€œModel inversion robustness: Can transfer learning help?,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12183â€“12193, 2024. [131] X. Gong, Z. Wang, S. Li, Y . Chen, and Q. Wang, â€œA gan-based defense framework against model inversion attacks,â€ IEEE Transactions on Information Forensics and Security , 2023. [132] S. Pan and Q. Yang, â€œA survey on transfer learning. ieee transaction on knowledge discovery and data engineering, 22 (10),â€ 2010. [133] N. Kandpal, E. Wallace, and C. Raffel, â€œDeduplicating training data mitigates privacy risks in language models,â€ in International Confer- ence on Machine Learning , 2022. [134] Q. Tan, Q. Li, Y . Zhao, Z. Liu, X. Guo, and K. Xu, â€œDefending against data reconstruction attacks in federated learning: An information theory approach,â€ arXiv preprint arXiv:2403.01268 , 2024. [135] S. Lei and D. Tao, â€œA comprehensive survey of dataset distillation,â€ IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [136] J. Cohen, E. Rosenfeld, and Z. Kolter, â€œCertified adversarial robustness via randomized smoothing,â€ in International Conference on Machine Learning, 2019. [137] B. Wang, X. Cao, N. Z. Gong, et al. , â€œOn certifying robustness against backdoor attacks via randomized smoothing,â€ arXiv preprint arXiv:2002.11750, 2020. [138] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., â€œLearning transferable visual models from natural language supervision,â€ in International Conference on Machine Learning , 2021. [139] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y . Lo, et al. , â€œSegment anything,â€ arXiv preprint arXiv:2304.02643 , 2023. [140] H. Yin, P. Molchanov, J. M. Alvarez, Z. Li, A. Mallya, D. Hoiem, N. K. Jha, and J. Kautz, â€œDreaming to distill: Data-free knowledge transfer via deepinversion,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp. 8715â€“8724, 2020. [141] G. Fang, J. Song, X. Wang, C. Shen, X. Wang, and M. Song, â€œContrastive model inversion for data-free knowledge distillation,â€ in International Joint Conference on Artificial Intelligence , 2021.20 IX. B IOGRAPHY SECTION Hao Fang received a bachelorâ€™s degree from the School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, in 2023. He is currently pursuing a Ph.D. degree in Computer Science and Technology from Tsinghua Shenzhen International School, Tsinghua University, China. His research interests include trustworthy AI and computer vision, especially in model inver- sion and adversarial attacks and defenses. He has published research on model inversion and led the development of the first open-source Python toolbox of model inversion attacks and defenses. He has also served as a reviewer for top-tier journals and conferences, such as IJCAI-24. Yixiang Qiu received a bachelorâ€™s degree from the School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, in 2024. He is currently a masterâ€™s student at Tsinghua Shenzhen International School, Tsinghua Univer- sity. He has published his first article on model inversion attacks at the European Conference on Computer Vision (ECCV) and led in developing the first open-source toolbox for this field. The link is https://github.com/ffhibnese/Model-Inversion- Attack-ToolBox. His current research interest in- cludes machine learning, computer vision, AI security and privacy. Hongyao Yu is an undergraduate student at the Department of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China. He will pursue a masterâ€™s degree in Computer Technol- ogy from Tsinghua Shenzhen International School, Tsinghua University. He has published articles on model inversion attacks and participated in devel- oping the first open-source toolbox for this field. His research interests generally include machine learning, computer vision and trustworthy AI. Wenbo Yu received a bachelorâ€™s degree from the School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, in June 2024. He is currently pursuing a mas- terâ€™s degree in Computer Technology from Tsinghua Shenzhen International Graduate School, Tsinghua University, China. His research interests mainly in- clude Machine Learning and AI Security. He has been invited to serve as a reviewer for many top-tier journals and conferences, such as IEEE JSAC. Jiawei Kong is currently an undergraduate student at the School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China. He has been admitted and will pursue a masterâ€™s degree in Computer Technology from Tsinghua Shenzhen International Graduate School, Tsinghua University, China. His research interests mainly focus on AI security, including adversarial attacks and backdoor attacks and defenses. Baoli Chong is currently an undergraduate student in the School of Computer Science and Technology at Harbin Institute of Technology, Shenzhen, China. She will pursue a masterâ€™s degree in Computer Science and Technology from Harbin Institute of Technology, Shenzhen, China. Her research interests include deep learning and AI security. Bin Chen (Member, IEEE) received the B.S. and M.S. degrees in mathematics from South China Normal University, Guangzhou, China, in 2014 and 2017, respectively, and a Ph.D. degree from the Department of Computer Science and Technology, Tsinghua University, Beijing, China, in 2021. From December 2019 to May 2020, he visited the De- partment of Electrical and Computer Engineering, the University of Waterloo, Canada. From May 2021 to November 2021, he was a Post-Doctoral Researcher with Tsinghua Shenzhen International Graduate School, Tsinghua University. Since December 2021, he has been with the School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China, where he is currently an Associate Professor. He served as a Guest Editor of Entropy, and PC members for CVPR-23, ICCV-23, AAAI-21/22/23, and IJCAI-21/22/23. His research interests include coding and information theory, machine learning, and deep learning. Xuan Wang (Member, IEEE) received the Ph.D. degree in computer science from Harbin Institute of Technology in 1997. He is one of the inventors of Microsoft Pinyin, and once worked in Microsoft headquarter in Seattle due to his contribution to Microsoft Pinyin. He is currently a professor of the School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China. His main research interests include cybersecurity, information game theory, and artificial intelligence. Shu-Tao Xia (Member, IEEE) received the B.S. de- gree in mathematics and the Ph.D. degree in applied mathematics from Nankai University, Tianjin, China, in 1992 and 1997, respectively. From March 1997 to April 1999, he was with the Research Group of Information Theory, Department of Mathemat- ics, Nankai University. Since January 2004, he has been with Tsinghua Shenzhen International Graduate School, Tsinghua University, Guangdong, China, where he is currently a Full Professor. His papers have been published in multiple top-tier journals and conferences, such as IEEE TPAMI, IEEE TIFS, IEEE TDSC, CVPR, ICLR, ICCV , and NeurIPS. His current research interests include coding and information theory, networking, machine learning, and AI security. Ke Xu (Fellow, IEEE) received the Ph.D. degree from the Department of Computer Science and Tech- nology, Tsinghua University, Beijing, China. He is currently a Full Professor in the Department of Com- puter Science and Technology, Tsinghua University. He has published more than 200 technical articles and holds 11 U.S. patents in the research areas of next-generation Internet, blockchain systems, the Internet of Things, and network security. He is a member of ACM and an IEEE Fellow. He was the Steering Committee Chair of IEEE/ACM IWQoS. He has guest-edited several special issues in IEEE and Springer journals.",
      "references": [],
      "meta_data": {
        "arxiv_id": "2402.04013v2",
        "authors": [
          "Hao Fang",
          "Yixiang Qiu",
          "Hongyao Yu",
          "Wenbo Yu",
          "Jiawei Kong",
          "Baoli Chong",
          "Bin Chen",
          "Xuan Wang",
          "Shu-Tao Xia",
          "Ke Xu"
        ],
        "published_date": "2024-02-06T14:06:23Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "[Unavailable]",
        "methodology": "[Unavailable]",
        "experimental_setup": "[Unavailable]",
        "limitations": "[Unavailable]",
        "future_research_directions": "[Unavailable]",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Automatic Chain of Thought Prompting in Large Language Models",
      "full_text": "AUTOMATIC CHAIN OF THOUGHT PROMPTING IN LARGE LANGUAGE MODELS Zhuosheng Zhangâ€ âˆ—, Aston Zhangâ€¡, Mu Liâ€¡, Alex Smolaâ€¡ â€ Shanghai Jiao Tong University,â€¡Amazon Web Services ABSTRACT Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like â€œLetâ€™s think step by stepâ€ to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-speciï¬c demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the â€œLetâ€™s think step by stepâ€ prompt to generate reasoning chains for demonstrations one by one, i.e., letâ€™s think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we ï¬nd that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto- CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot 1 Introduction Large language models (LLMs) [Brown et al., 2020, Thoppilan et al., 2022, Rae et al., 2021, Chowdhery et al., 2022] have performed impressively on complex reasoning tasks by decomposing the multi-step problems into intermediate steps before producing the answer. This reasoning process is elicited by a very recent technique: chain-of-thought (CoT) prompting [Wei et al., 2022a]. CoT prompting can be categorized into two major paradigms. One adds a single prompt like â€œLetâ€™s think step by stepâ€ after the test question to facilitate the reasoning chains in LLMs [Kojima et al., 2022]. Since this prompting paradigm is task-agnostic and does not need input-output demonstrations, it is called Zero-Shot-CoT (left of Figure 1). With Zero-Shot-CoT, LLMs have shown to be decent zero-shot reasoners. The other paradigm is few-shot prompting with manual reasoning demonstrations one by one [Wei et al., 2022a]. Each demonstration has a question and a reasoning chain. A reasoning chainis composed of a rationale (a series of intermediate reasoning steps) and an expected answer. With all the demonstrations being manually designed, this paradigm is referred to as Manual-CoT (right of Figure 1). In practice, Manual-CoT has obtained stronger performance than Zero-Shot-CoT [Wei et al., 2022a, Kojima et al., 2022]. However, this superior performance hinges on the hand-drafting of effective demonstrations. Speciï¬cally, the hand-drafting involves nontrivial efforts in designs of both questions and their reasoning chains for demonstrations. Moreover, human efforts for designing task-speciï¬c demonstrations are even more: different tasks, such as arithmetic [Roy and Roth, 2015] and commonsense reasoning [Talmor et al., 2019], require different ways of demonstrations. To eliminate such manual designs, we advocate another Auto-CoT paradigm to automatically construct demonstrations with questions and reasoning chains. Speciï¬cally, Auto-CoT leverages LLMs with the â€œLetâ€™s think step by stepâ€ prompt to generate reasoning chains for demonstrations one by one, i.e., letâ€™s think not just step by step, but also one by one. âˆ—Work done during an internship at Amazon Web Services. Correspondence to Zhuosheng Zhang <zhangzs@sjtu.edu.cn> and Aston Zhang <astonz@amazon.com> arXiv:2210.03493v1  [cs.CL]  7 Oct 2022A:Letâ€™sthinkstepbystep.Thereare64puppies.28ofthemweresold.Thisleaves36puppies.Eachcagehas4puppies,soweneed9cages.Therefore,theanswer(arabicnumerals)is Test QuestionGenerated Rationale Manual DemosOne by One Q:Apetstorehad64puppies.Inonedaytheysold28ofthemandputtherestintocageswith4ineachcage.Howmanycagesdidtheyuse? Q:Thereare15treesinthegrove.Groveworkerswillplanttreesinthegrovetoday.Aftertheyaredone,therewillbe21trees.Howmanytreesdidthegroveworkersplanttoday?A:Thereare15treesoriginally.Thentherewere21treesaftersomemorewereplanted.Sotheremusthavebeen21-15=6.Theansweris6. â€¦ RationaleGeneration 9.LLMAnswerExtractionThepetstorehad64puppies.Theysold28ofthem.Sotheyhad64-28=36puppiesleft.Theyputthemintocageswith4ineachcage.Sotheyused36/4=9cages.Theansweris9. Q:Apetstorehad64puppies.Inonedaytheysold28ofthemandputtherestintocageswith4ineachcage.Howmanycagesdidtheyuse?A: (a) Zero-Shot-CoT (b) Manual-CoT Question AnswerRationale LLM LLM Q:Apetstorehad64puppies.Inonedaytheysold28ofthemandputtherestintocageswith4ineachcage.Howmanycagesdidtheyuse?A:Letâ€™sthinkstepbystep. Figure 1: Zero-Shot-CoT [Kojima et al., 2022] (using the â€œLetâ€™s think step by stepâ€ prompt) and Manual-CoT [Wei et al., 2022a] (using manually designed demonstrations one by one) with example inputs and outputs of an LLM. However, we ï¬nd that this challenge cannot be effectively addressed by simple solutions. For example, given a test question of a dataset, retrieving semantically similar questions and invoking Zero-Shot-CoT to generate reasoning chains will fail. Although LLMs are decent zero-shot reasoners, they are not perfect: Zero-Shot-CoT can still make mistakes in reasoning chains. To mitigate the effect of reasoning chain mistakes from Zero-Shot-CoT, our analysis shows that diversity of demonstration questions is the key. Based on this insight, we propose an Auto-CoT method to automatically construct demonstrations. Auto-CoT consists of two main steps. First, partition questions of a given dataset into a few clusters. Second, select a representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. We evaluate Auto-CoT on ten benchmark reasoning tasks including: (i) arithmetic reasoning (MultiArith [Roy and Roth, 2015], GSM8K [Cobbe et al., 2021], AQUA-RAT [Ling et al., 2017], SV AMP [Patel et al., 2021]); (ii) commonsense reasoning (CSQA [Talmor et al., 2019], StrategyQA [Geva et al., 2021]); (iii) symbolic reasoning (Last Letter Concatenation, Coin Flip) [Wei et al., 2022a]. Experimental results show that with GPT-3, Auto-CoT consistently matches or exceeds the performance of Manual-CoT that requires manual designs. This indicates that LLMs can perform CoT reasoning by automatically constructing demonstrations. 2 Related Work This section reviews two lines of research that form the basis of this work: chain-of-thought (CoT) prompting for multi-step reasoning and in-context learning for inducing LLMs to learn from demonstrations. 2.1 Chain-of-thought Prompting CoT prompting is a gradient-free technique of inducing LLMs to produce intermediate reasoning steps that lead to the ï¬nal answer. Wei et al. [2022a] formally studied the topic of CoT prompting in language models. This technique elicits LLMs to generate a coherent series of intermediate reasoning steps that lead to the ï¬nal answer to a question. Studies have shown that LLMs can perform CoT reasoning with zero-shot prompting (Zero-Shot-CoT) [Kojima et al., 2022] or manually written few-shot demonstrations (Manual-CoT) [Wei et al., 2022a]. Zero-Shot-CoT. Kojima et al. [2022] showed that LLMs are decent zero-shot reasoners whose generated rationales have already reï¬‚ected the CoT reasoning. This ï¬nding inspires our work to leverage the self-generated rationales for demonstrations. Generating rationales by LLMs was shown to be practical in a recent work [Zelikman et al., 2022]. In 2their work, an LLM is prompted to generate rationales and those rationales that lead to the correct answer are selected. The selection requires a training dataset of questions with annotated answers. In contrast, our work considers a more challenging scenario where only a set of test questions are given (without a training dataset), following CoT prompting studies by Wei et al. [2022a] and Kojima et al. [2022]. Manual-CoT. Manual-CoT achieves stronger performance by eliciting the CoT reasoning ability with effective manual demonstrations. The demonstrations for the reasoning process are manually designed. However, the human efforts in designs of both questions and their reasoning chains are nontrivial. Instead of addressing this limitation, recent studies mainly focus on hand-crafting more complex demonstrations or leveraging ensemble-like methods. One trend is problem decomposition. In least-to-most prompting [Zhou et al., 2022], complex problems are reduced to sub-problems, and then the sub-problems are solved sequentially. The other trend is to vote over multiple reasoning paths for a test question. Wang et al. [2022a] introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the ï¬nal answers. Wang et al. [2022b] and Li et al. [2022] introduced randomness in the input space to produce more diverse outputs for voting. They used manually-designed demonstrations as the seed set and generated additional rationales: leave one question from the seed set and use the remaining demonstrations to generate rationales for this question by the LLM. Unlike the aforementioned research lines that rely on manually-designed demonstrations, our work intends to eliminate manual designs with competitive performance. 2.2 In-Context Learning CoT prompting is closely related to in-context learning (ICL) [Radford et al., 2019, Brown et al., 2020]. ICL enables LLMs to perform a target task by feeding a few prompted examples as part of the input. Without gradient update, ICL allows a single model to perform various tasks universally. There are various research lines to improve the performance of ICL: (i) retrieving related demonstrations to the test instance where the popular practice is dynamically retrieving related training examples for a given test input [Rubin et al., 2022, Su et al., 2022]; (ii) augmenting with ï¬ne-grained information, such as incorporating task instruction [Mishra et al., 2022, Wei et al., 2022b, Sanh et al., 2022]; (iii) manipulating output probabilities of LLMs instead of directly computing the likelihood of target labels [Holtzman et al., 2021, Zhao et al., 2021, Min et al., 2022a]. Despite the success of ICL, studies [Liu et al., 2022a, Lu et al., 2022] have shown that the strength of ICL may vary widely depending on the choice of in-context demonstrations [Liu et al., 2022b]. In detail, the formatting of the prompt, such as wording or order of demonstrations, may lead to performance ï¬‚uctuations [Webson and Pavlick, 2022, Zhao et al., 2021]. A recent work [Min et al., 2022b] even questioned the necessity of ground-truth input- output mapping: using incorrect labels in the examples only marginally lowers the performance. However, the existing analysis of ICL is mainly based on standard classiï¬cation and multi-choice datasets that only have simple <inputâ†’output> mappings. We discover that those ï¬ndings may not be applicable to the CoT prompting scenario with more complex <inputâ†’rationaleâ†’output> mappings. For example, mistakes in either the <inputâ†’rationale> mapping or the <rationaleâ†’output> mapping will lead to a dramatic performance drop (Appendix A.1). 3 Challenge of Auto-CoT As just discussed, the performance of ICL hinges on hand-crafted demonstrations. As reported in Manual-CoT [Wei et al., 2022a], using demonstrations written by different annotators brings up to 28.2% accuracy disparity in a symbolic reasoning task, while changing the order of demonstrations results in less than 2% changes in most tasks. This suggests that the key challenge of Auto-CoT lies in automatically constructing demonstrations with good questions and their reasoning chains. Recall that Manual-CoT hand-crafts a few (e.g., 8) questions in demonstrations. With similarity-based retrieval methods being widely adopted for prompting LLMs [Rubin et al., 2022, Su et al., 2022], a promising candidate solution is to sample demonstration questions using similarity-based retrieval. We follow the more challenging assumption in CoT studies [Wei et al., 2022a, Kojima et al., 2022] that only a set of test questions are given (without a training dataset). Following Liu et al. [2022a], we use Sentence-BERT [Reimers and Gurevych, 2019] to encode questions. For each question qtest in a test dataset, we sample demonstration questions qdemo i (i = 1, . . . , k) from the rest of the questions. We design a Retrieval-Q-CoT method to retrieve the top-k (e.g., k = 8) similar questions based on cosine similarity. To compare with this similarity-based method, we also test a relatively more diversity-based method: Random-Q-CoT, which randomly samples k other test questions for each test question. Both Retrieval-Q-CoT and Random-Q-CoT invoke Zero-Shot-CoT [Kojima et al., 2022] to generate the reasoning chain cdemo i (rationale and answer) for each sampled question qdemo i , as LLMs are decent zero-shot reasoners [Kojima et al., 2022]. We use GPT-3 [Brown et al., 2020] with 175B parameters (text-davinci-002) for the LLM unless otherwise stated. 3On a high level, both Retrieval-Q-CoT and Random-Q-CoT take the concatenation of qdemo i , cdemo i pairs (i = 1, . . . , k) and qtest as input to predict the reasoning chain for qtest, which contains the answer in the end (like right of Figure 1). Table 1: Accuracy (%) of different sampling methods. Symbol â€ indicates using training sets with annotated reasoning chains. Method MultiArith GSM8K AQuA Zero-Shot-CoT 78.7 40.7 33.5 Manual-CoT 91.7 46.9 35.8 â€  Random-Q-CoT 86.2 47.6â€  36.2â€  Retrieval-Q-CoT 82.8 48.0â€  39.7â€  To our surprise, Retrieval-Q-CoT underperforms Random-Q-CoT on the arithmetic dataset MultiArith [Roy and Roth, 2015] (Table 1). Note that the retrieval methods were originally proposed in tasks with annotated labels [Rubin et al., 2022, Su et al., 2022], however, invoking Zero-Shot-CoT does not guarantee entirely correct reasoning chains. Thus, we hypothesize that the inferior performance of Retrieval-Q-CoT is caused by incorrect reasoning chains by Zero- Shot-CoT. To test this hypothesis, we experiment with Retrieval-Q- CoT on two other datasets GSM8K [Cobbe et al., 2021] and AQuA [Ling et al., 2017] that have training sets with annotated reasoning chains. The results are shown with â€ in Table 1. Under the setting with annotated reasoning chains, Retrieval-Q-CoT even outperforms Manual-CoT. The result indicates that Retrieval-Q-CoT is effective when human annotations are available. Although human annotations are useful, such manual efforts are nontrivial. However, automatically generating reasoning chains via Zero-Shot-CoT underperforms Manual-CoT, especially when the challenge of question sampling is not addressed. To design more effective Auto-CoT, we need to understand its challenge better. 3.1 Retrieval-Q-CoT Fails due to Misleading by Similarity Since Retrieval-Q-CoT uses a few prompting demonstrations like in Manual-CoT, Retrieval-Q-CoT is expected to perform competitively as well. However, reasoning chains (both rationales and answers) in Retrieval-Q-CoT are generated by Zero-Shot-CoT: they may have mistakes that lead to wrong answers. Let us simply call demonstrations with wrong answers as wrong demonstrations. Intuitively, after similar questions to a test question are retrieved, wrong demonstrations caused by Zero-Shot-CoT may mislead the same LLM to reason similarly with a wrong answer (e.g., replicating mistakes) for the test question. We refer to this phenomenon as misleading by similarity. We will investigate whether misleading by similarity contributes to the inferior performance of Retrieval-Q-CoT. Retrieval-Q-CoT Random-Q-CoT 20 30 40 50 Rate (%) Figure 2: Unresolving Rate. To begin with, we invoke Zero-Shot-CoT on all the 600 questions from the MultiArith dataset. Among them, we collect those 128 questions (denoted as Q) where Zero-Shot-CoT generates wrong answers (error rate: 21.3% = 128 /600). As we mentioned, with extra demonstrations, Retrieval-Q-CoT and Random-Q-CoT are expected to perform more competitively than Zero-Shot-CoT. Among Qwhere Zero-Shot-CoT fails, we call those where Retrieval-Q-CoT or Random-Q-CoT still fail as their unresolved questions. We divide the number of unresolved questions by 128 (number of questions in Q) to calculate the unresolving rate. A higher unresolving rate means that a method more likely still makes mistakes like Zero-Shot-CoT. Figure 2 shows that the unresolving rate of Retrieval-Q-CoT (46.9%) is much higher than Random-Q-CoT (25.8%). It indicates that with similar questions being sampled for test questions, Retrieval-Q-CoT is negatively affected by misleading by similarity. To show that unresolved questions of Retrieval-Q-CoT tend to be similar, we present a case study in Table 2. In the left part, the retrieved demonstration questions are similar to the test question and ask â€œhow long will it take him to cook the rest?â€ The reasoning chains generated by Zero-Shot-CoT produce answers regarding â€œthe total ofâ€ instead of â€œthe restâ€. Following the demonstrations, Retrieval-Q-CoT also fails by misunderstanding the meaning of â€œthe restâ€. In contrast, Random-Q-CoT correctly understands â€œthe restâ€ better without making similar mistakes in the demonstrations, thanks to relatively more diverse (random) demonstrations. 3.2 Errors Frequently Fall into the Same Cluster Motivated by the observations in Table 2, we use k-means to partition all the 600 test questions into k = 8 clusters, where each cluster contains similar questions.2 With these clusters and reasoning chains generated by Zero-Shot-CoT 2We use Sentence-BERT [Reimers and Gurevych, 2019] to encode questions and applyk-means for clustering. 4Table 2: Examples of Retrieval-Q-CoT and Random-Q-CoT for constructing demonstrations and prediction. The upper part contains the automatically constructed demonstrations, and the lower part contains test questions along with predicted reasoning chains. Retrieval-Q-CoT Random-Q-CoT Q: A chef needs to cook 9 potatoes. He has already cooked 7. If each potato takes 3 minutes to cook, how long will it take him to cook the rest? Q: Wendy uploaded 79 pictures to Facebook. She put 44 pics into one album and put the rest into 5 different albums. How many pictures were in each album? A: Letâ€™s think step by step. The chef has already cooked 7 potatoes. That means it has taken him 7 * 3 minutes to cook those 7 potatoes. That means it will take him 3 more minutes to cook each of the remaining 2 potatoes. That means it will take him a total of 7 * 3 + 2 * 3 = 21 minutes to cook all 9 potatoes. The answer is 21. \u0017 A: Letâ€™s think step by step. Wendy uploaded 79 pictures to Facebook. She put 44 pictures into one album. That means she put the rest of the pictures into 5 different albums. That means each album would have (79-44)/5 = 15 pictures in each album. The answer is 15.âœ“ Q: A chef needs to cook 13 potatoes. He has already cooked 5. If each potato takes 6 minutes to cook, how long will it take him to cook the rest? Q: At a restaurant each adult meal costs $5 and kids eat free. If a group of 15 people came in and 8 were kids, how much would it cost for the group to eat? A: Letâ€™s think step by step. The chef has already cooked 5 potatoes. That means it has taken him 30 minutes so far. There are 13 potatoes in total. 5 have been cooked so far. That means there are 8 left to cook. Each potato takes 6 minutes to cook. That means it will take the chef 48 minutes to cook the 8 remaining potatoes. In total, it will take the chef 78 minutes to cook all 13 potatoes. The answer is 78. \u0017 A: Letâ€™s think step by step. There are 8 kids, so that means there are 7 adults. Each adult meal costs $5, so that means that the 7 adults will cost $5*7=$35. Now we just need to add the two numbers together. $35+8=43$. So it would cost $43 for the group to eat. The answer is 43. \u0017 Q: A chef needs to cook 15 potatoes. He has already cooked 8. If each potato takes 9 minutes to cook, how long will it take him to cook the rest? Q: A chef needs to cook 15 potatoes. He has already cooked 8. If each potato takes 9 minutes to cook, how long will it take him to cook the rest? A: Letâ€™s think step by step. The chef has already cooked 8 potatoes. That means it has taken him 8 * 9 minutes to cook those 8 potatoes. That means it will take him 9 minutes to cook each of the remaining 7 potatoes. That means it will take him a total of 8 * 9 + 7 * 9 = 135 minutes to cook all 15 potatoes. The answer is 135. \u0017 A: Letâ€™s think step by step. The chef needs to cook 15 potatoes. He has already cooked 8. That means he has to cook 15-8=7 more potatoes. Each potato takes 9 minutes to cook. That means it will take him 9*7=63 minutes to cook the restof the potatoes. The answer is 63. âœ“ (in Section 3.1), now we are curious if certain clusters contain questions where Zero-Shot-CoT frequently fails. Thus, we calculate the error rate (questions with wrong Zero-Shot-CoT answers / total questions) for each cluster. 1 2 3 4 5 6 7 80 20 40 60 Error Rate (%) Figure 3: Clusters of similar questions. As shown in Figure 3, there exists a cluster (Cluster 2) with frequent Zero-Shot-CoT errors (52.3%). The phenomenon could be generic as Zero-Shot-CoT may lack some skills to solve some common problems in target tasks.3 For convenience of descriptions, let us call the cluster with the highest error rate as the frequent-error cluster(e.g., Cluster 2 in Figure 3). Therefore, the imperfect nature of generated reasoning chains in a zero-shot fashion poses risks of retrieving multiple similar questions inside a frequent-error cluster by using similarity-based methods. For the test question in the frequent-error cluster, Retrieval- Q-CoT more easily constructs demonstrations with multiple similar mistakes. As a result, Retrieval-Q-CoT often makes similar mistakes like Zero-Shot-CoT, reiterated by its higher unresolving rate in Figure 2. 3.3 Diversity May Mitigate Misleading by Similarity The analysis so far compellingly shows that LLMs are still not perfect zero-shot reasoners; thus, we aim to mitigate the effect of their Zero-Shot-CoT errors, especially to mitigate misleading by similarity in the design of Auto-CoT. As we will show later (Section 5.5), presenting a small portion of mistakes (e.g., 1 or 2 wrong demonstrations out of 8) would not harm the overall reasoning performance for test questions. Suppose that questions of all the wrong demonstrations fall into the same frequent-error cluster; then sampling one question from every different cluster will lead to a higher than 7/8 = 87.5% chance to construct all the 8 correct demonstrations. Since different clusters reï¬‚ect diverse semantics of the questions, this clustering-based sampling method can be considered as diversity-based, which is in sharp contrast to similarity-based Retrieval-Q-CoT. On one hand, sampling questions with diversity may mitigate 3We observe similar phenomena when changing the cluster number or using other datasets (Appendix A.2). 5the effect of misleading by similarity (Section 3.1). On the other hand, if we took each demonstration as a kind of skill, diverse demonstrations seem to cover more alternative skills for solving target questions: even though there still exists a small portion (e.g., 1/8) of mistakes in the demonstrations, the performance will not be negatively affected (to be shown in Figure 6). Nevertheless, the clustering-based sampling method may still construct a small portion of wrong demonstrations, such as from questions in the frequent-error cluster. As we will show later, some of these wrong demonstrations may be eliminated with heuristics. For example, wrong demonstrations often come with long questions and long rationales. Using simple and generic heuristics, such as only considering shorter questions with shorter rationales, further helps mitigate the effect of imperfect Zero-Shot-CoT capabilities (Appendix C.2). 4 Auto-CoT: Automatic Chain-of-Thought Prompting Based on the observations and considerations in Section 3, we propose anAuto-CoT method to construct demonstrations with questions and reasoning chains automatically. Auto-CoT consists of two main stages: (i) question clustering: partition questions of a given dataset into a few clusters; (ii) demonstration sampling: select a representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. The overall procedure is illustrated in Figure 4. Q:Whileshoppingformusiconline,Zoebought3countryalbumsand5popalbums.Eachalbumcamewithalyricsheetandhad3songs.HowmanysongsdidZoebuytotal?A:Letâ€™sthinkstepbystep.Zoebought3countryalbums.Eachalbumhas3songs.Soshebought3*3=9songsfromthecountryalbums.Zoebought5popalbums.Eachalbumhas3songs.Soshebought5*3=15songsfromthepopalbums.Zoebought9+15=24songsintotal.Theansweris24.â€¦Q:Achefneedstocook9potatoes.Hehasalreadycooked7.Ifeachpotatotakes3minutestocook,howlongwillittakehimtocooktherest?A:Letâ€™sthinkstepbystep.Thechefhasalreadycooked7potatoes.Thatmeansithastakenhim7*3minutestocookthose7potatoes.Thatmeansitwilltakehim3moreminutestocookeachoftheremaining2potatoesâ€¦ DemoConstruction LLMIn-Context Reasoning Q: While shopping for music online, Zoe bought 3 â€¦ Q: A chef needs to cook 9 potatoes. He has alreadyâ€¦ LLM Q: While shopping for music online â€¦A: Letâ€™s â€¦1 Q: A chef needs to cook 9 potatoes ...A: Letâ€™s â€¦k 1 kClustering SamplingbySelectionCriteria Q:Apetstorehad64puppies.Inonedaytheysold28ofthemandputtherestintocageswith4ineachcage.Howmanycagesdidtheyuse?A:Letâ€™sthinkstepbystep. Thepetstorehad64puppies.Theysold28ofthem.Thatmeanstheyhave36puppiesleft.Theyputtherestintocageswith4ineachcage.Thatmeanstheyhave9cages.Theansweris9. Auto Demos One by One Test Question Figure 4: Overview of the Auto-CoT method. Different from Manual-CoT in Figure 1, demonstrations (on the right) are automatically constructed one by one (total: k) using an LLM with the â€œLetâ€™s think step by stepâ€ prompt. 4.1 Question Clustering Since diversity-based clustering may mitigate misleading by similarity (Section 3.3), we perform cluster analysis for a given set of questions Q. We ï¬rst compute a vector representation for each question in Qby Sentence-BERT [Reimers and Gurevych, 2019]. The contextualized vectors are averaged to form a ï¬x-sized question representation. Then, the question representations are processed by the k-means clustering algorithm to produce k clusters of questions. For questions in each cluster i, sort them into a list q(i) = [q(i) 1 , q(i) 2 , . . .] in the ascending order of the distance to the center of cluster i. This question clustering stage is summarized in Algorithm 1. 4.2 Demonstration Sampling In the second stage, we need to generate reasoning chains for those sampled questions and then sample demonstrations that satisfy our selection criteria. 6More concretely, we construct a demonstration d(i) (concatenation of a question, a rationale, and an answer) for each cluster i (i = 1, . . . , k). For cluster i, we iterate over questions in the sorted list q(i) = [q(i) 1 , q(i) 2 , . . .] (obtained by Algorithm 1) until satisfying our selection criteria. In other words, a question that is closer to the center of cluster i is considered earlier. Say that the j-th closest question q(i) j is being considered. A prompted input is formulated as: [Q: q(i) j . A: [P]], where [P] is a single prompt â€œLetâ€™s think step to stepâ€. This formed input is fed into an LLM using Zero-Shot-CoT [Kojima et al., 2022] to output the reasoning chain consisting of the rationale r(i) j and the extracted answer a(i) j . Then, a candidate demonstration d(i) j for the i-th cluster is constructed by concatenating the question, rationale, and answer: [Q: q(i) j , A: r(i) j â—¦a(i) j ]. Similar to the criteria of the hand-crafting demonstrations in Wei et al. [2022a], our selection criteria follow simple heuristics to encourage sampling simpler questions and rationales: set the selected demonstration d(i) as d(i) j if it has a question q(i) j with no more than 60 tokens and a rationale r(i) j with no more than 5 reasoning steps.4 Algorithm 1Cluster Require: A set of questions Q and the number of demonstrations k Ensure: Sorted questions q(i) = [ q(i) 1 , q(i) 2 , . . .] for each cluster i (i = 1, . . . , k) 1: procedure CLUSTER (Q, k) 2: for each question q in Qdo 3: Encode q by Sentence-BERT 4: Cluster all the encoded question representations into k clusters 5: for each cluster i = 1, . . . , kdo 6: Sort questions q(i) = [ q(i) 1 , q(i) 2 , . . .] in the ascending order of the distance to the cluster center 7: return q(i) (i = 1, . . . , k) Algorithm 2Construct Require: Sorted questions q(i) = [q(i) 1 , q(i) 2 , . . .] for each cluster i (i = 1, . . . , k), empty demonstration list d Ensure: Demonstration list d = [d(1), . . . , d(k)] 1: procedure CONSTRUCT (q(i), . . . ,q(k)) 2: for each cluster i = 1, . . . , kdo 3: for each question q(i) j in q(i) do 4: Generate rationale r(i) j and answer a(i) j for q(i) j using Zero-Shot-CoT 5: if q(i) j , r(i) j satisfy selection criteria then 6: Add d(i) = [Q: q(i) j , A: r(i) j â—¦a(i) j ] to d 7: break 8: return d As summarized in Algorithm 2, after demonstration sampling for all the k clusters, there will be k constructed demonstrations [d(1), . . . , d(k)]. The constructed demonstrations are used to augment a test question qtest for in-context learning. Speciï¬cally, the input is the concatenation of all the demonstrations [d(1), . . . , d(k)] followed by [Q: qtest. A: [P]]. This input is fed to LLMs to obtain the reasoning chain with the answer in the end for qtest (right of Figure 4). 5 Experiments We brieï¬‚y describe the experimental setup and present main experimental results. More experimental details and results can be found in the appendices. 5.1 Experimental setup Tasks and Datasets. Our method is evaluated on ten benchmark datasets from three categories of reasoning tasks: (i) arithmetic reasoning (MultiArith [Roy and Roth, 2015], GSM8K [Cobbe et al., 2021], AddSub [Hosseini et al., 2014], AQUA-RAT [Ling et al., 2017], SingleEq [Koncel-Kedziorski et al., 2015], SV AMP [Patel et al., 2021]); (ii) commonsense reasoning (CSQA [Talmor et al., 2019], StrategyQA [Geva et al., 2021]); (iii) symbolic reasoning (Last Letter Concatenation, Coin Flip) [Wei et al., 2022a]. Implementation. We use the public GPT-3 [Brown et al., 2020] of the text-davinci-002 version with 175B parameters for the LLM [Ouyang et al., 2022] unless otherwise stated. We select this LLM because it has the strongest CoT reasoning performance among public LLMs, as reported in Kojima et al. [2022] and Wei et al. [2022a]. We also evaluate the Codex model [Chen et al., 2021] (code-davinci-002) as the LLM. Following Wei et al. [2022a], the number of demonstrations k is 8 except for AQuA and Letter (4), CSQA (7), and StrategyQA (6). 4Because Zero-Shot-CoT often uses â€œ\\nâ€ for separating the reasoning steps, the rule can be easily implemented by counting the â€œ\\nâ€ tokens in the generated rationales. 7Table 3: Accuracy on ten datasets from three categories of reasoning tasks. Model Arithmetic Commonsense Symbolic MultiArith GSM8K AddSub AQuA SingleEq SV AMP CSQA Strategy Letter Coin Zero-Shot 22.7 12.5 77.0 22.4 78.7 58.8 72.6 54.3 0.2 53.8 Zero-Shot-CoT 78.7 40.7 74.7 33.5 78.7 63.7 64.6 54.8 57.6 91.4 Few-Shot 33.8 15.6 83.3 24.8 82.7 65.7 79.5 65.9 0.2 57.2 Manual-CoT 91.7 46.9 81.3 35.8 86.6 68.9 73.5 65.4 59.0 97.2 Auto-CoT 92.0 47.9 84.8 36.5 87.0 69.5 74.4 65.4 59.7 99.9 Baselines. We compare our methods with four baseline methods: Zero-Shot [Kojima et al., 2022], Zero-Shot-CoT [Kojima et al., 2022], Few-Shot [Wei et al., 2022a], and Manual-CoT [Wei et al., 2022a]. Zero-Shot-CoT and Manual- CoT are illustrated in Figure 1. The Zero-Shot baseline concatenates a test question with the prompt â€œThe answer isâ€ as the LLM input. The Few-Shot baseline has the same LLM input as Manual-CoT except for removed rationales from all the demonstrations. 5.2 Competitive Performance of Auto-CoT on Ten Datasets Table 4: Accuracy using the Codex LLM. Method MultiArith GSM8K AddSub Zero-Shot-CoT 64.8 31.8 65.6 Manual-CoT 96.8 59.4 84.6 Auto-CoT 93.2 62.8 91.9 Table 3 compares accuracy on ten datasets from three categories of reasoning tasks. The Zero-Shot and Zero- Shot-CoT results are taken from Kojima et al. [2022], the Few-Shot and Manual-CoT results are taken from Wei et al. [2022a], and the Auto-CoT results are averaged over three random runs. Overall, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Due to the cost of manual designs, Manual-CoT may design the same demonstrations for multiple datasets (e.g., 5/6 of the arithmetic datasets). In contrast, Auto-CoT is more ï¬‚exible and task-adaptive: every single dataset gets its own demonstrations that are automatically constructed. 5.3 Visualization of Question Clustering Figure 5 visualizes question clustering (with PCA projection) in ten datasets. The illustration indicates that there exist generic patterns, where different patterns may be characterized by questions from different clusters. We present the constructed demonstrations of Auto-CoT in Appendix D. #5 AddSub SingleEq Coin Flip  GSM8K Last Letter Concatenation MultiArith StrategyQA AQUA CSQA SVAMP Figure 5: Question clustering on ten datasets of reasoning tasks. Stars denote cluster centers. 85.4 General Effectiveness Using the Codex LLM To evaluate the general effectiveness of Auto-CoT using different LLMs, here we change the LLM to the Codex model [Chen et al., 2021]. As in Table 4, the Codex LLM leads to performance improvement for Manual-CoT when compared with Table 3 that uses the GPT-3 (text-davinci-002) LLM. Nonetheless, using the Codex LLM, the overall performance of Auto-CoT is still competitive compared to Manual-CoT, providing additional empirical evidence for the effectiveness of Auto-CoT. 5.5 Effect of Wrong Demonstrations Recall our discussions in Section 3.3 that there can be wrong demonstrations (whose answers are wrong). To see if diversity mitigates this effect, we design an In-Cluster Sampling baseline that constructs demonstrations by randomly sampling questions from the same cluster that contains a test question. Figure 6 compares accuracy with varying amounts of wrong demonstrations on MultiArith. Compared with In-Cluster Sampling, Auto-CoT (using diversity-based clustering) is less affected by wrong demonstrations: its performance still does not degrade signiï¬cantly even when presented with 50% wrong demonstrations. 12.5% 25.0% 37.5% 50.0% 80 85 90 95 100 Percentage of wrong demonstrations Accuracy (%) In-Cluster Sampling Auto-CoT Figure 6: Effect of wrong demonstrations. 1 2 3 4 5 6 7 8 9 10 60 70 80 90 100 Batch Accuracy (%) Zero-Shot-CoT Manual-CoT Auto-CoT* Figure 7: Bootstraping for the streaming setting. 5.6 More Challenging Streaming Setting CoT studies commonly assume that a full dataset with test questions is given [Wei et al., 2022a, Kojima et al., 2022]. Based on the given dataset, Auto-CoT samples questions to construct the demonstrations. Nonetheless, now we consider a more challenging streaming settingwhere a small batch of test questions (say m questions) arrive at a time like in data streams. To address this challenge, we extend Auto-CoT to a bootstrapping version Auto-CoT*: (i) Initialize an empty set M0; (ii) When batch 1 of questions q(1) 1 , . . . , q(1) m arrive, invoke Zero-Shot-CoT (no clustering due to small m) for each q(1) i to obtain its reasoning chain c(1) i . Add question-chain pairs (q(1) 1 , c(1) 1 ), . . . ,(q(1) m , c(1) m ) to M0 and call the new set M1; (iii) When batch b (b >1) of questions q(b) 1 , . . . , q(b) m arrive, construct demonstrations with existing questions and reasoning chains in Mbâˆ’1 (like Auto-CoT) and use the demonstrations for in-context reasoning for each q(b) i . Add question-chain pairs (q(b) 1 , c(b) 1 ), . . . ,(q(b) m , c(b) m ) to Mbâˆ’1 and call the new set Mb. Figure 7 compares the accuracy on MultiArith at each batch ( m = 30) in this streaming setting (extended version: Figure 11 in the Appendix). As expected, for batch 1, Auto-CoT* and Zero-Shot-CoT obtain equal accuracy. From batch 2, Auto-CoT* performs comparably with Manual-CoT. This result indicates that our method is still effective in the more challenging streaming setting. 6 Conclusion LLMs have shown reasoning capabilities with CoT prompting. The superior performance of Manual-CoT hinges on the hand-crafting of demonstrations. To eliminate such manual designs, we proposed Auto-CoT to automatically construct demonstrations. It samples questions with diversity and generates reasoning chains to construct demonstrations. Experimental results on ten public benchmark reasoning datasets showed that with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. 9References Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html. Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications, 2022. URL https://arxiv.org/abs/2201.08239. Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson dâ€™Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language models: Methods, analysis & insights from training gopher, 2021. URL https://arxiv.org/abs/2112. 11446. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs/2204.02311. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022), 2022a. URL https://arxiv.org/abs/2201.11903. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Thirty-sixth Conference on Neural Information Processing Systems (NeurIPS 2022), 2022. URL https://arxiv.org/abs/2205.11916. Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1743â€“1752, Lisbon, Portugal, 2015. Association for Computational Linguistics. doi: 10.18653/v1/D15-1202. URL https://aclanthology.org/D15-1202. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American 10Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149â€“4158, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/N19-1421. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training veriï¬ers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 158â€“167, Vancouver, Canada, 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1015. URL https://aclanthology.org/P17-1015. Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2080â€“2094, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL https://aclanthology.org/2021.naacl-main.168. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346â€“361, 2021. doi: 10.1162/tacl_a_00370. URL https://doi.org/10.1162/tacl_a_00370. Eric Zelikman, Yuhuai Wu, and Noah D Goodman. Star: Bootstrapping reasoning with reasoning. arXiv preprint arXiv:2203.14465, 2022. URL https://arxiv.org/abs/2203.14465. Denny Zhou, Nathanael SchÃ¤rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022. URL https://arxiv.org/abs/2205.10625. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022a. URL https://arxiv.org/abs/ 2203.11171. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Rationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022b. URL https://arxiv.org/abs/2207.00747. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022. URL https://arxiv.org/abs/2206. 02336. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, page 9, 2019. Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2655â€“2671, 2022. doi: 10.18653/v1/2022.naacl-main.191. URL https://aclanthology.org/2022.naacl-main.191. Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al. Selective annotation makes language models better few-shot learners. arXiv preprint arXiv:2209.01975, 2022. URL https://arxiv.org/abs/2209.01975. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3470â€“3487, 2022. doi: 10.18653/v1/2022.acl-long.244. URL https: //aclanthology.org/2022.acl-long.244. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022b. URL https://openreview.net/forum?id=gEZrGCozdqR. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chafï¬n, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted training enables zero-shot task generalization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4. 11Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form competition: Why the highest probability answer isnâ€™t always right. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7038â€“7051, 2021. doi: 10.18653/v1/2021.emnlp-main.564. URL https: //aclanthology.org/2021.emnlp-main.564. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pages 12697â€“12706, 2021. URL http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf. Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Noisy channel language model prompting for few-shot text classiï¬cation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5316â€“5330, 2022a. doi: 10.18653/v1/2022.acl-long.365. URL https://aclanthology.org/2022.acl-long.365. Jiachang Liu, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100â€“114, 2022a. doi: 10.18653/v1/2022.deelio-1.10. URL https://aclanthology.org/2022.deelio-1.10. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to ï¬nd them: Overcoming few-shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086â€“8098, 2022. doi: 10.18653/v1/2022.acl-long.556. URL https://aclanthology.org/2022.acl-long.556. Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin Raffel. Few-shot parameter-efï¬cient ï¬ne-tuning is better and cheaper than in-context learning. arXiv preprint arXiv:2205.05638, 2022b. URL https://arxiv.org/abs/2205.05638. Albert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2300â€“2344, Seattle, United States, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.167. URL https://aclanthology.org/2022.naacl-main. 167. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022b. URL https://arxiv.org/abs/2202.12837. Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982â€“3992, Hong Kong, China, 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1410. URL https://aclanthology.org/ D19-1410. Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523â€“533, Doha, Qatar, 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1058. URL https://aclanthology.org/D14-1058. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585â€“597, 2015. doi: 10.1162/tacl_a_00160. URL https://aclanthology.org/Q15-1042. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. URL https://arxiv.org/abs/2107.03374. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152â€“1157, San Diego, California, 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL https://aclanthology.org/ N16-1136. 12A Extended analysis for the challenge of Auto-CoT A.1 Impact of demonstration elements. A demonstration is a triple composed by <question, rationale, answer> as shown in Figure 1. We shufï¬‚e either of the demonstration components to see how the performance changes. According to the results in Table 5, shufï¬‚ing questions has the least performance reduction (91.7% â†’73.8%). A possible reason for the decent performance is that the model may capture the rationale-answer mapping patterns. The pattern is expected to reï¬‚ect how the intermediate reasoning steps can lead to the ï¬nal answer. This ï¬nding shows that it is possible to leverage the generated rationales by Zero-Shot-CoT because those rationales are often logically correct even though they may lead to wrong answers [Kojima et al., 2022]. Table 5: Impact of different components in the demonstrations. Method Accuracy Manual-CoT 91.7 Shufï¬‚e Questions 73.8 Shufï¬‚e Rationales 43.8 Shufï¬‚e Answers 17.0 In contrast, shufï¬‚ing either rationales or answers reduces the accuracy signiï¬cantly (91.7% â†’43.8%/17.0%). The observation indicates that the rationale-answer consistency is critical. This kind of mismatch actually happens in Zero-Shot-CoT. An example is shown in Table 6. Using such demonstrations might teach the model illusionâ€”predicting answers without basis. Table 6: Mistakes made by rationale-answer inconsistency in the demonstration. The ï¬rst block shows the demonstration used for the test question in the second block. Q: For the school bake sale Wendy made pastries. She baked 4 cupcakes and 29 cookies. After the sale she had 24 to take back home. How many pastries did she sell? A: Letâ€™s think step by step. Wendy made 4 cupcakes and 29 cookies. She sold some pastries. She had 24 to take back home. How many pastries did she sell?We can set up an equation to solve this problem. Wendy made 4 cupcakes and 29 cookies. She sold some pastries. She had 24 to take back home. How many pastries did she sell?We can set up an equation to solve this problem. 4 cupcakes + 29 cookies - pastries sold = 24 past. The answer is 29. \u0017 Q: A chef needs to cook 13 potatoes. He has already cooked 5. If each potato takes 6 minutes to cook, how long will it take him to cook the rest? A: Letâ€™s think step by step. A chef needs to cook 15 potatoes. He has already cooked 8. If each potato takes 9 minutes to cook, how long will it take him to cook the rest?We can set up an equation to solve this problem. A chef needs to cook 15 potatoes. He has already cooked 8. If each potato takes 9 minutes to cook, how long will it take him to cook the rest?We can set up an equation to solve this problem. 8 potatoes + (15-8) potatoes = 15 potatoes. The answer is 72 minutes. \u0017 A.2 Observation of frequent-error clusters To verify if Zero-Shot-CoT fails at similar problems, we cluster the questions into a few clusters and calculate the error rate of the answers to the questions in each cluster. As shown in Figure 8, the mistakes tend to gather in one or more clusters across different datasets. We observe a similar phenomenon when the cluster number changes, as shown in Figure 9. The phenomenon has shown to be generic that Zero-Shot-CoT may lack some skills to solve some common problems in target tasks. We call the cluster with the highest error rate as a frequent-error cluster. Therefore, the imperfect nature of generated reasoning chains poses risks of retrieving a set of similar questions inside the frequent-error cluster for similarity-based retrieval. B Experimental Details B.1 Tasks and Datasets Our method is evaluated on ten benchmark datasets that cover arithmetic reasoning, commonsense reasoning, and symbolic reasoning tasks. The statistics of the datasets are shown in Table 7. 131 2 3 4 5 6 7 80 20 40 60 Error Rate (%) (a) MultiArith (âˆ†=43) 1 2 3 4 5 6 7 80 20 40 60 Error Rate (%) (b) AddSub (âˆ†=46) 1 2 3 4 5 6 7 80 20 40 60 Error Rate (%) (c) SingleEq (âˆ†=48) 1 2 3 4 5 6 7 30 40 50 Error Rate (%) (d) CSQA (âˆ†=19) Figure 8: Question clustering in different datasets. (âˆ† is computed by the difference of largest and smallest values. 1 20 10 20 30 Error Rate (%) (a) Clusters Num. = 2 1 2 3 40 10 20 30 40 Error Rate (%) (b) Clusters Num. = 4 1 2 3 4 5 60 20 40 60 Error Rate (%) (c) Clusters Num. = 6 1 2 3 4 5 6 7 80 20 40 60 Error Rate (%) (d) Clusters Num. = 8 Figure 9: Question clustering with different numbers of clusters. Arithmetic Reasoning. For arithmetic reasoning, we consider the following six datasets: (i) MultiArith [Roy and Roth, 2015], (ii) GSM8K [Cobbe et al., 2021], (iii) AddSub [Hosseini et al., 2014], (iv) AQUA [Ling et al., 2017], (v) SingleEq [Koncel-Kedziorski et al., 2015], and (vi) SV AMP [Patel et al., 2021]. The ï¬rst three are from the classic Math World Problem Repository [Koncel-Kedziorski et al., 2016], and the last three are from more recent benchmarks. Commonsense Reasoning. For commonsense reasoning, we use (i) CommonsenseQA (CSQA) [Talmor et al., 2019] and (ii) StrategyQA [Geva et al., 2021]. CommonsenseQA asks questions with complex semantics that often require reasoning based on prior knowledge [Talmor et al., 2019]. StrategyQA requires models to infer an implicit multi-hop reasoning to answer questions [Geva et al., 2021]. Symbolic Reasoning. For symbolic reasoning, we use (i) Last Letter Concatenation [Wei et al., 2022a] and (ii) Coin Flip tasks [Wei et al., 2022a]. Last letter Concatenation requires the model to concatenate the last letters of each word. The goal of Coin Flip is to answer whether a coin is still heads up after people either ï¬‚ip or do not ï¬‚ip the coin. Table 7: Dataset Description. Dataset Number of samples Average words Answer Format Licence MultiArith 600 31.8 Number Unspeciï¬ed AddSub 395 31.5 Number Unspeciï¬ed GSM8K 1319 46.9 Number MIT License AQUA 254 51.9 Multiple choice Apache-2.0 SingleEq 508 27.4 Number No License SV AMP 1000 31.8 Number MIT License CSQA 1221 27.8 Multiple choice Unspeciï¬ed StrategyQA 2290 9.6 Yes or No Apache-2.0 Last Letters 500 15.0 String Unspeciï¬ed Coin Flip 500 37.0 Yes or No Unspeciï¬ed B.2 Implementation Details We use GPT-3 [Brown et al., 2020] of the text-davinci-002 version with 175B parameters for the LLM [Ouyang et al., 2022] unless otherwise stated. We select the model because it is public and is widely used to assess the ability of CoT 14reasoning in LLMs [Wei et al., 2022a, Kojima et al., 2022]. The model is accessed via the OpenAI API. 5 Greedy decoding is used to generate the output. We set max_tokens = 256 and temperature = 0. Following Wei et al. [2022a], the number of demonstrations k used for in-context learning is 8 in most tasks, except for 4 in AQuA and Last Letter Concatenation, 7 in CSQA, and 6 in StrategyQA. C Analysis C.1 Comparisons of criteria for sorting questions We compare different ways of sorting questions in each cluster, including: (i) minimal distance to the cluster center (In-Cluster Min Dist, as adopted in Auto-CoT), (ii) maximal distance to the cluster center (In-Cluster Max Dist), and (iii) random sampling inside the cluster (In-Cluster Random). To alleviate the inï¬‚uence of wrong demonstrations, we only sample the demonstrations with correct answers for this analysis. Table 8: Inï¬‚uence of demonstration sampling. Method MultiArith Auto-CoT 93.7 In-Cluster Min Dist 93.7 In-Cluster Random 89.2 In-Cluster Max Dist 88.7 Comparing the results in Table 8, we see that the demonstrations are generally better if they are closer to the cluster center. C.2 Effectiveness of the simple heuristics In Section 4, we apply simple heuristics to encourage the model to use simple and accurate demonstrations. Similar to the criteria of the hand-crafting demonstrations in Wei et al. [2022a], our selection criteria follow simple heuristics to encourage sampling simpler questions and rationales: set the selected demonstration d(i) as d(i) j if it has a question q(i) j with no more than 60 tokens and a rationale r(i) j with no more than 5 reasoning steps.6 For arithmetic reasoning tasks except for AQuA (because it is a multiple-choice problem), we require that a(i) j is not empty and appears in r(i) j 7 to mitigate the risk of rationale-answer mismatches (as we ï¬nd that such mistakes are harmful in Appendix A.1). If the question, rationale, and the answer satisfy the conditions above, then a candidate demonstration d(i) j for the i-th cluster is constructed by concatenating the question, rationale, and answer: [Q: q(i) j , A: r(i) j â—¦a(i) j ]. Table 9: Average mistakes in three runs of demonstration construction. MultiArith AddSub GSM8K AQuA SingleEq SV AMP CSQA Strategy Letter Coin Num. of Demos 8 8 8 4 8 8 7 6 4 8 Simple heuristics 0.3 1.7 1.7 1 1 0.7 2.7 2.3 0 0 w/o heuristics 1.3 5 3 2.7 2 3.3 3.3 2.3 3 1 We run the demonstration construction process three times before and after using simple heuristics to quantify its effect. Table 9 shows the comparison. The simple heuristics reduce the average number of wrong rationales in constructing demonstrations. Figure 10 further depicts the error rate with and without the simple heuristics. The error rate is computed by the average number of wrong rationales divided by the number of demonstrations. We see that our method can keep the error rate below 20% in most tasks (7/10). 5Our experiments are run between July-2022 and September-2022 by using OpenAI API. 6Because Zero-Shot-CoT often uses â€œ\\nâ€ for separating the reasoning steps, the rule can be easily implemented by counting the â€œ\\nâ€ tokens in the generated rationales. 7In arithmetic reasoning tasks, rationales often infer their answers at the last few tokens, as demonstrations examples shown in Appendix D. 15MultiArith AddSub GSM8K AQuA SingleEq SV AMP CSQA Strategy Last Letter Coin Flip 0 20 40 60 80 100 4 20 20 25 12 8 38 38 0 0 16 62 37 66 25 41 47 38 75 12 Error Rate (%) Simple heuristics w/o heuristics Figure 10: Average error rate (%) of sampled demonstrations. C.3 Extended: More Challenging Streaming Setting In Section 5.6, we discussed the application of Auto-CoT in a more challenging streaming settingwhere a small batch of test questions (say m questions) arrive at a time like in data streams. Due to page space limits, we only showed the results of the ï¬rst 10 batches (300 test questions in total) in Section 5.6. In Figure 11, we illustrate the accuracy of each batch on all the 600 test questions in MultiArith. As expected, for batch 1, Auto-CoT* and Zero-Shot-CoT obtain equal accuracy. From batch 2, Auto-CoT* quickly performs comparably with Manual-CoT. This result indicates that our method is still effective in the more challenging streaming setting. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 60 80 90 100 Batch Accuracy (%) Zero-Shot-CoT Manual-CoT Auto-CoT* Figure 11: Bootstrapping for the streaming setting. 16D Appendix: Automatically constructed demonstrations Table 10: Automatically constructed demonstrations for MultiArith. Q: For Halloween Katie and her sister combined the candy they received. Katie had 8 pieces of candy while her sister had 23. If they ate 8 pieces the ï¬rst night, how many pieces do they have left? A: Letâ€™s think step by step. Katie and her sister have a total of 8 + 23 = 31 pieces of candy. If they eat 8 pieces the ï¬rst night, they have 31 - 8 = 23 pieces left. The answer is 23. Q: A pet store had 78 puppies. In one day they sold 30 of them and put the rest into cages with 8 in each cage. How many cages did they use? A: Letâ€™s think step by step. There are 78 puppies. 30 are sold, so that means there are 48 left. 48 divided by 8 is 6, so that means there are 6 cages with 8 puppies in each. The answer is 6. Q: A waiter had 14 customers to wait on. If 3 customers left and he got another 39 customers, how many customers would he have? A: Letâ€™s think step by step. The waiter had 14 customers to wait on. If 3 customers left, that means he would have 11 customers left. If he got another 39 customers, that means he would have 50 customers in total. The answer is 50. Q: Bianca was organizing her book case making sure each of the shelves had exactly 8 books on it. If she had 5 shelves of mystery books and 4 shelves of picture books, how many books did she have total? A: Letâ€™s think step by step. There are 5 shelves of mystery books. Each shelf has 8 books. So thatâ€™s 40 mystery books. There are 4 shelves of picture books. Each shelf has 8 books. So thatâ€™s 32 picture books. 40 + 32 = 72 books. The answer is 72. Q: Wendy uploaded 45 pictures to Facebook. She put 27 pics into one album and put the rest into 9 different albums. How many pictures were in each album? A: Letâ€™s think step by step. First, we know that Wendy uploaded 45 pictures in total. Second, we know that Wendy put 27 pictures into one album. That means that Wendy put the remaining 18 pictures into 9 different albums. That means that each album would have 2 pictures. The answer is 2. Q: A trivia team had 7 members total, but during a game 2 members didnâ€™t show up. If each member that did show up scored 4 points, how many points were scored total? A: Letâ€™s think step by step. There were 7 members on the team, but 2 members didnâ€™t show up. That means that there were 5 members that did show up. Each member that showed up scored 4 points. So if 5 members each scored 4 points, then the total number of points scored would be 5*4=20. The answer is 20. Q: Mike made 69 dollars mowing lawns over the summer. If he spent 24 dollars buying new mower blades, how many 5 dollar games could he buy with the money he had left? A: Letâ€™s think step by step. Mike made $69 from mowing lawns. He spent $24 on new mower blades. That means he has $45 left. Each game costs $5, so he could buy 9 games. The answer is 9. Q: The school cafeteria ordered 8 red apples and 43 green apples for students lunches. But, if only 42 students wanted fruit, how many extra did the cafeteria end up with? A: Letâ€™s think step by step. The school cafeteria ordered 8 red apples and 43 green apples for students lunches. This means that they ordered a total of 51 apples. But, if only 42 students wanted fruit, This means that the school cafeteria would have 9 apples leftover. The answer is 9. 17Table 11: Automatically constructed demonstrations for AddSub. Q: Sally picked 7 lemons and Mary picked 9 lemons from the lemon tree . How many lemons were picked in total ? A: Letâ€™s think step by step. Sally picked 7 lemons and Mary picked 9 lemons. So we can say that Sally picked 7 lemons and Mary picked 9 lemons. So we can say that 7 + 9 = 16 lemons were picked in total. The answer is 16. Q: Gordon bought 3.42 pounds of fruit for a class party . The class ate 2.2 pounds of the fruit . How much fruit is ? A: Letâ€™s think step by step. Gordon bought 3.42 pounds of fruit. The class ate 2.2 pounds of the fruit. That means that there is 3.42 - 2.2 = 1.22 pounds of fruit left. The answer is 1.22. Q: Mary had 21 dimes and 38 pennies in her bank . Her dad borrowed 18 pennies from Mary . How many pennies does she have now ? A: Letâ€™s think step by step. Mary had 21 dimes and 38 pennies in her bank. Her dad borrowed 18 pennies from Mary. So now, Mary has 21 dimes and 20 pennies in her bank. The answer is 20. Q: Joan found 72 seashells and 12 starï¬shes on the beach . She gave Alyssa some of her seashells . She has 28 seashell . How many seashells did she give to Alyssa ? A: Letâ€™s think step by step. Joan found 72 seashells and 12 starï¬shes. She gave Alyssa some of her seashells. So we need to ï¬nd out how many seashells she gave to Alyssa. Joan has 28 seashells. So she gave Alyssa 44 seashells. The answer is 44. Q: Last year , 90171 people were born in a country , and 16320 people immigrated to it . How many new people began living in the country last year ? A: Letâ€™s think step by step. First, 90171 people were born in the country. Then, 16320 people immigrated to the country. That means there were 90171 + 16320 = 106491 new people living in the country last year. The answer is 106491. Q: A construction company is repaving a damaged road . So far , they have repaved a total of 4938 inches of the road . Today , they repaved 805 inches of the road . How many inches of the road had they repaved before today ? A: Letâ€™s think step by step. The company has repaved a total of 4938 inches of the road. Today, they repaved 805 inches of the road. So before today, they had repaved 4938 - 805 = 4133 inches of the road. The answer is 4133. Q: There are 5 scissors and 3 pencils in the drawer . Jason placed 4 scissors in the drawer . How many scissors are now there in total ? A: Letâ€™s think step by step. There are 5 scissors and 3 pencils in the drawer. Jason placed 4 scissors in the drawer. Now, there are 9 scissors in the drawer. The answer is 9. Q: There are 37 short bushes and 30 tall trees currently in the park . Park workers will plant 20 short bushes today . How many short bushes will the park have when the workers are ï¬nished ? A: Letâ€™s think step by step. There are 37 short bushes and 30 tall trees currently in the park. Park workers will plant 20 short bushes today. This means that when the workers are ï¬nished, there will be a total of 57 short bushes in the park. The answer is 57. 18Table 12: Automatically constructed demonstrations for GSM8K. Q: Two trains leave San Rafael at the same time. They begin traveling westward, both traveling for 80 miles. The next day, they travel northwards, covering 150 miles. Whatâ€™s the distance covered by each train in the two days? A: Letâ€™s think step by step. On the ï¬rst day, both trains travel 80 miles. So the ï¬rst train covers 80 miles, and the second train covers 80 miles. On the second day, both trains travel 150 miles. So the ï¬rst train covers 150 miles, and the second train covers 150 miles. In total, the ï¬rst train covers 230 miles, and the second train covers 230 miles. The answer is 230. Q: A tank of water has a depth of 17 feet on Monday. On Tuesday, the tank had 7 feet more water. On Wednesday, the depth of the water is two thirds of what it was on Tuesday. What is the tankâ€™s water depth on Wednesday? A: Letâ€™s think step by step. On Monday, the depth was 17 feet. On Tuesday, the depth was 7 feet more than on Monday, so the depth was 24 feet. On Wednesday, the depth was two thirds of what it was on Tuesday, so the depth was 16 feet. The answer is 16. Q: While at the dollar store, Sloane counts 100 customers entering the store. The next day, she counts 50 more customers than the ï¬rst day. If the total number of customers by the third day was 500, how many customers did she count on the third day? A: Letâ€™s think step by step. On the ï¬rst day, Sloane counted 100 customers. On the second day, she counted 50 more customers than she did on the ï¬rst day. So, she counted 150 customers on the second day. On the third day, she counted 500 customers. Therefore, on the third day, Sloane counted 500 customers. The answer is 500. Q: Laurelâ€™s friend gave her 24 baby outï¬ts that her child no longer needed. At her baby shower, Laurel received twice the amount of new baby outï¬ts. Then, Laurelâ€™s mom gifted her with another 15 baby outï¬ts. How many outï¬ts does she have for her baby? A: Letâ€™s think step by step. Laurelâ€™s friend gave her 24 baby outï¬ts. So she has 24 baby outï¬ts. At her baby shower, Laurel received twice the amount of new baby outï¬ts. So she has 48 baby outï¬ts. Then, Laurelâ€™s mom gifted her with another 15 baby outï¬ts. So she has 63 baby outï¬ts. The answer is 63. Q: Pam and Fred went to a carnival. Pam rode the roller coaster 2 times while Fred rode it 4 times. After that, each of them decided to ride the luge 2 times. If each ride cost 6 tickets, how many tickets did they use that day? A: Letâ€™s think step by step. Pam rode the roller coaster 2 times. So, thatâ€™s 2 * 6 = 12 tickets. Fred rode the roller coaster 4 times. So, thatâ€™s 4 * 6 = 24 tickets. Pam rode the luge 2 times. So, thatâ€™s 2 * 6 = 12 tickets. Fred rode the luge 2 times. So, thatâ€™s 2 * 6 = 12 tickets. In total, they used 12 + 24 + 12 + 12 = 60 tickets. The answer is 60. Q: The eighth-grade class held a bake-off. Kelsie made two times more cookies than Josh. Josh made one-fourth the number of cookies that Suzanne made. If Suzanne made 36 cookies, how many did Kelsie make? A: Letâ€™s think step by step. Josh made one-fourth the number of cookies that Suzanne made. So if Suzanne made 36 cookies, Josh made 9 cookies. Kelsie made two times more cookies than Josh. So if Josh made 9 cookies, Kelsie made 18 cookies. The answer is 18. Q: Jane has saved $4 of her allowance every week for the past 8 weeks. If she continues to save the same amount each week, how many more weeks will it take for her to have saved a total of $60? A: Letâ€™s think step by step. Jane has saved $4 of her allowance every week for the past 8 weeks. That means she has $32 in savings so far. $60 - $32 = $28 $28 / $4 = 7 It will take Jane 7 more weeks to have saved a total of $60. The answer is 7. Q: Mia and Emma are currently 16 years apart in age. If Mia, who is younger than Emma, is 40 years old, whatâ€™s the average of their ages? A: Letâ€™s think step by step. Mia is currently 16 years old. Mia is younger than Emma. Mia is 40 years old. Therefore, Emma must be 40 + 16 = 56 years old. The average of their ages is (40 + 56)/2 = 48 years old. The answer is 48. 19Table 13: Automatically constructed demonstrations for AQuA. Q: Julieâ€™s yard is rectangular. One side of the yard is 100 feet wide. The total area of the yard is 3,000 square feet. What is the length of the other side of the yard? Answer Choices: (A) 30 feet (B) 20 feet (C) 10 feet (D) 50 feet (E) 60 feet A: Letâ€™s think step by step. We know that the yard is rectangular, so it has four sides. We also know that one side of the yard is 100 feet wide. The total area of the yard is 3,000 square feet. We can set up the equation: 3,000 = 100x x = 30 So, the length of the other side of the yard is 30 feet. The answer is A. Q: In a bag of red and green sweets, the ratio of red sweets to green sweets is 3:4. If the bag contains 120 green sweets, how many red sweets are there? Answer Choices: (A) 90 (B) 80 (C) 95 (D) 100 (E) 85 A: Letâ€™s think step by step. There are 3 red sweets for every 4 green sweets. That means that for every 12 sweets, there are 9 red sweets. That means that for every 120 green sweets, there are 90 red sweets. The answer is (A). The answer is A. Q: A car ï¬nishes a journey in 20 hours at the speed of 60 km/hr. If the same distance is to be covered in 10 hours, how much speed does the car gain? Answer Choices: (A) 80 kmph (B) 50 kmph (C) 120 kmph (D) 70 kmph (E) 80 kmph A: Letâ€™s think step by step. The car is currently travelling at 60 km/hr. In order to travel the same distance in 10 hours, the car would need to travel at 6 times the speed, or 360 km/hr. Therefore, the car would need to gain 300 km/hr. The answer is C. The answer is C. Q: Mark told John \"If you give me half your money I will have Rs.75. John said, \"if you give me one third of your money, I will have Rs.75/-. How much money did John have ? Answer Choices: (A) 22 (B) 60 (C) 28 (D) 26 (E) 18 A: Letâ€™s think step by step. If Mark has Rs.75, then John has Rs.75 as well. If Mark gives John half his money, then Mark will have Rs.37.50 and John will have Rs.112.50. However, this isnâ€™t one of the answer choices. If Mark gives John one third of his money, then Mark will have Rs.50 and John will have Rs.100. This is one of the answer choices, so John must have had Rs.100 to begin with. The answer is D. 20Table 14: Automatically constructed demonstrations for SingleEq. Q: Markâ€™s father gave him $85. Mark bought 10 books, each of which cost $5. How much money does Mark have left? A: Letâ€™s think step by step. Markâ€™s father gave him $85. Mark bought 10 books, each of which cost $5. That means that Mark spent $50 on books. So Mark has $85 - $50 = $35 left. The answer is 35. Q: Tim has 44 books. Sam has 52 books. How many books do they have together ? A: Letâ€™s think step by step. Step 1: How many books does Tim have? Step 2: How many books does Sam have? Step 3: How many books do they have together? Tim has 44 books. Sam has 52 books. They have 96 books together. The answer is 96. Q: There are 22 walnut trees currently in the park. Park workers will plant more walnut trees today. When the workers are ï¬nished there will be 55 walnut trees in the park. How many walnut trees did the workers plant today ? A: Letâ€™s think step by step. There are currently 22 walnut trees in the park. The workers will plant more walnut trees today. When the workers are ï¬nished, there will be 55 walnut trees in the park. The workers planted 33 walnut trees today. The answer is 33. Q: Karen added 0.25 of a cup of walnuts to a batch of trail mix. Later, she added 0.25 of a cup of almonds. How many cups of nuts did Karen put in the trail mix in all? A: Letâ€™s think step by step. Karen added 0.25 of a cup of walnuts to a batch of trail mix. This is the ï¬rst step, and so far, she has added 0.25 cups of walnuts. Later, she added 0.25 of a cup of almonds. This is the second step, and so far, she has added 0.5 cups of nuts (0.25 cups of walnuts + 0.25 cups of almonds). Therefore, in total, Karen has added 0.5 cups of nuts to the trail mix. The answer is 0.5. Q: Sara, Keith, Benny, and Alyssa each have 96 baseball cards. How many dozen baseball cards do they have in all? A: Letâ€™s think step by step. There are 12 dozens in a gross, so we need to divide 96 by 12. 96/12=8 Therefore, they have 8 dozens of baseball cards in all. The answer is 8. Q: Craig walked 0.2 of a mile from school to Davidâ€™s house and 0.7 of a mile from Davidâ€™s house to his own house. How many miles did Craig walk in all? A: Letâ€™s think step by step. Craig walked 0.2 miles from school to Davidâ€™s house. Then, Craig walked 0.7 miles from Davidâ€™s house to his own house. In total, Craig walked 0.2 + 0.7 = 0.9 miles. The answer is 0.9. Q: Tom found 5 seashells on the beach. he gave Jessica 2 of the seashells. How many seashells does he now have ? A: Letâ€™s think step by step. Tom found 5 seashells. Tom gave Jessica 2 seashells. Tom now has 3 seashells. The answer is 3. Q: Brendaâ€™s mother made cookies for 5. If she prepared 35 cookies and each of them had the same number of cookies, how many did each of them have? A: Letâ€™s think step by step. Brendaâ€™s mother made 35 cookies. We know that she made them for 5 people. That means that she made 7 cookies for each person. The answer is 7. 21Table 15: Automatically constructed demonstrations for SV AMP. Q: Marco and his dad went strawberry picking. Marcoâ€™s dadâ€™s strawberries weighed 11 pounds. If together their strawberries weighed 30 pounds. How much did Marcoâ€™s strawberries weigh? A: Marcoâ€™s dadâ€™s strawberries weigh 11 pounds. If together their strawberries weigh 30 pounds, then Marcoâ€™s strawberries weigh 19 pounds. The answer is 19. Q: 19 red peaches, 11 yellow peaches and 12 green peaches are in the basket. How many more red peaches than yellow peaches are in the basket? A: There are 19 red peaches and 11 yellow peaches. 19-11=8 There are 8 more red peaches than yellow peaches. The answer is 8. Q: Baker made 144 cakes. He sold 71 of them. Then he made 111 more cakes. How many more cakes did baker make than those he sold? A: First, he made 144 cakes and sold 71 of them. So, he has 144-71 = 73 cakes left. Then, he made 111 more cakes. So, he has 73 + 111 = 184 cakes in total. Now, we need to ï¬nd out how many more cakes he made than those he sold. There are 184 cakes in total. He sold 71 of them. So, he made 184-71 = 113 more cakes than those he sold. The answer is 113. Q: Bryan took a look at his books as well. If he has a total of 42 books and each bookshelf contains 2 books How many bookshelves does he have? A: We know that Bryan has 42 books in total. We know that each bookshelf has 2 books. From this, we can infer that Bryan has 21 bookshelves. The answer is 21. Q: In a school there are 902 girls and 811 boys. 44 more girls joined the school. How many girls are there in the school now? A: There are 902 girls and 811 boys in the school. 44 more girls joined the school. That means there are now (902 + 44) = 946 girls in the school. The answer is 946. Q: He also had 26 aquariums for saltwater animals. The aquarium had 52 saltwater animals in total and every aquarium had the same number of animals. How many saltwater animals does each aquarium have? A: We know that there are 26 aquariums and 52 animals in total. That means that if we divide 52 by 26, we will know how many animals are in each aquarium. 52 divided by 26 equals 2. So, each aquarium has 2 saltwater animals. The answer is 2. Q: A mailman has to give 25 pieces of junk mail to each block. If he gives 5 mails to each house in a block How many houses are there in a block? A: The mailman has to give 25 pieces of junk mail to each block. That means if the mailman gives 5 pieces of junk mail to each house, he would have given away 25 pieces of junk mail after visiting 5 houses. Therefore, if the mailman has to give 25 pieces of junk mail to each block, he would have to visit 5 houses in a block. The answer is 5. Q: Every day Ryan spends 3 hours on learning english and some more hours on learning chinese. If he spends a total of 4 hours on learning english and chinese everyday How many hours does he spend on learning chinese? A: First, we know that Ryan spends 3 hours on learning English every day. Second, we know that Ryan spends a total of 4 hours on learning English and Chinese every day. From this, we can infer that Ryan spends 1 hour on learning Chinese every day. The answer is 1. 22Table 16: Automatically constructed demonstrations for CSQA. Q: Where can you ï¬nd a dogs house? Answer Choices: (A) found outside (B) faithful (C) frightening (D) cold (E) four legs A: Letâ€™s think step by step. A dog needs a house to protect it from the weather and predators. The house should be big enough for the dog to move around in, but not too big or the dog will be cold. The house should have a door to keep out the cold and a roof to keep out the rain. The house should be made of sturdy materials that will not collapse in the wind. The best place to ï¬nd a dog house is at a pet store or a home improvement store. You can also ï¬nd dog houses online. The answer is A. Q: Where would you ï¬nd a ticket booth and see a concert? Answer Choices: (A) clerk (B) indoors (C) movie theater (D) venue (E) auditorium A: Letâ€™s think step by step. You would go to a ticket booth to purchase a ticket for a concert. The concert would take place at a venue, which could be indoors or outdoors. It is most likely that the concert would take place indoors at a large auditorium or a movie theater. The answer is D. Q: What do people aim to do at work? Answer Choices: (A) complete job (B) learn from each other (C) kill animals (D) wear hats (E) talk to each other A: Letâ€™s think step by step. People usually go to work in order to earn money, which they can use to buy things they want or need. In order to do this, they must complete their job. This usually means working with others in order to get the job done efï¬ciently. Sometimes, people learn new things from their coworkers. For example, they might learn a new skill or how to do their job better. Finally, people usually talk to their coworkers during their break or after work. The answer is A. Q: Where do you store a large container? Answer Choices: (A) supermarket (B) factory (C) juice (D) hostel (E) cabinet A: Letâ€™s think step by step. We need to store a large container. It wonâ€™t ï¬t in a cabinet, so thatâ€™s not the answer. It also wonâ€™t ï¬t in a juice container, so thatâ€™s not the answer. It could ï¬t in a factory, but thatâ€™s not necessarily where you would store it. It could also ï¬t in a supermarket, but thatâ€™s not necessarily where you would store it. The most likely answer is a hostel. The answer is D. Q: If there is a pond with trees around it, where it it likely located? Answer Choices: (A) ground (B) bathroom (C) forest (D) countryside (E) rural area A: Letâ€™s think step by step. A pond is likely to be located near trees because trees need water to survive. Therefore, the most likely location for a pond with trees around it is in a forest. The answer is C. Q: James was meeting a friend. They had planed a slow day. They didnâ€™t want to do much. They just wanted what? Answer Choices: (A) relaxation (B) panic (C) alarm (D) joy (E) cheer A: Letâ€™s think step by step. James was meeting a friend. They had planned a slow day. They didnâ€™t want to do much. They just wanted _____. The only logical answer choice is relaxation. The answer is A. Q: what does a person do to stay healthy? Answer Choices: (A) fever (B) eat every day (C) excited (D) headache (E) expressive A: Letâ€™s think step by step. To stay healthy, a person should probably do things like eating healthy foods, exercising, and getting enough rest. So the answer choices that best ï¬t are (B) eat every day, (C) excited, and (D) headache. The answer is B. 23Table 17: Automatically constructed demonstrations for StrategyQA. Q: Would a vegetarian be able to eat something at Chick-ï¬l-A? A: Letâ€™s think step by step. Chick-ï¬l-A is a restaurant chain that specializes in chicken sandwiches. So right off the bat, we can say that a vegetarian probably wouldnâ€™t be able to ï¬nd a sandwich that they could eat. However, Chick-ï¬l-A does offer salads and sides that a vegetarian could eat. The answer is yes. Q: Can Justin Timberlake ride Shipwreck Falls at Six Flags? A: Letâ€™s think step by step. First, letâ€™s consider the height requirement for Shipwreck Falls, which is 48 inches. Next, weâ€™ll look up Justin Timberlakeâ€™s height. According to various sources, Timberlake is between 5â€™8\" and 5â€™11\". Finally, weâ€™ll compare Timberlakeâ€™s height to the height requirement for Shipwreck Falls. Since Timberlake is taller than 48 inches, he would be able to ride Shipwreck Falls. The answer is yes. Q: Would a snake have reasons to fear a honey badger? A: Letâ€™s think step by step. A snakeâ€™s main method of attack is to coil around an animal and constrict it until it suffocates. This would be difï¬cult to do to a honey badger, which has thick skin and is very agile. In addition, honey badgers are known to be very aggressive, and they have sharp claws and teeth that could do a lot of damage to a snake. So, it is reasonable to say that a snake would have reasons to fear a honey badger. The answer is yes. Q: Will Queen Elizabeth be buried in the Pantheon? A: Letâ€™s think step by step. First, the Pantheon is a church, so itâ€™s possible that she could be buried there. Second, Queen Elizabeth II is still alive, so she hasnâ€™t been buried yet. Third, even if she were to be buried in the Pantheon, itâ€™s unlikely that we would know about it ahead of time, so itâ€™s hard to say for sure. The answer is no. Q: Can you see the moon in Wembley Arena? A: Letâ€™s think step by step. Wembley Arena is an indoor venue, so the moon would only be visible if there was a hole in the roof. Even then, the moon would only be visible during the night. So the answer is probably no. The answer is no. Q: Did the 40th president of the United States forward lolcats to his friends? A: Letâ€™s think step by step. The 40th president of the United States was Ronald Reagan. There is no evidence that Ronald Reagan ever forwarded lolcats to his friends. The answer is no. Table 18: Automatically constructed demonstrations for Last Letter Concatenation. Q: Take the last letters of each words in \"Lesley Luna Nadia Adriana\" and concatenate them. A: Letâ€™s think step by step. The last letter of \"Lesley\" is \"y\". The last letter of \"Luna\" is \"a\". The last letter of \"Nadia\" is \"a\". The last letter of \"Adriana\" is \"a\". So the ï¬nal answer is \"yaaa\". The answer is yaaa. Q: Take the last letters of each words in \"Alfonso Collin Amado Dick\" and concatenate them. A: Letâ€™s think step by step. The last letter of \"Alfonso\" is \"o\". The last letter of \"Collin\" is \"n\". The last letter of \"Amado\" is \"o\". The last letter of \"Dick\" is \"k\". So the ï¬nal answer would be \"onok\". The answer is onok. Q: Take the last letters of each words in \"Cristina Saad Bridget Katie\" and concatenate them. A: Letâ€™s think step by step. The last letter of \"Cristina\" is \"a\". The last letter of \"Saad\" is \"d\". The last letter of \"Bridget\" is \"t\". The last letter of \"Katie\" is \"e\". So the ï¬nal answer would be \"adte\". The answer is adte. Q: Take the last letters of each words in \"Idalia Arnoldo Marla Duane\" and concatenate them. A: Letâ€™s think step by step. The last letter of \"Idalia\" is \"a\". The last letter of \"Arnoldo\" is \"o\". The last letter of \"Marla\" is \"a\". The last letter of \"Duane\" is \"e\". So the ï¬nal answer would be \"aoae\". The answer is aoae. 24Table 19: Automatically constructed demonstrations for Coin Flip. Q: A coin is heads up. Shari ï¬‚ips the coin. Bella ï¬‚ips the coin. Liza does not ï¬‚ip the coin. Maira does not ï¬‚ip the coin. Is the coin still heads up? Note that \"ï¬‚ip\" here means \"reverse\". A: Letâ€™s think step by step. Shari ï¬‚ips the coin. The coin is now tails up. Bella ï¬‚ips the coin. The coin is now heads up. Liza does not ï¬‚ip the coin. The coin is still heads up. Maira does not ï¬‚ip the coin. The coin is still heads up. The answer is yes. Q: A coin is heads up. Joyce ï¬‚ips the coin. Leroy ï¬‚ips the coin. Alyssa does not ï¬‚ip the coin. Maggie does not ï¬‚ip the coin. Is the coin still heads up? Note that \"ï¬‚ip\" here means \"reverse\". A: Letâ€™s think step by step. Joyce ï¬‚ips the coin, so now it is tails up. Leroy ï¬‚ips the coin, so now it is heads up again. Alyssa does not ï¬‚ip the coin, so it remains tails up. Maggie does not ï¬‚ip the coin, so it remains heads up. So the ï¬nal answer is yes, the coin is still heads up. The answer is yes. Q: A coin is heads up. Ubaldo does not ï¬‚ip the coin. Katrina ï¬‚ips the coin. Francis does not ï¬‚ip the coin. Lynn ï¬‚ips the coin. Is the coin still heads up? Note that \"ï¬‚ip\" here means \"reverse\". A: Letâ€™s think step by step. Ubaldo does not ï¬‚ip the coin, so the coin remains heads up. Katrina ï¬‚ips the coin, so the coin is now tails up. Francis does not ï¬‚ip the coin, so the coin remains tails up. Lynn ï¬‚ips the coin, so the coin is now heads up again. The answer is yes. Q: A coin is heads up. Carmelo ï¬‚ips the coin. Tony ï¬‚ips the coin. Susan ï¬‚ips the coin. Sara does not ï¬‚ip the coin. Is the coin still heads up? Note that \"ï¬‚ip\" here means \"reverse\". A: Letâ€™s think step by step. Carmelo ï¬‚ips the coin. The coin is now tails up. Tony ï¬‚ips the coin. The coin is now heads up. Susan ï¬‚ips the coin. The coin is now tails up. Sara does not ï¬‚ip the coin. The coin is still tails up. The answer is no. Q: A coin is heads up. Paulo ï¬‚ips the coin. Tatyana ï¬‚ips the coin. Bernice does not ï¬‚ip the coin. RaÃºl ï¬‚ips the coin. Is the coin still heads up? Note that \"ï¬‚ip\" here means \"reverse\". A: Letâ€™s think step by step. Paulo ï¬‚ips the coin. This means that the coin is now tails up. Tatyana ï¬‚ips the coin. The coin is now heads up again. Bernice does not ï¬‚ip the coin. The coin remains heads up. RaÃºl ï¬‚ips the coin. The coin is now tails up again. The answer is no. Q: A coin is heads up. Claudia ï¬‚ips the coin. Cole does not ï¬‚ip the coin. Matthew does not ï¬‚ip the coin. Juan Pablo does not ï¬‚ip the coin. Is the coin still heads up? Note that \"ï¬‚ip\" here means \"reverse\". A: Letâ€™s think step by step. Claudia ï¬‚ips the coin, so it is now tails up. Cole does not ï¬‚ip the coin, so it remains tails up. Matthew does not ï¬‚ip the coin, so it remains tails up. Juan Pablo does not ï¬‚ip the coin, so it remains tails up. So the answer is no, the coin is not heads up. The answer is no. Q: A coin is heads up. Aj does not ï¬‚ip the coin. Jd ï¬‚ips the coin. Maddie does not ï¬‚ip the coin. Francisca does not ï¬‚ip the coin. Is the coin still heads up? Note that \"ï¬‚ip\" here means \"reverse\". A: Letâ€™s think step by step. Aj does not ï¬‚ip the coin, so the coin remains heads up. Jd ï¬‚ips the coin, so the coin is now tails up. Maddie does not ï¬‚ip the coin, so the coin remains tails up. Francisca does not ï¬‚ip the coin, so the coin remains tails up. So, the ï¬nal answer is that the coin is tails up. The answer is no. Q: A coin is heads up. Albert does not ï¬‚ip the coin. Felicia does not ï¬‚ip the coin. Margo ï¬‚ips the coin. Patty does not ï¬‚ip the coin. Is the coin still heads up? Note that \"ï¬‚ip\" here means \"reverse\". A: Letâ€™s think step by step. Albert does not ï¬‚ip the coin, so the coin remains heads up. Felicia does not ï¬‚ip the coin, so the coin remains heads up. Margo ï¬‚ips the coin, so the coin is now tails up. Patty does not ï¬‚ip the coin, so the coin remains tails up. The answer is no. 25",
      "references": [
        "Language models are few-shot learners.",
        "Lamda: Language models for dialog applications",
        "Scaling language models: Methods, analysis & insights from training gopher",
        "Palm: Scaling language modeling with pathways",
        "Chain of thought prompting elicits reasoning in large language models.",
        "Large language models are zero-shot reasoners.",
        "Solving general arithmetic word problems.",
        "CommonsenseQA: A question answering challenge targeting commonsense knowledge.",
        "Training verifiers to solve math word problems",
        "Program induction by rationale generation: Learning to solve and explain algebraic word problems.",
        "Are NLP models really able to solve simple math word problems?",
        "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies.",
        "Star: Bootstrapping reasoning with reasoning.",
        "Least-to-most prompting enables complex reasoning in large language models.",
        "Self-consistency improves chain of thought reasoning in language models.",
        "Rationale-augmented ensembles in language models.",
        "On the advance of making language models better reasoners.",
        "Language models are unsupervised multitask learners.",
        "Learning to retrieve prompts for in-context learning.",
        "Selective annotation makes language models better few-shot learners.",
        "Cross-task generalization via natural language crowdsourcing instructions.",
        "Finetuned language models are zero-shot learners.",
        "Multitask prompted training enables zero-shot task generalization.",
        "Surface form competition: Why the highest probability answer isnâ€™t always right.",
        "Calibrate before use: Improving few-shot performance of language models.",
        "Noisy channel language model prompting for few-shot text classiï¬cation.",
        "What makes good in-context examples for gpt-3?",
        "Fantastically ordered prompts and where to ï¬nd them: Overcoming few-shot prompt order sensitivity.",
        "Few-shot parameter-efï¬cient ï¬ne-tuning is better and cheaper than in-context learning.",
        "Do prompt-based models really understand the meaning of their prompts?",
        "Rethinking the role of demonstrations: What makes in-context learning work?",
        "Sentence-BERT: Sentence embeddings using Siamese BERT-networks.",
        "Learning to solve arithmetic word problems with verb categorization.",
        "Parsing algebraic word problems into equations.",
        "Training language models to follow instructions with human feedback,",
        "Evaluating large language models trained on code.",
        "MAWPS: A math word problem repository."
      ],
      "meta_data": {
        "arxiv_id": "2210.03493v1",
        "authors": [
          "Zhuosheng Zhang",
          "Aston Zhang",
          "Mu Li",
          "Alex Smola"
        ],
        "published_date": "2022-10-07T12:28:21Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses the bottleneck of manual chain-of-thought (CoT) few-shot demonstrations (Manual-CoT) by proposing Auto-CoT, a fully automatic way to construct CoT demonstrations (question + rationale + answer) using an LLM itself. Key contributions: (1) empirical/diagnostic analysis showing similarity-based retrieval of demo questions can hurt when rationales are self-generated (\"misleading by similarity\"), because errors concentrate in semantic clusters (frequent-error clusters); (2) a diversity-driven demo construction algorithm that clusters questions and samples one representative per cluster to reduce correlated reasoning mistakes; (3) simple, task-agnostic heuristics to filter for simpler questions/rationales to further reduce bad demonstrations; (4) strong empirical results on 10 reasoning benchmarks showing Auto-CoT matches/exceeds manually crafted CoT prompting with GPT-3, plus an online/streaming bootstrapped variant (Auto-CoT*) that improves as more questions arrive.",
        "methodology": "Auto-CoT is a two-stage pipeline for automatically building k in-context CoT demonstrations from an unlabeled pool of questions: (1) Question clustering for diversity: encode each question with Sentence-BERT, average token embeddings to get a fixed vector, then run k-means to partition questions into k clusters (k equals the number of demos). Within each cluster, sort questions by distance to the cluster centroid. (2) Demonstration sampling + rationale generation: for each cluster, iterate from centroid-nearest questions, and generate a rationale/answer using Zero-Shot-CoT by prompting the LLM with \"Letâ€™s think step by step\". Select the first candidate meeting simple heuristics (e.g., question <= 60 tokens; rationale <= 5 reasoning steps; for arithmetic tasks, enforce basic answer/rationale consistency). Concatenate the k selected demos and append the test query with the same CoT trigger to perform in-context reasoning. The paper also proposes Auto-CoT* for streaming: initialize with zero-shot generated chains for the first batch, then cluster/sample from accumulated prior batches to build demos for subsequent batches.",
        "experimental_setup": "Models: primarily GPT-3 text-davinci-002 (175B) via OpenAI API with greedy decoding, max_tokens=256, temperature=0; additional validation on Codex code-davinci-002. Benchmarks (10 total): Arithmeticâ€”MultiArith, GSM8K, AddSub, AQUA-RAT (multiple choice), SingleEq, SVAMP; Commonsenseâ€”CommonsenseQA (CSQA), StrategyQA; Symbolicâ€”Last Letter Concatenation, Coin Flip. Prompting configurations: number of demonstrations k follows prior CoT workâ€”typically 8, except AQuA and Last Letter (4), CSQA (7), StrategyQA (6). Baselines: Zero-Shot, Zero-Shot-CoT, Few-Shot (no rationales in demos), Manual-CoT (handwritten rationales). Metrics/validation: task accuracy on test sets; Auto-CoT results averaged over 3 random runs of demo construction; targeted analyses include (i) unresolving rate on instances where Zero-Shot-CoT fails, (ii) cluster-wise error rates to reveal frequent-error clusters, (iii) controlled corruption with varying percentages of wrong demonstrations, and (iv) streaming evaluation over sequential batches (m=30) for Auto-CoT*.",
        "limitations": "(1) Reliance on an unlabeled pool of questions to cluster and sample from; the default setting assumes access to the full set of test questions (transductive use), and the streaming variant needs enough accumulated questions before performance stabilizes. (2) Quality is bounded by the base LLMâ€™s Zero-Shot-CoT ability; self-generated rationales can be wrong or contain rationaleâ€“answer inconsistencies, and filtering via simple length/step heuristics is brittle and task-dependent. (3) Clustering uses Sentence-BERT + k-means with a fixed k tied to the demo budget; performance may depend on embedding quality, k choice, and cluster separability, with no guarantees. (4) Demonstration generation requires multiple LLM calls, which can be expensive and latency-heavy. (5) Evaluation focuses on a limited set of reasoning benchmarks and GPT-3/Codex-era APIs; results may not generalize to domains needing external knowledge/tools, long-context reasoning, or adversarially constructed problems. (6) Error analyses show correlated failure modes (frequent-error clusters) but Auto-CoT does not explicitly learn or repair missing skills; it mainly reduces exposure to correlated errors via diversity.",
        "future_research_directions": "(1) Better demo validation/selection: incorporate automatic verification (e.g., self-consistency voting, verifier models, symbolic checkers for arithmetic) to filter incorrect rationales rather than relying on length heuristics. (2) Adaptive diversity strategies: dynamic or nonparametric clustering, submodular/coverage-based sampling, or uncertainty-aware sampling to avoid frequent-error regions while maintaining skill coverage. (3) End-to-end optimization of demo construction: learn policies for selecting questions/prompts/order, possibly via reinforcement learning or Bayesian optimization over ICL configurations. (4) Robust online Auto-CoT: stronger bootstrapping in streaming settings (warm-start from external unlabeled corpora, continual clustering, forgetting/outlier handling). (5) Extend beyond benchmarks: apply to tool-augmented reasoning, retrieval-augmented generation, planning, code/math with execution feedback, and multimodal tasks. (6) Cost/efficiency: cache/reuse generated rationales, compress demonstrations, distill Auto-CoT prompts into smaller models, or exploit longer-context models to increase diversity without more calls.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Chain of Thought Empowers Transformers to Solve Inherently Serial Problems",
      "full_text": "Who Would be Interested in Services? An Entity Graph Learning System for User Targeting Dan Yang âˆ—, Binbin Hu âˆ—, Xiaoyan Yang âˆ—, Yue Shen, Zhiqiang Zhang, Jinjie Gu â€ , Guannan Zhang Ant Group, China {luoyin.yd, joyce.yxy}@antgroup.com,{bin.hbb, zhanying, lingyao.zzq, jinjie.gujj, zgn138592 }@antfin.com Abstractâ€”With the growing popularity of various mobile devices, user targetinghas received a growing amount of attention, which aims at effectively and efficiently locating target users that are interested in specific services. Most pioneering works for user targeting tasks commonly perform similarity-based expansion with a few active users as seeds, suffering from the following major issues: the unavailability of seed users for new- coming services and the unfriendliness of black-box procedures towards marketers. In this paper, we design an E ntity G raph Learning (EGL) system to provide explainable user targeting ability meanwhile applicable to addressing the cold-start issue. EGL System follows the hybrid online-offline architecture to satisfy the requirements of scalability and timeliness. Specifically, in the offline stage, the system focuses on the heavyweight entity graph construction and user entity preference learning, in which we propose a T hree-stage R elation M ining P rocedure (TRMP), breaking loose from the expensive seed users. At the online stage, the system offers the ability of user targeting in real- time based on the entity graph from the offline stage. Since the user targeting process is based on graph reasoning, the whole process is transparent and operation-friendly to marketers. Finally, extensive offline experiments and online A/B testing demonstrate the superior performance of the proposed EGL System. Index Termsâ€”user targeting, graph neural networks, entity graph construction, contrastive learning I. I NTRODUCTION The innovative mobile economy has served as a competitive market to provide internet companies ( e.g., Google, Tencent, and Alipay) with a variety of opportunities to promote their products and services. Alipay has already become a platform for enabling inclusive, convenient digital life and digital fi- nancial services for consumers. Aiming at effectively and efficiently locating target users that are interested in certain services, user targeting [1]â€“[6] has received a growing amount of attention, since its potential ability to derive high-quality users is well-aligned with marketersâ€™ needs for both facilitating the conversion population and reducing the operation costs. Roughly speaking, current approaches devoted to user tar- geting mainly fall into two lines. The first type denotes the rule-based methods (Fig. 1 (a)) following the service-centered design, which targets users with prefabricated domain knowl- edge [1], [2], i.e., tag mining and rule expression. As a com- parison, the look-alike based methods (Fig. 1 (b)) seek to learn âˆ— Equal contributionsâ€  Corresponding author high-quality representations of seed users, and the target users can be effectively matched in the embedding space [3]â€“[6]. Owing to the powerful ability of representation learning for massive historical data summarization, the latter user-centered methods usually achieve better performance. Unfortunately, they are still distant from optimal or even satisfactory in real scenarios, facing the following major issues: i) New services appear every day, causing the unavailability of seed users for corresponding services. Besides, insufficient seed users may easily have coverage bias [5]. ii) The interpretability of user targeting is essential. Most look-alike based systems [3], [6] utilize black-box algorithms to generate target user sets. Such an operation-unfriendly manner seems detrimental to the subsequent iteration of user targeting for marketers. To fill this gap, we come up with a novel E ntity G raph Learning System ( EGL System) for user targeting issues. As exhibited in Fig. 1 (c), given several phrases related to a spe- cific service ( i.e., â€œNBAâ€ in Fig. 1 (c)), EGL System extends their connections iteratively along a well-established entity graph to discover their hierarchical relations ( e.g., â€œNBAâ€ â†’ â€œJamesâ€ â†’ â€œThe Lakersâ€ in Fig. 1 (c)). Based on the set of k-hop relevant entities, EGL System locates the target users with explicit preferences towards these candidate entities. EGL System performs cognitive reasoning upon entity graphs w.r.t. service-related phrases in an automatic manner, such that i) service-based tag mining or seed users are not necessary for marketing the service, and; ii) entity graph based reasoning offers intuitive explanations for user targeting, as well as an interactive environment for marketers to flexibly control the depth of entity extension. Constructing a high-quality entity graph is at the core of the EGL System, and given such a graph, it is critical to store and access relational knowledge of similar entities efficiently. However, the entity relation mining process is non-trivial, given three intractable challenges: i) The process of filtering undesired relations between entities is expected to be adaptive. ii) The negative sampling, the core of the model learning, is desired to be semantically augmented. iii) The entity relation mining procedure requires the stability of prediction due to the fluctuation of the data source. To address these challenges, we propose a T hree-stage R elation M ining Procedure (TRMP). In particular, we aim at gathering as many similar relations between entities as possible in the Candidate arXiv:2305.18780v1  [cs.LG]  30 May 2023Seed users Service system Service related phrases NBA Basketball Encoder â€¦ Embedding Search (a) (b) (c) NBA Spalding Basketball Soccer Yao Ming James CBA The Lakers Fig. 1: Comparison of three modes of user targeting Service System RequestEntity word Extractor Entity Graph Reasoning Marketers Selection Target Customers Entity Graph Entity Sequence Extractor TRMP Online Offline User Entity Preference Candidate Generation Ranking Ensemble Fig. 2: EGL System diagram, consisting of an offline pipeline and an online serving procedure. Generation Stage , equipped with Skip-gram [7] based co- occurrence behaviors modeling and BERT [8] based semantic mining module. In terms of the subsequent Ranking Stage, as the key component of TRMP, we endow the powerful ability of reliable relation filtering with A daptive threshold L ink Prediction with C ontrastive learning model (ALPC) based on graph neural networks [9]â€“[12] and the semantic augmented contrastive strategy [13], [14]. To maintain the stability of the entity mining process in the daily services, we present an Ensemble Stage to integrate multiple entity representations derived from several well-trained ranking models with a multi- head attention encoder, such that the whole TRMP is more robust than one single ranking model. To our knowledge, EGL System is the first user target- ing system that automatically matches target users inter- ested in services with efficient cognitive reasoning over well- established entity graphs, breaking loose from the expensive seed users and the black-box manner. We demonstrate the superiority of the proposed TRMP through extensive exper- iments on real-world datasets. Moreover, an in-depth analysis of online experiments also shows that our EGL System is effective, explainable, and operation-friendly. II. S YSTEM OVERVIEW In this section, we present the overview of the EGL System, following a hybrid online-offline architecture, shown in Fig. 2. A. Offline Stage The bottom part of Fig. 2 shows the offline pipeline of the EGL System: Entity sequence extractor â†’ TRMP: Relation mining procedure â†’ Entity graph storage system â†’ User entity preference (User preference generator towards entity). Specifically, the entity sequence extractor is responsible for collecting and preprocessing the data source ( e.g., user search and visit logs), which will be fed into the following relation mining procedure. Reliable relations between entities are fully mined through our proposed TRMP approach and further stored in the Alipay database called Geabase [15]. Meanwhile, the entity embedding extracted in TRMP will be stored for the following module. To help EGL System locate target users effectively and efficiently in the online stage, the user entity preference module is employed to pre-compute user preference towards entities. Note that the offline stage is the cornerstone of supporting the online stage, the corresponding algorithm designs of each module will be detailed in Section III. B. Online Stage The online stage of EGL System aims at discovering target users rapidly when a specific service needs to be promoted. In particular, a marketer is expected to request our EGL System with several phrases (represented as entities) related to the service. Centered on these given entities, the entity graph reasoning module extends their connections iteratively along the entity graph (well established in the offline stage) to discover their hierarchical potential relations. The depth of the extension could be flexibly controlled by marketers to achieve the trade-off between the relevancy and the diversity of the set of k-hop entities. Next, marketers select the entity they require and use the user entity preference module of the offline stage to retrieve all users associated with the chosen entities. Finally, given a central entity, EGL System only keeps top K users with the highest average similarities, to whom the contents of the service will be promoted. Remark EGL System runs 200 âˆ¼ 300 user targeting experiments everyday. To keep the local structure and user entity preference up-to-date, the entity graph derived from the relation mining procedure is updated weekly and the user preference generator towards entity is in daily execution. Meanwhile, the relations chosen by marketers in the operation process will be recorded as relations with high confidence to guide the learning of the relation mining procedure, i.e., our proposed TRMP framework. III. D IVING INTO THE OFFLINE STAGE OF EGL SYSTEM In this section, we will zoom into each module in the offline stage of EGL System. A. Entity Sequence Extractor 1) Entity Dict: In real-world applications ( e.g., Alipay), usersâ€™ behaviors are widely distributed throughout multiple scenarios, and the contents of different services are also di- verse. To effectively employ user targeting over entity graphs, it is of crucial importance to perform content alignmentä½ æœ‰âŒ´ä¸€ä¸ªâŒ´å¿«é€’âŒ´å¾…é¢†å– åŸç¥âŒ´ä»˜æ¬¾ç âŒ´çš®è‚¤âŒ´æ¥å•¦ èš‚èšåº„å›­âŒ´æ•°å­—å…¬ä»”âŒ´ä¸Šæ–°å•¦ Bert - CRF Bert - CRF Bert - CRF (You have a delivery waiting to be picked up) (There comes payment code skin of Genshin Impact) (The ant-forest digital doll is new on) [å¿«é€’] (Delivery) [åŸç¥, ä»˜æ¬¾ç , çš®è‚¤] (Genshin impact, Payment code, Skin]) [èš‚èšåº„å›­, æ•°å­—å…¬ä»”] (Ant forest, Digital doll) [å¿«é€’,åŸç¥,â€¦,æ•°å­—å…¬ä»”] (Delivery, Genshin impact, Digital doll) User entity sequence â€¦ Content of behaviors èš‚èšåº„å›­ æ•°å­—å…¬ä»” å•¦ä¸Šæ–° Bert Preprocessing CRF Layer ğ‘©ğ’“ğ’‚ğ’ğ’… ğ‘·ğ’“ğ’ğ’…ğ’–ğ’„ğ’• / / Fig. 3: Extracting entities from behaviors between scenarios and services for entity-level uniformity. Hence, we introduce the Entity Dict as the basis for bridging diverse contents and unified entities, each row of which is a tuple consisting of the entity and entity type . In particular, the Entity Dict is carefully designed by our dedicated group of experts, which involves millions of entities with 26 types. It is worthwhile to note that the Entity Dict is automatically updated weekly to keep the fitness of entities. 2) Extracting Entities From Behaviors: Based on the Entity Dict, we then shift attention to the entity extraction from a variety of user behaviors in Alipay, e.g., search and visit logs. Naturally, such a process could be formulated as a NER task [16]â€“[18], which are widely studied in the NLP field. Hence, we adopt the state-of-the-art BertCRF model to perform entity extraction, which combines the transfer capabilities of BERT [8] with the structured predictions of CRF [19]. For each user behavior, we feed the corresponding content into the BertCRF [20] model 1, whose output is an entity list tagged on the user behavior. Moreover, we collect user behaviors in the past 30 days, which conducts the final entity sequence via chronological concatenation. The overall procedure is detailed in Fig. 3. B. TRMP Design Intuitively, the success of the EGL System greatly hinges on the building of entity graph with high quality, and thus, we propose the Three-stage Relation Mining Procedure, called TRMP, which consists of the candidate generation, ranking and the ensemble stage, as shown in Fig. 4. In the following parts, we will take a closer look at each well-designed stage. 1) Stage I: Candidate generation: As shown in Fig. 4 (a), the candidate generation task aims to generate the initial entity graph from co-occurrence and semantic aspects. In particular, we adopt the Skip-gram model [7] to mine the co-occurrence relevance between entities among the abundant entity sequences derived from the entity sequence extractor. In terms of the semantic-level relevance, we utilized the Bert [8], which is pre-trained on a large amount of public corpus, e.g., Wikipedia 2. Moreover, we denote the graph in this stage as GC, and respectively denote the co-occurrence-level and semantic-level embedding matrices as ESe and ECo, which will be used in the ranking stage. 1The BertCRF model is well pre-trained based on manually labeled data. 2https://dumps.wikimedia.org/zhwiki/ Through manual evaluation of the relations generated from the candidate generation stage, we surprisingly find the accu- racy of all the relations far lower than 90%. A fine-grained ranking stage is necessary to improve the accuracy. 2) Stage II: Ranking Stage: The performance of the ranking stage greatly hinges on the correlated entity pairs retrieved in the candidate generation stage. Generally, it can be formulated as a link prediction task [21]â€“[29], which could improve the accuracy of the existing relations derived from the candidate generation stage, as well as explore unknown relations for the richness of the target entity graph. As a powerful tool for exploiting structural information, graph neural networks [9], [10], [12], [30]â€“[32] have been widely applied in link pre- diction tasks [28], [29], and attain great success. Due to its excellent performance, we adopt the GeniePath [12] as the backbone for entity representation. Formally, given a source and target entity pair (u, v), the semantic-level and co- occurrence-level embedding i.e., {eSe u , eSe v } and {eCo u , eCo v } (element of ESe and ECo) will be fed into Geniepath as entity features, the whole encoding process is as follows: zu = fGeniePath ([eSe u , eCo u ]), zv = fGeniePath ([eSe v , eCo v ]). (1) Based on the representation, a graph neural network based link prediction could be well-trained through the widely-adopted CrossEntropy-based objective [28], [33], [34]. su,v = g([zu||zv]), Ë†yu,v = Ïƒ(su,v), Lpred = âˆ’ X yu,vlog(Ë†yu,v) + (1âˆ’ yu,v)log(1 âˆ’ Ë†yu,v), (2) where g(Â·) is a scoring function (e.g., inner product, bilinear function or a neural network), yu,v is the ground truth and Ë†yu,v is the predicted correlation score between the source entity and target entity. Nevertheless, employing such an optimization procedure in our scenarios still faces the following challenges: Challenge 1 : Different source entities have different cor- related target entities, and the distribution of the predicted correlation scores Ë†yu,v of each source entity is different, shown in Fig. 5 (a), where NBAâ€™s score distribution is similar to footballâ€™s while Teslaâ€™s score is similar to BYDâ€™s. Hence, when we make threshold truncation, the threshold should be different for different source entities. Challenge 2 : Previous research in metric learning has es- tablished that the hard negative sample is of particular concern in representation learning, while traditional link prediction methods commonly adopt the native random sampling strategy, such that derived â€œeasyâ€ samples are prone to restrict the performance [13], [35]. So we propose a novel link prediction model ALPC to tackle both challenges. As seen in the ranking stage of Fig. 4 (b), ALPC mainly adds an adaptive threshold task and a contrastive learning task to the former optimization procedure, aiming at handling challenge 1 and challenge 2 respectively. Adaptive threshold learning task The task is to learn the personalized threshold of each source entity. So we add Multi Layer Perceptron (MLP) to process the source entity and predict a threshold score Ïµ. Aiming at enlarging the marginPrediction task Adaptive threshold task \tâ„’!\"\t\t Contrastive learning task â€¦ GNN Entity representation extractor âŠ• â„’$%&'\t\t b1 b2 m1 m2 \t\tâ„’() m1b1 ï¼Ÿ Initial Entity Graph b2 m2 Predictor User entity sequence Wikipedia corpus (Delivery, Genshin impact, â€¦ , Digital doll) (coffee, is, a, drink, â€¦ , coffee beans) BertSkip Gram â€¦ â€¦ Embedding Retrieval Embedding Retrieval (coffee, milky tea) (genshin impact, Paimon) (coffee, pour-over coffee) (genshin impact, Aigami) (adult football, football match) (adult football, child football) â€¦ â€¦ Candidate relations of entities Target entitySource entity Concatenate Multi-head attention z!!\" â€¦ Embedding Extractor b1 m1 Candidate generation stage Ranking stage Ensemble stage (a) (b) (c) TRMP Framework mediabrand productembedding b1 m1 b2 m2 Subgraph extraction z!!# z\"!\" z\"!$ â€¦ Fig. 4: Overview of TRMP framework consisting of three stages. between prediction score s and threshold Ïµ, the loss of adaptive threshold learning task is defined as follows, Ïµu = MLP(zu), Ë†yâ€² u,v = Ïƒ(su,v âˆ’ Ïµu), Lth = âˆ’ X yu,vlog(Ë†yâ€² u,v) + (1âˆ’ yu,v)log(1 âˆ’ Ë†yâ€² u,v). (3) Contrasive learning task Inspired by contrastive learn- ing [13], [14], [36], which works by pulling positive samples closer and pushing negative samples further, we enhance the representation of entities with the auxiliary contrastive super- vision. In real-world applications, entities are commonly asso- ciated with abundant textual information, which is a beneficial signal for facilitating representation learning. In particular, for a (source or target) entity e, we construct the anchor pairs < e, e+ > through semantic-level similarities higher than a threshold in all the correlated entity lists. Subsequently, our contrastive learning objective is to minimize the following function based on InfoNCE [13], [37]: Lcl = X log exp(ze Â· ze+ /Ï„)P eâˆ’ exp(ze Â· zeâˆ’/Ï„), (4) where Ï„ is the temperature hyper-parameter and eâˆ’ is drawn from the widely-used in-batch negative sampling strategy. The modelâ€™s total loss is the weighted sum of prediction loss Lpred, threshold loss Lth, and contrastive loss Lcl with hyper-parameters Î± and Î². Experimentally, our model yields the best performance when Î± = Î² = 1. L = Lpred + Î± âˆ— Lth + Î² âˆ— Lcl. (5) 3) Stage III: Ensemble Stage: Due to the change in data distribution of upstream data sources (search logs, visit logs, etc.), the ranking model ALPCâ€™s performance is not stable enough, thus resulting in a big fluctuation of accuracy. Fig. 5 (b) shows the weekly accuracy trend of ALPC. The accuracyâ€™s upper bound is up to 97.5% and the lower bound is 95.5%, where the accuracyâ€™s variance is up to 0.31. (a) (b) NBA Tesla Football BYD Fig. 5: (a) Skewed distribution of predictions w.r.t. different source entities. (b) The weekly accuracy trend of ALPC. Therefore, we add the ensemble stage (Fig. 4 (c)) to improve the stability of the accuracy. It consists of an embedding extractor, a multi-head attention encoder, and MLP modules. Since the ALPC model is updated weekly, we can extract the entity embedding zeti from weekly ALPC model respectively. hu = Concatenate(zut1 , ...,zuti ), i= 1, 2, ... hv = Concatenate(zvt1 , ...,zvti ), i= 1, 2, ... Hu,v = Concatenate(hu, hv). (6) And new predicted values will be obtained through multi-head attention encoder and MLP module. We also use cross entropy to calculate the loss of this stage. Specifically, we store the concatenated entity embedding he for the following module. C. Entity Graph Storage and User Entity Preference Entity Graph Storage. The relations mined from TRMP framework such as <NBA, CBA>, would form the final entity graph and we store it in the Geabase for online serving [15]. User Entity Preference. The inputs of this module are user entity sequence from the entity sequence extractor mod- ule and entity embedding he extracted from the ensemble task. The user embedding is the element-wise sum of he in the corresponding user entity sequence. The dot product between user embedding and entity embedding is the user entity preference score. The equations are as follows. rukdenotes the embedding of user k, l denotes the length of user entity sequence, s<uk,em> denotes the userâ€™s preference score towards entity m. ruk = lX j=1 hej /l, s<uk,em> = ruk âˆ— hem . (7) IV. E XPERIMENTS A. Experiment Settings 1) Evaluation metrics: To evaluate the key components of EGL System, i.e.,TRMP and ALPC, we adopt several metrics, including ACC (Accuracy), CorS (Correlation Score), AEEC (Average Expansion Entity Count), AUC (Area under the ROC Curve), where ACC and CorS are calculated through manual evaluation while AEEC denotes the average number of corre- lated entities for each source entity. Details of manual evalu- ation: we randomly sample entity pairs and ask 8 annotators to decide whether the entity pairs are correlated. The annotators have three choices: highly correlated, medium correlated, and uncorrelated, which respectively represent correlation score=1, 0.5, 0. And â€œcorrelation score = 0 â€ denotes inaccurate relation while â€œcorrelation score > 0 â€ denotes accurate relation. CorS = PN i=1 PN j=1 Ci,j PN i=1 PN j=1 Ti,j , AEEC= PN i=1 PN j=1 Ti,j N , (8) where N is the amount of the Entity Dict, Ci,j represents the correlation score and Ti,j denotes whether there is a relation between entities. 2) Datasets: Since this paper mainly focuses on the indus- trial problem of user targeting of service in the digital market- ing scenario, we employ the real-world industrial datasets 3 of Alipay. And for different stages of TRMP, the dataset differs. Dataset of candidate generation stage The dataset of the co- occurrence part is the user entity sequence which comes from the entity sequence extractor, consisting of about ten million samples after random sampling The dataset of the semantic part comes from the public Wikipedia corpus. Dataset of ranking and ensemble stage Since the TRMP framework includes manual evaluation after the candidate generation stage, we retain the relations of the candidate gen- eration stage only if the accuracy achieves a certain threshold and form the initial entity graph. The initial entity graph has millions of entities and billions of edges, making up 78% of the total entity dictionary. We randomly remove 10% of existing relations from the initial graph as positive testing data. Following a standard manner of learning-based link prediction, we randomly sample the same number of nonexistent relations (unconnected node pairs) as negative testing data. We use the remaining 90% of existing links as well as the same number of additionally sampled nonexistent links to construct the training 3The data set does not contain any Personal Identifiable Information (PII). The data set is desensitized and encrypted. Adequate data protection was carried out during the experiment to prevent the risk of data copy leakage, and the data set was destroyed after the experiment. TABLE I: Metrics of each stage Stage ACC CorS AEEC Variance of ACC TRMP w.o. E&R s 68.60% 0.673 78.0 0.30 TRMP w.o E&R 80.60% 0.780 78.0 0.32 TRMP w.o E 97.70% 0.950 61.2 0.31 TRMP 97.76% 0.951 59.5 0.08 data. The other negative samples come from negative sampling methods. In short, the datasets of both ranking and ensemble stage consist of 6 million positive samples and 18 million negative samples, called Dataset-M. B. Effectiveness of TRMP The performance of each stage of TRMP is reported in Table I, in which we prepare two variants of TRMP, i.e., TRMP w.o E without ensemble stage and TRMP w.o E&R without both ensemble and ranking stage. And the stage TRMP w.o. E&R s denotes forming entity pairs through pop- ularity sampling methods from Entity Dict. We can observe that in terms of ACC and CorS metrics, TRMP > TRMP w.o E > TRMP w.o E&R > TRMP w.o. E&R s. The candidate generation stage improves the ACC from 68.6% to 80.6%, and the ranking stage improves the ACC from 80.6% to 97.7%. We can obtain that the ranking stage plays the most important role in terms of ACC and CorS metrics. The AEEC metric of the candidate generation stage is the biggest, which shows the richness of expanding entities. In terms of the variance of ACC, the ensemble stage shows great potential (0.31 â†’ 0.08). In summary, the whole TRMP framework improves the ACC to 97%+ through the ranking stage and maintains the ACC & CorS in a steady value through the ensemble stage, which achieves an ideal level for entity graph construction. C. Effectiveness of ALPC Baseline methods In this experiment, we prepare the following baselines: (1) Graph embedding based methods: DeepWalk [25], Node2Vec [26]. (2) GNN-based methods: VGAE [38], SEAL [28], Geniepath [12], CompGCN [39], PaGNN [40]. To verify the consistent performance of ALPC, We evaluate ALPC and other methods on three sampled sub- dataset A, B, and C with different sampling ratios (details can be seen in Table II) from Dataset-M, and report the results in Table II. In addition, to verify the effectiveness of the proposed auxiliary tasks, we prepare variants ALP Cthâˆ’ (i.e., ALPC without the adaptive threshold network), and ALP Cclâˆ’ (i.e., ALPC without the contrastive learning task). From Table II, we have the following observations and analyses. First, among all the methods, ALPC performs best in all the datasets for both metrics, especially the ACC (manual evaluation metrics), indicating the effectiveness of ALPC by bringing in the adaptive threshold network and contrastive learning task. Second, we compare ALPC, ALP Cthâˆ’ and ALP Cclâˆ’. The AUC gap between ALPC and ALP Cthâˆ’ is low since the latter version only lacks an adaptive threshold task, which mainly learns a threshold score. However, the ALPCâ€™s ACC is much better than ALP Cthâˆ’, which verifiesTABLE II: Performance comparison on offline datasets. Dataset A Dataset B Dataset C # Entities 113,267 42,529 92,651 # Edges 11,570,856 4,337,924 9,272,733 Methods AUC ACC AUC ACC AUC ACC DeepWalk 0.846 0.909 0.837 0.911 0.852 0.921 Node2Vec 0.848 0.915 0.839 0.913 0.856 0.932 SEAL 0.868 0.940 0.863 0.936 0.873 0.943 VGAE 0.847 0.928 0.857 0.930 0.874 0.939 Geniepath 0.870 0.944 0.865 0.942 0.877 0.945 CompGCN 0.869 0.942 0.865 0.943 0.876 0.944 PaGNN 0.872 0.951 0.867 0.951 0.878 0.955 ALPC 0.879 0.967 0.870 0.961 0.883 0.973 ALPCthâˆ’ 0.875 0.960 0.868 0.956 0.882 0.960 ALPCclâˆ’ 0.871 0.950 0.862 0.944 0.879 0.953 TABLE III: Online experiments performance Services # exposure # conversion CVR Running Time Railway +0.30% 23.20% 23.00% 3.0 min Dicos +0.50% 16.90% 16.30% 2.0 min Cosmetics -0.20% 19.50% 19.80% 2.5 min Dessert +0.73% 33.60% 32.90% 3.2 min Women Football +0.10% 9.40 % 9.20 % 2.2 min the effectiveness of the adaptive threshold task. The com- parison between ALPC and ALP Cclâˆ’ shows that adding a contrastive learning task can improve the ACC greatly. On the other hand, we find that the contrastive learning task is better than the adaptive threshold task in improving ACC. D. Online Performance of the EGL System The EGL System has already been deployed in the produc- tion environment of Alipay to serve the marketers to promote their services. Here, we conduct online A/B testing experi- ments to demonstrate the performance of the EGL System (shown in Table III) in real traffic when there are no seed users of the service. And we report the results based on the following four metrics: # exposure means the number of users who have been exposed by the service, # conversion means the number of users who have clicked inside the service, CVR means the conversion rate of the service, and running time means the total running time of user targeting task. Effectiveness Note that the user targeting task is expected to find users who are most likely like the service. A higher CVR indicates a higher quality of selected users, and we report the gains of the EGL against the online baseline ( i.e., rule-based method) in Table III. It shows that the proposed EGL System maintains a great improvement in conversion and CVR. Efficiency In terms of user targeting efficiency, the whole operation process only needs 2-4 minutes on average, which is 3 times faster than the former system in Alipay, i.e., Hubble System [5]. In summary, the proposed EGL System performs effectively and efficiently in real-world online A/B testing, thus being more suitable for the industry. E. Application Case In this part, we will show a practical application case of the EGL System including processes of both user targeting 1.Input Keywords 2.Real time reasoning 3.Obtain potential entities Loreal Estee Lauder Avene Lancome Loreal Essence Shu uemura Armani Biotherm Thermal spring water Eyebrow pencil Origins Shiseido Enter your words Loreal Chosen entities Delete Potential user size estimation Loreal Lancome Estee Lauder Loreal Essence Armani Avene Thermal spring water 4.Entity performance of the crowd 5.User targeting iteration Loreal Estee Lauder LancomeLoreal Essence ArmaniShiseido Shiseido (a) (b) Fig. 6: A real case of user targeting of EGL system in Alipay and the marketerâ€™s next iteration to the target users. Fig. 6 (a) and Fig. 6 (b) represent these two processes respectively. In the user targeting process, when a marketer brings in a new service namely the Lâ€™Oreal service on the Alipay app, the marketer needs to propagandize it. In this scenario, the marketer only needs to search the word Lâ€™Oreal (entity) in the input box (the first step in Fig. 6 (a)) and our system will show the entity and the entityâ€™s two-hops subgraph in default (the second step in Fig. 6 (a), the marketers can choose any hops they need). Then they choose the entities they need and the chosen entities will be at the bottom of the final box (the third step in Fig. 6). When the marketer clicks the export button, our EGL will compute the target users of certain services and complete the exportation. The whole user targeting process only needs 2-4 minutes on average. Once the target users are exported and used for the service, the EGL computes the performance of all the chosen entities of corresponding target users (the fourth step in Fig. 6). Note the performance, the marketers can promote their serviceâ€™s performance by iterating the user targeting process. Hence through our system, the marketers can not only obtain the serviceâ€™s target users on their own, but also they can iterate the above process to discover the target users that meet their needs. V. C ONCLUSION In this paper, we propose an innovative industrial system for audience targeting in mobile marketing scenarios, called EGL System, supported by our well-designed TRMP framework. In particular, the TRMP framework consists of candidate generation, ranking and ensemble stage, where we proposed a novel ALPC for effective relation mining in the ranking stage. Extensive experiments in offline and online environments demonstrate the effectiveness and efficiency of EGL System. Future works The dynamic nature of entity graphs renders ALPC vulnerable to the distribution shift in realistic scenarios, thus incorporating stable learning [41], [42] and causal infer- ence [43] for out-of-distribution generalization is a promising direction. Moreover, we are also interested in investigating hyperbolic graph learning [44], [45] for modeling hierarchical structures in our entity graphs.REFERENCES [1] A. Mangalampalli, A. Ratnaparkhi, A. O. Hatch, A. Bagherjeiran, R. Parekh, and V . Pudi, â€œA feature-pair-based associative classification approach to look-alike modeling for conversion-oriented user-targeting in tail campaigns,â€ in Proceedings of the 20th international conference companion on World wide web , 2011, pp. 85â€“86. [2] J. Shen, S. C. Geyik, and A. Dasdan, â€œEffective audience extension in online advertising,â€ in Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , 2015, pp. 2099â€“2108. [3] Q. Ma, E. Wagh, J. Wen, Z. Xia, R. Ormandi, and D. Chen, â€œScore look-alike audiences,â€ in 2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW) . IEEE, 2016, pp. 647â€“654. [4] S. deWet and J. Ou, â€œFinding users who act alike: transfer learning for expanding advertiser audiences,â€ in Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019, pp. 2251â€“2259. [5] C. Zhuang, Z. Liu, Z. Zhang, Y . Tan, Z. Wu, Z. Liu, J. Wei, J. Gu, G. Zhang, J. Zhou et al. , â€œHubble: An industrial system for audience expansion in mobile marketing,â€ in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2455â€“2463. [6] Y . Zhu, Y . Liu, R. Xie, F. Zhuang, X. Hao, K. Ge, X. Zhang, L. Lin, and J. Cao, â€œLearning to expand audience via meta hybrid experts and critics for recommendation and advertising,â€ in Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining , 2021, pp. 4005â€“4013. [7] T. Mikolov, K. Chen, G. Corrado, and J. Dean, â€œEfficient estimation of word representations in vector space,â€ arXiv preprint arXiv:1301.3781 , 2013. [8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, â€œBert: Pre-training of deep bidirectional transformers for language understanding,â€ arXiv preprint arXiv:1810.04805, 2018. [9] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfar- dini, â€œThe graph neural network model,â€ IEEE transactions on neural networks, vol. 20, no. 1, pp. 61â€“80, 2008. [10] W. L. Hamilton, R. Ying, and J. Leskovec, â€œInductive representation learning on large graphs,â€ 2017. [11] Z. Liu, C. Chen, X. Yang, J. Zhou, X. Li, and L. Song, â€œHeteroge- neous graph neural networks for malicious account detection,â€ in Acm International Conference, 2018, pp. 2077â€“2085. [12] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y . Qi, â€œGeniepath: Graph neural networks with adaptive receptive paths,â€ 2018. [13] A. v. d. Oord, Y . Li, and O. Vinyals, â€œRepresentation learning with contrastive predictive coding,â€ arXiv preprint arXiv:1807.03748 , 2018. [14] A. Jaiswal, A. R. Babu, M. Z. Zadeh, D. Banerjee, and F. Makedon, â€œA survey on contrastive self-supervised learning,â€ Technologies, vol. 9, no. 1, p. 2, 2020. [15] Z. Fu, Z. Wu, H. Li, Y . Li, M. Wu, X. Chen, X. Ye, B. Yu, and X. Hu, â€œGeabase: A high-performance distributed graph database for industry- scale applications,â€ in 2017 Fifth International Conference on Advanced Cloud and Big Data (CBD) . IEEE, 2017, pp. 170â€“175. [16] Y . Zhang and J. Yang, â€œChinese ner using lattice lstm,â€ arXiv preprint arXiv:1805.02023, 2018. [17] W. Liu, T. Xu, Q. Xu, J. Song, and Y . Zu, â€œAn encoding strategy based word-character lstm for chinese ner,â€ in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , 2019, pp. 2379â€“2389. [18] J. Li, A. Sun, J. Han, and C. Li, â€œA survey on deep learning for named entity recognition,â€ IEEE Transactions on Knowledge and Data Engineering, vol. 34, no. 1, pp. 50â€“70, 2020. [19] J. Lafferty, A. McCallum, and F. C. Pereira, â€œConditional random fields: Probabilistic models for segmenting and labeling sequence data,â€ 2001. [20] F. Souza, R. Nogueira, and R. Lotufo, â€œPortuguese named entity recognition using bert-crf,â€ arXiv preprint arXiv:1909.10649 , 2019. [21] D. Liben-Nowell and J. Kleinberg, â€œThe link prediction problem for social networks,â€ in Proceedings of the twelfth international conference on Information and knowledge management , 2003, pp. 556â€“559. [22] L. A. Adamic and E. Adar, â€œFriends and neighbors on the web,â€ Social networks, vol. 25, no. 3, pp. 211â€“230, 2003. [23] L. Katz, â€œA new status index derived from sociometric analysis,â€ Psychometrika, vol. 18, no. 1, pp. 39â€“43, 1953. [24] J. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, â€œLine: Large-scale information network embedding,â€ in Proceedings of the 24th international conference on world wide web , 2015, pp. 1067â€“1077. [25] B. Perozzi, R. Al-Rfou, and S. Skiena, â€œDeepwalk: Online learning of social representations,â€ in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining , 2014, pp. 701â€“710. [26] A. Grover and J. Leskovec, â€œnode2vec: Scalable feature learning for networks,â€ in Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining , 2016, pp. 855â€“ 864. [27] C. Shi, B. Hu, W. X. Zhao, and S. Y . Philip, â€œHeterogeneous informa- tion network embedding for recommendation,â€ IEEE Transactions on Knowledge and Data Engineering , vol. 31, no. 2, pp. 357â€“370, 2018. [28] M. Zhang and Y . Chen, â€œLink prediction based on graph neural net- works,â€ Advances in neural information processing systems , vol. 31, 2018. [29] K. Teru, E. Denis, and W. Hamilton, â€œInductive relation prediction by subgraph reasoning,â€ in International Conference on Machine Learning. PMLR, 2020, pp. 9448â€“9457. [30] T. N. Kipf and M. Welling, â€œSemi-supervised classification with graph convolutional networks,â€ 2016. [31] P. Velikovi, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and Y . Bengio, â€œGraph attention networks,â€ 2017. [32] D. Bo, B. Hu, X. Wang, Z. Zhang, C. Shi, and J. Zhou, â€œRegularizing graph neural networks via consistency-diversity graph augmentations,â€ in AAAI, 2022, pp. 3913â€“3921. [33] R. Y . Rubinstein and D. P. Kroese, The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation, and machine learning. Springer, 2004, vol. 133. [34] M. A. Hasan and M. J. Zaki, â€œA survey of link prediction in social networks,â€ in Social network data analytics . Springer, 2011, pp. 243â€“ 275. [35] F. Schroff, D. Kalenichenko, and J. Philbin, â€œFacenet: A unified embed- ding for face recognition and clustering,â€ in Proceedings of the IEEE conference on computer vision and pattern recognition , 2015, pp. 815â€“ 823. [36] P. H. Le-Khac, G. Healy, and A. F. Smeaton, â€œContrastive representation learning: A framework and review,â€ IEEE Access, vol. 8, pp. 193 907â€“ 193 934, 2020. [37] M. Gutmann and A. Hyvrinen, â€œNoise-contrastive estimation: A new es- timation principle for unnormalized statistical models,â€ in International Conference on Artificial Intelligence and Statistics , 2010. [38] T. N. Kipf and M. Welling, â€œVariational graph auto-encoders,â€ arXiv preprint arXiv:1611.07308, 2016. [39] S. Vashishth, S. Sanyal, V . Nitin, and P. Talukdar, â€œComposition-based multi-relational graph convolutional networks,â€ 2019. [40] S. Yang, B. Hu, Z. Zhang, W. Sun, Y . Wang, J. Zhou, H. Shan, Y . Cao, B. Ye, Y . Fanget al., â€œInductive link prediction with interactive structure learning on attributed graph,â€ in Joint European Conference on Machine Learning and Knowledge Discovery in Databases . Springer, 2021, pp. 383â€“398. [41] Z. Shen, P. Cui, J. Liu, T. Zhang, B. Li, and Z. Chen, â€œStable learning via differentiated variable decorrelation,â€ in Proceedings of the 26th acm sigkdd international conference on knowledge discovery & data mining , 2020, pp. 2185â€“2193. [42] X. Zhang, P. Cui, R. Xu, L. Zhou, Y . He, and Z. Shen, â€œDeep stable learning for out-of-distribution generalization,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2021, pp. 5372â€“5382. [43] Y .-X. Wu, X. Wang, A. Zhang, X. He, and T.-S. Chua, â€œDiscov- ering invariant rationales for graph neural networks,â€ arXiv preprint arXiv:2201.12872, 2022. [44] Q. Liu, M. Nickel, and D. Kiela, â€œHyperbolic graph neural networks,â€ Advances in Neural Information Processing Systems , vol. 32, 2019. [45] M. Yang, M. Zhou, Z. Li, J. Liu, L. Pan, H. Xiong, and I. King, â€œHy- perbolic graph neural networks: A review of methods and applications,â€ arXiv preprint arXiv:2202.13852 , 2022.",
      "references": [
        "A feature-pair-based associative classification approach to look-alike modeling for conversion-oriented user-targeting in tail campaigns",
        "Effective audience extension in online advertising",
        "Score look-alike audiences",
        "Finding users who act alike: transfer learning for expanding advertiser audiences",
        "Hubble: An industrial system for audience expansion in mobile marketing",
        "Learning to expand audience via meta hybrid experts and critics for recommendation and advertising",
        "Efficient estimation of word representations in vector space",
        "Bert: Pre-training of deep bidirectional transformers for language understanding",
        "The graph neural network model",
        "Inductive representation learning on large graphs",
        "Heteroge-\nneous graph neural networks for malicious account detection",
        "Geniepath: Graph neural networks with adaptive receptive paths",
        "Representation learning with contrastive predictive coding",
        "A survey on contrastive self-supervised learning",
        "Geabase: A high-performance distributed graph database for industry-\nscale applications",
        "Chinese ner using lattice lstm",
        "An encoding strategy based word-character lstm for chinese ner",
        "A survey on deep learning for named entity recognition",
        "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
        "Portuguese named entity recognition using bert-crf",
        "The link prediction problem for social networks",
        "Friends and neighbors on the web",
        "A new status index derived from sociometric analysis",
        "Line: Large-scale information network embedding",
        "Deepwalk: Online learning of social representations",
        "node2vec: Scalable feature learning for networks",
        "Heterogeneous informa-\ntion network embedding for recommendation",
        "Link prediction based on graph neural net-\nworks",
        "Inductive relation prediction by subgraph reasoning",
        "Semi-supervised classification with graph convolutional networks",
        "Graph attention networks",
        "Regularizing graph neural networks via consistency-diversity graph augmentations",
        "The cross-entropy method: a unified approach to combinatorial optimization, Monte-Carlo simulation, and machine learning.",
        "A survey of link prediction in social networks",
        "Facenet: A unified embed-\nding for face recognition and clustering",
        "Contrastive representation learning: A framework and review",
        "Noise-contrastive estimation: A new es-\ntimation principle for unnormalized statistical models",
        "Variational graph auto-encoders",
        "Composition-based multi-relational graph convolutional networks",
        "Inductive link prediction with interactive structure learning on attributed graph",
        "Stable learning via differentiated variable decorrelation",
        "Deep stable learning for out-of-distribution generalization",
        "Discov-\nering invariant rationales for graph neural networks",
        "Hyperbolic graph neural networks",
        "Hy-\nperbolic graph neural networks: A review of methods and applications"
      ],
      "meta_data": {
        "arxiv_id": "2305.18780v1",
        "authors": [
          "Dan Yang",
          "Binbin Hu",
          "Xiaoyan Yang",
          "Yue Shen",
          "Zhiqiang Zhang",
          "Jinjie Gu",
          "Guannan Zhang"
        ],
        "published_date": "2023-05-30T06:24:50Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Proposes EGL, an explainable, seed-user-free user-targeting system for marketing services via cognitive reasoning over an automatically mined large-scale entity graph; introduces TRMP (Three-stage Relation Mining Procedure) to construct and maintain a high-quality entity graph from behavioral logs and text; proposes ALPC, a GNN-based adaptive-threshold link prediction model with semantic-augmented contrastive learning to filter relations; adds an ensemble stage to stabilize relation accuracy under data drift; deploys in Alipay with real-time targeting and marketer-controllable hop-based entity expansion, showing offline gains (>97% relation accuracy) and online A/B improvements over rule-based targeting with 2â€“4 minute latency.",
        "methodology": "Hybrid onlineâ€“offline architecture. Offline: (1) Entity sequence extraction: expert-built Entity Dict (millions of entities, 26 types) + BERT-CRF NER to extract entities from user search/visit content; build 30-day chronological user entity sequences. (2) TRMP Stage I candidate generation: Skip-gram on entity sequences for co-occurrence embeddings + BERT on Wikipedia for semantic embeddings; retrieve candidate related-entity pairs. (3) TRMP Stage II ranking: formulate as link prediction on initial graph; use GeniePath GNN as encoder with concatenated semantic+co-occurrence features; ALPC augments standard cross-entropy link prediction with (a) per-source adaptive threshold network (MLP predicts Îµ_u and shifts logits s_{u,v}-Îµ_u) to handle skewed score distributions, and (b) InfoNCE contrastive loss using semantically similar entity pairs as positives and in-batch negatives to create harder negatives. (4) TRMP Stage III ensemble: aggregate weekly ALPC embeddings across time using concatenation + multi-head attention + MLP to produce more stable predictions and store final entity embeddings. Store mined relations in Geabase; precompute userâ€“entity preference by averaging entity embeddings in a userâ€™s sequence and dot-product scoring for retrieval/ranking online. Online: marketer inputs service-related entities; system performs k-hop graph reasoning/expansion; marketer selects entities; retrieve and rank top-K users by preference scores; explanations via traversed entity paths.",
        "experimental_setup": "Industrial-scale Alipay datasets (PII-free, desensitized/encrypted). Candidate generation: ~10M sampled user entity sequences (from last 30 days) for Skip-gram; Chinese Wikipedia dump for BERT semantic mining. Ranking/ensemble: build an initial graph with millions of entities and billions of edges (covers ~78% of Entity Dict); create Dataset-M with 6M positive links and 18M negatives (non-edges + sampled negatives). Split: remove 10% observed links as positive test set; sample equal number of negative test pairs; train on remaining 90% links plus same number of sampled non-links. Metrics: offline manual evaluation by 8 annotators labeling sampled pairs as highly/medium/uncorrelated (mapped to 1/0.5/0) to compute ACC (correlated vs not), CorS (mean correlation score), AEEC (avg expansion count), plus AUC. Baselines for link prediction: DeepWalk, Node2Vec, VGAE, SEAL, GeniePath, CompGCN, PaGNN; ALPC ablations without threshold (ALPCthâˆ’) and without contrastive loss (ALPCclâˆ’). TRMP ablations: without ranking/ensemble, etc. Online: A/B tests for cold-start services (no seed users) against rule-based baseline; report exposure, conversions, CVR lift, and end-to-end runtime; observed 2â€“4 minute targeting time and CVR gains across multiple services (e.g., railway, fast food, dessert, women football).",
        "limitations": "Depends on a large, expert-maintained Entity Dict (coverage/quality and weekly updates) and a supervised BERT-CRF NER model requiring labeled data; entity relation â€œaccuracyâ€ relies heavily on costly manual annotation and subjective correlation labels; evaluation focuses on correlation links rather than explicit relation types/directions, potentially mixing heterogeneous semantics; link prediction training uses random negative sampling that may not reflect true hard negatives or temporal causality; ensemble improves stability but increases storage/complexity and still requires periodic retraining; user preference model is simple (average pooling + dot product), ignoring recency, context, multi-intent, and causal effects, which may limit targeting precision; graph updates are weekly (graph) and daily (preferences), so fast-evolving trends may lag; online results are reported on a limited set of services and against a rule-based baseline (not against modern look-alike systems in cold-start), so generality is uncertain; potential bias/fairness and privacy impacts are not deeply analyzed beyond desensitization statements.",
        "future_research_directions": "Improve robustness to distribution shift in dynamic graphs via stable learning and out-of-distribution generalization, potentially with causal inference; explore hyperbolic graph learning to better model hierarchical entity structures and improve multi-hop reasoning; enhance negative sampling and contrastive objectives with stronger semantic/hard negatives and temporal constraints; incorporate temporal/recency-aware and multi-interest user modeling beyond mean pooling; add typed/directed relations and relation-specific scoring for finer-grained explanations; develop automated evaluation/active learning to reduce manual labeling cost and close the loop with marketer feedback more formally; study fairness, bias mitigation, and privacy-preserving graph learning for marketing targeting.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Chain-of-Thought Reasoning Without Prompting",
      "full_text": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality Emre KÄ±cÄ±man âˆ— emrek@microsoft.com Microsoft Research Robert Osazuwa Ness robertness@microsoft.com Microsoft Research Amit Sharma amshar@microsoft.com Microsoft Research Chenhao Tan chenhao@uchicago.edu University of Chicago Abstract The causal capabilities of large language models (LLMs) are a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We conduct a â€œbehavorialâ€ study of LLMs to benchmark their capability in generating causal arguments. Across a wide range of tasks, we find that LLMs can generate text corresponding to correct causal arguments with high probability, surpassing the best-performing existing methods. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain) and event causality (86% accuracy in determining necessary and sufficient causes in vignettes). We perform robustness checks across tasks and show that the capabilities cannot be explained by dataset memorization alone, especially since LLMs generalize to novel datasets that were created after the training cutoff date. That said, LLMs exhibit unpredictable failure modes and we discuss the kinds of errors that may be improved and what are the fundamental limits of LLM-based answers. Overall, by operating on the text metadata, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. As a result, LLMs may be used by human domain experts to save effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. Given that LLMs ignore the actual data, our results also point to a fruitful research direction of developing algorithms that combine LLMs with existing causal techniques. Code and datasets are available at https://github.com/py-why/pywhy-llm. 1 Introduction Recent advances in scaling large language models (LLMs) have led to breakthroughs in AI capabilities. As language models increase in number of parameters and are trained on larger datasets, they gain complex, emergent behaviors, such as abilities to write code in programming languages, generate stories, poems, essays, and other texts, and demonstrate strong performance in certain reasoning tasks (Chen et al., 2021; Nguyen & Nadi, 2022; Bubeck et al., 2023; Katz âˆ—Authors listed alphabetically. 1 arXiv:2305.00050v3  [cs.AI]  20 Aug 2024Iterate on  question Causal  question Causal  assumptions  (Graph) Identification Estimation Refutation  and  Validation Iterate on  question Causal  question Logical  Premises  (SCM) Logical  proof and  inference Critique  premises  and  implications Identify  missing  pieces Covariance-based Causality Logic-based Causality Real-world  Causal Task Break causal task into (sub-)questions Recurse, iterate, verify Causal Question Figure 1: When tackling real-world causal tasks, people strategically alternate between logical- and covariance-based causal inferences as they formulate (sub-)questions, iterate, and verify their premises and implications. Now, LLMs may have the capability to automate or assist with every step of this process and seamlessly transition between covariance- and logic-based causality. et al., 2023; Wei et al., 2022a). Impressively, when asked to explain their outputs, update their conclusions given new evidence, and even generate counterfactuals, LLMs can create plausible responses (Nori et al., 2023; Lee et al., 2023a;b). This apparent capacity for both implicit and explicit consideration of causal factors has created excitement towards understanding their reasoning capabilities (Hobbhahn et al., 2022; Kosoy et al., 2022; Willig et al., 2022; Liu et al., 2023; Zhang et al., 2023). Figure 2(a) shows an example of such generation of causal arguments1. At the same time, LLMs are imperfect: they can make absurd claims and are often observed to make basic errors of logic and mathematics, much less complex reasoning (Bubeck et al., 2023; Zhong et al., 2023; Ghazal et al., 2017). Figure 2(b) shows an example of an LLM making basic causal mistakes. This has incited a debate about whether LLMs truly perform causal reasoning, or are simply unreliable mimics, regenerating memorized responses in the context of critical analysis and decision-making tasks across many domains, including health, medicine, legal reasoning, education, policy making and business strategy (Bender et al., 2021; Marcus, 2022). Complicating the situation is the variety of distinct formal and informal methods that people ascribe to â€˜causal reasoningâ€™, and the breadth of purposes for which people apply causal reasoning in some form. In short, what kinds of causal arguments can LLMs generate, how valid are these arguments, and what causal reasoning workflows can this generation support or automate? This paper clarifies this debate, advancing our understanding of LLM models and their causal implications, and proposes a framework for future research at the intersection of LLMs and causality. We begin with a recognition of the distinct kinds of causal knowledge and reasoning implicated in this debate, including prior knowledge of general and domain-specific causal mechanisms, intuitive and logical methods for causal and counterfactual reasoning, and covariance-based causality, such as building causal DAGs and inferring causal effects (see Figure 1 for an illustration). We present intriguing results in each of areas and then proceed with deeper analyses to better understand the drivers of LLM behavior and the reliability of their causal capabilities in two areasâ€”causal DAG generation and token causality. Our results indicate that LLMs bring significant new capabilities which are complementary to existing causal methods. They do so by capturing the human domain knowledge relevant to the task, which forms an essential part of any causal analysis. As a result, LLMs have the capability of transforming how causal 1We use the phrase generate causal arguments to distinguish LLM generation of the text of a causal statement or argument with causal reasoning itself. 2SYSTEM : You are a business advisor and give clear, well- reasoned but brief (5-15 line) advice in response to questions about what people should do. USER : A toy shop owner in the western USA wants to decide whether the ad they bought in early December is really better than their previous ads. Here is their sales data: October: $10,200 ; November: $10,000; Decem- ber: $13,000; January: $10,100 They now want to decide which ad to show in February. Can you help them decide whether the increase in sales in December was due to the ads? Note that the new ad costs $1000 more to produce, so the toy maker is interested in maxi- mizing their profit. GPT-4: Based on the available data, it is difficult to definitively attribute the increase in December sales to the new ad, as holiday season and other external factors may have influenced sales. To make a more informed decision, I recommend conducting an A/B test in February by dividing the targeted audience into two groups, showing one group the old ad and the other group the new ad. Compare the sales performance of both groups and calculate the net profit (considering the $1000 additional cost for the new ad) to de- termine which ad is more effective in driving sales and maximizing profit. This will provide a clearer understanding of the adâ€™s impact on sales. (a) A GPT-4 powered advisor gives sound causal advice USER : Thanks. In February, the toy shop owner split their audience into two groups to show them two different ads. They showed the old ad to people on the newspaper; and showed the new ad to people online. When people came into the store, the owner would ask them which ad they saw. In February, they found out that they sold $6000 of toys to people who saw the new online ad, and $4500 to people who saw the old newspaper ad. What should the toymaker do in March? GPT-4: Based on the February results, the new online ad generated $1,500 more in sales than the old newspaper ad. Considering the new ad costs $1,000 more to produce, the net profit increase is $500. In this case, it seems that the new online ad is more effective in driving sales. For March, the toy shop owner should continue using the new online ad, but also consider monitoring the long-term performance of the ad, as well as any potential changes in the target audienceâ€™s pref- erences or external factors that may influence sales. Additionally, the owner may want to ex- plore optimizing the new ad further to maximize its cost-effectiveness and reach. (b) Continuing the conversation, GPT-4 gives a wrong causal answer Figure 2: Probing LLM-ability to build causal arguments. Two example outputs from an LLM (GPT-4). In the first dialog, the LLM discusses causal issues, such as a potential confounder and recommends an A/B experiment to correctly characterize effects and drive the requested decision-making. The second example continues the conversation and requires arguably the same kind of causal awareness of potential confounders (e.g., the population characteristics and even population sizes of the online and newspaper audience are unknown) but the LLM proceeds regardless and provides an incorrect answer. analysis is done, with the potential to automate or assist in each step of a causal reasoning process, shown by the arrows in Figure 1. Summary of results. Given a set of variables, causal DAG generation is the task of specifying the causal graph characterizing their data-generating process. Starting with a pairwise causal task, we find that LLM-based methods using variable names as input substantially outperform existing algorithms for building causal DAGs from statistical evidence in data (causal discovery algorithms (Peters et al., 2017)), achieving 97% accuracy on the TÃ¼bingen benchmark compared to the previous best of 83%. The benchmark includes over hundred causal relationships from a variety of domains, including physics, biology, zoology, cognitive science, epidemiology, and soil science. We repeat the experiment on a more specialized, real-world task related to medical pain diagnosis and obtain similarly high accuracy for GPT 3.5 and 4 models. On the more challenging task of discovering the full graph, we find that LLMs obtain similar accuracy to recent deep learning-based methods. We extend these experiments to pairwise and full graph datasets that were made public after the LLMsâ€™ training cutoff date and find similar high accuracies. The errors made by the two kinds of methodsâ€”LLMs using variable names and discovery algorithms using the observed dataâ€”are not the same, highlighting the potential of combining these methods for greater accuracy. That said, LLMs do make unexpected mistakes; the surprising part is how few such mistakes are, since LLMs only consider the metadata (i.e., variable names) rather than analyzing the data values. To characterize our measurement constructs, we probe whether LLM has been 3trained on and memorized the datasets underlying our benchmarks and lay out the implications for interpreting causal benchmarks. The second major causal task we consider is token causality (Halpern, 2016). Unlike building causal DAGs, which deals with variables and their effect on each other, token causality considers individual events and aims to find what caused them. Inferring causes of an event involves simulation of different counterfactual scenarios, but it also involves human judgment to understand the background context and determine which causes need to be considered. For counterfactual reasoning tasks, we find that LLMs like GPT-4 are capable of answering natural language questions. On a benchmark of counterfactual queries spanning basic physics, logic, and common sense, gpt-4 obtains 92% accuracy, 20 points higher than previously reported accuracy. These counterfactual capabilities also help LLMs to isolate the necessary and sufficient causes given any event, one of the key building blocks of token causality methods. GPT-4 obtains over 86% accuracy in identifying the necessary and sufficient causes on a commonly used benchmark of 15 vignettes, as well as a novel benchmark that avoids any memorization concerns. That said, tasks that heavily depend on an understanding of human factors in deciding the cause of an event, e.g., assessing the normality of a particular action, remain challenging for LLMs: GPT-4 obtains 70% on our benchmark task for assessing normality. Implications for causality research. Irrespective of whether LLMs can perform true causal reasoning (as opposed to merely generate well-formed causal arguments that support such reasoning), their empirically observed ability to perform certain causal tasks is strong enough to provide a useful augmentation for aspects of causal reasoning where we currently rely on humans alone. For example, conventional causal DAG generation and causal effect inference rely strongly on prior domain knowledge of potential causal mechanisms in a system. Current best practice is to rely on human domain experts to provide this knowledge, yet correctly capturing domain knowledge in a formal representation suitable for analysis remains a challenge and is often a primary point of weakness for the validity of causal analyses. LLM capabilities now open the possibility of programmatic access to an array of (memorized or inferred) causal mechanisms, capturing general and domain-specific knowledge, and may augment human domain experts by aiding in bootstrapping, critiquing, etc. Other areas where LLMs provide significant benefit include the ability to understand and formalize causal scenarios, generate relevant formal premises based on background knowledge about the world; and ability to identify and correctly frame challenging causal constraints, validations and refutations, such as negative and positive controls, monotonicity relationships, etc. These are all tasks where previously we relied on human experts alone, and now can be partially or entirely automated with human supervision. That said, LLMs do have unexpected failure modes. In each of the tasks that we studied, LLMs achieve high average accuracies but also make simple, unpredictable mistakes on certain inputs. Further, their accuracy (and consequently robustness) depends substantially on the prompt used, as observed by Long et al. (2023). We provide some basic empirical tests to probe their robustness to specific prompt language and understand the relative contribution of data memorization. However, more research is needed to understand when LLM outputs can be trusted and increase their robustness, either through external tools or other instances of LLMs themselves. We close with an updated landscape for causal analysis research and practice. Fully characterizing LLMs inherent capacity to build causal arguments and understanding its underlying mechanisms, requires significant research effortsâ€” and until this is accomplished, it is not prudent to trust LLMs alone in critical decision-making tasks and other causal applications. However, current capabilities are sufficiently advanced to be useful in conjunction with existing methods for formal causal reasoning, discovery, and effect inference. We discuss these opportunities, including the possibility that LLMs may provide opportunities for building tighter automated integration between logical and covariance-based approaches to causality. Outline of the paper. Section 2 reviews background in causality and LLMs. Section 3 provides our key results of causal DAG generation on causal discovery benchmarks and the associated implications for causality effect inference research. Section 4 discusses token causality and the performance of LLMs in generating counterfactual arguments, determining necessary and sufficient causes, and causal judgment tasks. Finally, Section 5 discusses how these LLM capabilities provide a way to augment existing causal analyses with domain knowledge and natural language-based reasoning, while also making it possible to combine different kinds of causal methods in a single analysis. 42 Background and Preliminaries Working with natural language, LLMs bring capabilities to causal questions that are complementary to existing approaches. These capabilities span across hitherto disparate fields of causal enquiry, from specifying models of general causal mechanisms to token causality over events, and from statistical effect inference to logic-based reasoning. As a result, we believe that LLMs offer an opportunity to bring together different approaches to causality and build unified pipelines that seamlessly transition between language-based and data-based analyses. Below we review these different approaches or subfields under which causality is studied. We also provide an introduction to LLMs and summarize methods for probing their capabilities. 2.1 Many Kinds of Causality The science of causality is the study of cause and effect relationships, and is a fundamental tool for understanding and reasoning about the world and how it works. Correct causal reasoning is crucial to making correct decisions, building robust systems, and scientific discovery itself. While advances in causal modeling have formalized core concepts of causality, the varying tasks and problems across domains have led different fields to use related but distinct conceptualizations and tools. Here we describe three orthogonal categories of causal approaches and tasks. The first axis (covariance- and logic-based causality) distinguishes methodological approaches that emphasize data analysis from those that emphasize logical reasoning. The second axis (type vs. token causality) 2 focuses on the setting for a causal question: Are we asking about causal relationships in general, such as for a population, or asking questions about specific causes for a specific event. The third axis, presented in Section 2.2 focus on the causal question or task itself: are we interested in inferring new relationships, characterizing the strength of specific known or hypothesized relationships, attributing blame or reward for some outcome, etc. Covariance- and Logic-based Causality: Many fields, including statistics, biostatistics, and econometrics, place primary emphasis on covariance-based causal analysis. This family of methods uses statistical approaches to infer and estimate the strengths of causal relationships from data (Imbens & Rubin, 2015; HernÃ¡n & Robins, 2010). Applications include the evaluation of drug efficacy, understanding effects of novel economic policies, and optimizing business decisions. A causal analysis typically starts with a question whose answer is converted to a statistical estimand and then estimated using statistical estimators that work with available data (Pearl, 2009a). Other domains such as law, forensic investigations, and fault diagnosis, often emphasizelogic-based causality, which uses logical reasoning and domain knowledge to reason about causal relationships in systems (Hellner, 2000). For example, some notions of legal liability involve establishing proximate cause for an event based on reasoning about counterfactuals and plausible scenarios (Knobe & Shapiro, 2021). Type and Token Causality: Type causality encompasses inference on causal relationships between variables, such as causal effect estimation (Peters et al., 2017). In contrast, token causality (also called specific causality) refers to inference of the degree to which specific events cause other events (Halpern, 2016; Hausman, 2005). That is, token causality is concerned with reasoning about specific events and their causes whereas type causality focuses on variables and their average effects. For example, questions concerning medical science such as â€œdoes smoking causes lung cancerâ€ or â€œwhat are causes of lung cancerâ€ or â€œhow much does smoking increase the risk of lung cancerâ€ are examples of type-causal inference. Scientists are often interested in type causality questions, because these questions help develop theory and make predictions. For example, average causal effect of an intervention tells us something about its effect on a general population and can be used to inform policy. In contrast, questions under the umbrella of token causality include â€œWas Fredâ€™s smoking habit responsible for his lung cancer? Or was it his exposure to asbestos?â€, or â€œWhat was the reason this machine failed?â€. These are questions that concern decision-making in specific situations and their answers need not generalize to other situations. Still, the answers are important to inform a specific decision or conclusion that needs to be made, e.g., deciding a legal case or 2Type causality is sometimes referred to as general causality. Token causality is also known as specific causality or actual causality, though the latter term also refers to the modeling framework for token causality primarily described in (Halpern, 2016). 5fixing a machine in a factory. In practice, beyond simple logical reasoning, to determine token causality, people use mental â€œforward simulationsâ€ of processes to predict the potential outcomes of an event, or attribute the underlying events led to an observed outcome (Hegarty, 2004; Jeannerod, 2001). 2.2 Different Causal Tasks and their Connection to Kinds of Causality Causal tasks or questions seek to reason about cause-and-effect relationships in a system, often with the goal of understanding and improving a desired outcome, as other mechanisms and environment change. This broad umbrella encompasses a large array of distinct causal tasks, with its own developed methodologies. Notably, while some of these tasks intersectâ€”they may share common abstractions (e.g., causal graphs) or the result of one task may be fed as an input of anotherâ€”an ability to solve one task does not imply an ability to solve the others. Specifying the causal graph is the type-causal task of specifying a graph that represents the underlying causal mech- anisms that govern a system. Causal discovery is an approach to causal graph specification that attempts to reverse engineer the causal graph from covariance in data. (Peters et al., 2017; Glymour et al., 2019). Effect inference is the task of characterizing the strength and shape of a known or hypothesized causal relationship. While it is most commonly characterized as relying on covariance-based methods, effect inference does rely on logical reasoning approaches for determining validity of causal assumptions (Sharma et al., 2021) and identifying approaches to validation and sensitivity analyses (e.g., negative controls (Lipsitch et al., 2010)). Effect inference is primarily focused in type causality scenarios. However, individual treatment effect estimation and counterfactual estimation straddle the realm of token causality. Attribution is the task of determining the cause or causes of a change. Depending on the application domain, approaches to attribution include both covariance-based and logical-based approaches, and are set in both type and token causality settings. For example, determining the likely cause of performance slowdowns in a large-scale Internet service is usually a covariance-based analysis in a type causality setting (Budhathoki et al., 2021); determining the root cause of a specific error in a system execution can be a covariance-based analysis in an token causality setting (Sharma et al., 2022); whereas determining the cause of a fire in an arson investigation may rely purely on logical-reasoning in an token causality setting (Halpern, 2016). Judgement tasks (Gerstenberg et al., 2014) extend attribution tasks to questions of reward or blame assignment for outcomes. They usually incorporate additional considerations of morality, normality, and intent of agents (Sloman & Lagnado, 2015). These are only some of the many causal tasks. Others include policy optimization, decision-making, explanation, scientific discovery, and others. 2.3 LLMs and Causality A large language model is a particular kind of machine learning model, built using transformers, a class of deep neural network architectures (Devlin et al., 2019). The primary task of LLMs is next-word completion. Initially, next-word prediction is based primarily on word distribution probabilities. Further training applies additional human feedback to shape the reward function to take into account factors beyond word distribution probability, such as instruction following and safety (Ouyang et al., 2022). Recent work has explored causal capabilities of LLMs (Willig et al., 2022). For instance, Long et al. (2023) consider simple graphs of 3-4 nodes and test whether LLMs can recover the correct structure. They consider each variable pair (A,B) and ask an LLM to score two competing statements, one implying that A causes B and the other that B causes A. Tu et al. (2023) consider a harder task using a dataset on medical pain diagnosis and find that LLM-based graph inference obtains poor accuracy. On the question of inferring causality from natural language, Hobbhahn et al. (2022) study whether LLMs can understand causal implications embedded in natural language, i.e., given two sentences, whether (the event in) one sentence is the cause of another. In this paper, we extend this line of work and make two contributions. First, we investigate causal graph generation capabilities of LLMs, as an analog to covariance-based causal discovery, over a broad set of complex real-world datasets and explore the robustness of LLM-based graph generation. As one example, in Section 3.1, we revisit the case study on LLM-based graph graph generation from Tu 6et al. (2023) and show that with an appropriate prompt, LLMs can achieve substantially higher accuracy (increasing the F1 score for retrieved edges from 0.21 to 0.68). Second, we probe the ability of LLMs to do counterfactual reasoning and infer necessary or sufficient cause based on natural language description of events. We also provide a unifying framework that shows how LLMs can be used to transfer knowledge between covariance-based and logic-based causal methods for a given real-world problem. 2.4 Probing LLM behaviors The primary input-output mechanism exposed by an LLM is a textual prompt as input and a textual response as output. Given black-box access to an LLM, therefore, the main way to understand its causal capabilities is to probe it with different inputs and observe how its output changes. This probing paradigm of building causal arguments, however, has the classic limitation of construct validity for the measurements (Smith, 2005). That is, we may be tempted to ascribe a particular causal capability to an LLM if it answers well on a set of questions related to the capability, but the answers may not necessarily be due to the capability; they may be due to other factors such as exploiting some structure in the questions, or in the case of LLMs, memorizing similar questions that it encountered in its web-scale training set (Carlini et al., 2022). Hence we use multiple probing strategies to understand the causal capabilities of LLMs. In addition to the standard practice of testing LLM performance on existing benchmarks, we conduct memorization tests to estimate the extent to which a benchmark dataset may have been memorized, construct novel datasets to avoid memorization concerns, and conduct semantically-equivalent perturbation and other interventions on the input prompts to understand what the LLM pays attention to. Benchmark Tests and Question-Answer Evaluation: The standard approach to evaluating LLM behavior is to ask it questions and evaluate its answers. Our question is the prompt for the LLM; and the LLMâ€™s completion of the text is the answer. Most large scale benchmarks and evaluations follow this approach, using purposefully created benchmarks and/or adapting standardized exams written to assess human capability (Ghazal et al., 2017; Nori et al., 2023; Zhong et al., 2023). Our evaluation approach to characterizing the causal capabilities of LLMs begins with such question-answering evaluations. For each evaluation, we ask a series of questions or causal challenges and score the correctness of the resulting answer. Then, we probe deeper, to better understand the threats to the validity of the question-answer evaluations, and the likely robustness of LLM capabilities. Memorization Test3: The primary threat to the validity of benchmark or question-answer style assessments is that the LLM may have directly memorized the benchmark answers. In this case, our questions are likely not testing the LLMâ€™s inherent capability to complete a task (unless memorization is the capability we are testing!) To test whether the LLM has memorized a particular dataset or benchmark, we give the LLM a partial row of data from the dataset and ask it to autocomplete the remainder of the row. For non-tabular datasets, we present a snippet of the file and ask it to auto-complete from that starting point. To encourage the LLM to succeed, we prepend details about the dataset, such as its name, URL, and description, and also provide few-shot examples. The final measurement from the memorization test is the percentage of rows the LLM was able to regenerate correctly. Supplementary E.1 provides additional details on the procedure. Redaction Test: When we see an LLM generate a correct answer to a question, we are still sometimes unsure why. Is the LLM attending to the appropriate aspects of the question, or has it learned to repeat unreliable patterns that may lead to errors in the future. To better understand what aspects of the prompt or question an LLM is attending to, we use redaction and perturbation tests, motivated from explainable AI methods for NLP Sinha et al. (2021); Danilevsky et al. (2020). First, we redact words in the prompt, one by one, and see how the answer changes each time. Changes in the answer indicate the LLM is attending to the redacted word. 3Our approach to testing for memorization and prior exposure of the LLM follows the method as developed in Bordt et al. (2024). 73 Generating causal graphs using LLMs The task of building a causal graph is a foundational step in a causal inference workflow. Typically, the goal is to obtain a directed graph where the edges denote the presence and direction of causal effect. Such a graph characterizes the underlying data-generating process (DGP) for a dataset and specifies how a change in one variable may (or may not) affect the others. This graph is then used as a base on which downstream analysis relevant to a task is conducted, such as for effect inference, prediction or attribution. Having the correct graph that encodes causal assumptions is critical for ensuring the correctness of any downstream analysis. Causal discovery attempts to address the challenge of specifying a causal graph by searching for a graph consistent with observed evidence of covariance between the variables in data Peters et al. (2017). The allure of causal discovery is relying on empiricism and avoiding subjective model misspecification errors that could bias the downstream workflow. The challenge, however, is that it is generally not possible to learn the correct graph for a given dataset, given only observational data. The reason is that multiple graph structures are equally likely given the same data distribution, a set of graphs known as the Markov equivalence class (Pearl, 2009c). In the last two decades, two main approaches have been proposed to overcome this limitation. The first is to restrict data-generating process to specific functional forms under which identification of a single graph is possible (Glymour et al., 2019). In some specific settings, such as adding non-gaussian noise to linear data-generating process (Shimizu et al., 2006) or assuming that all functions are non-linear with additive noise (Zhang & Chan, 2006; Zhang & HyvÃ¤rinen, 2009), identification is possible. However, there still exist simple settings that are unidentifiable, e.g., a dataset with linear equations and gaussian noise. The second approach is to utilize the power of deep learning to model covariances of all variables jointly and hope that it improves the quality of learnt graphs. However, the identification issue is still not resolved. As a result, recent evaluations of state-of-the-art causal discovery methods on real-world datasets present a sobering picture of their effectiveness (Kaiser & Sipos, 2022; Tu et al., 2019; Huang et al., 2021). LLMs offer a fresh perspective on the use of statistical algorithms for constructing a causal graph. This perspective focuses on the metadata associated with variables in a dataset, rather than their data values. Typically, such metadata- based reasoning is done by human domain experts when they construct causal graphs. By looking at the names of variables and the problem context, for example, people can construct a causal structure based on their knowledge of physics, common sense, or specialized domain knowledge. However, this is a challenging process, hindering the widespread use of such metadata-based reasoning. We find that LLMs can bridge this gap by filling in the domain knowledge that earlier only humans could provide. In contrast to causal discovery algorithms that use data values of variables, LLMs can infer causal structure by reasoning on metadata associated with the variables, for example, the name of the variable and the problem context expressed in natural language. In other words, similar to how domain experts formalize their knowledge in a graph, LLMs can use the knowledge from their training data to infer the edges of the causal graph. To differentiate from the existing covariance-based causal discovery, we call the LLMs capability asknowledge-based causal graph generation. Remarkably, LLMs practicing knowledge-based causal graph generation outperform state-of-the-art covariance-based algorithms on causal discovery benchmarks. Below we present experiments on inferring causal structure using LLMs. We first start with pairwise causal discovery: a simple task involving two variables where the goal is to decide the direction of causal effect, across a range of domains. Next, we will study the problem of full graph discovery in two datasets: one in medicine and one in climate science. While the first task includes many â€œsimpleâ€ relationships that an average person is expected to answer correctly, the other two tasks require specialized knowledge (in neuropathic pain and arctic atmosphere science respectively) in order to check whether LLMâ€™s graph generation capabilities extend to complex domains. 3.1 Pairwise causal edge inference: Inferring causal direction among variable pairs We start with the pairwise causal edge inference task (Hoyer et al., 2008). Typically, these tasks are set up as discovering causal relations between two variables based on observed data. In this work, instead of relying on observed data, we directly ask large language models whether a variable causes another variable. 8Variable A Variable B Domain Age of Abalone Shell weight Zoology Cement Compressive strength of concrete Engineering Alcohol Mean corpuscular volume Biology Organic carbon in soil Clay content in soil Pedology PPFD (Photosynthetic Photon Flux Density) Net Ecosystem productivity Physics Drinking water access Infant mortality Epidemiology Ozone concentration Radiation Atmospheric Science Contrast of tilted Gabor patches Accuracy of detection by participants Cognitive Science Time for 1/6 rotation of a Stirling engine Heat bath temperature Engineering Time for passing first segment of a ball track Time for passing second segment Basic Physics Table 1: Example cause-effect pairs from the TÃ¼bingen benchmark. The task is to determine whether Variable A causes Variable B, or vice-versa. 3.1.1 TÃ¼bingen cause-effect pairs dataset This dataset (Mooij et al., 2016) consists of data for 108 different cause-effect pairs selected from 37 datasets from various domains, including meteorology, biology, medicine, engineering, and economics (see examples in Table 1). As directionality of an edge is a fundamental building block for learning the full graph, it is a widely used dataset for benchmarking causal discovery algorithms. However, the Markov equivalence class of graphs admissible for a two-node graph (A,B) contains both A â†’B and B â†’A. Therefore, the dataset remains a challenging one for causal discovery algorithms, with many recent methods achieving less than 80% accuracy, as shown in Table 2. The best known accuracy is 83%, achieved by the Mosaic algorithm (Wu & Fukumizu, 2020). We now apply LLMs to the pairwise edge inference problem. We extract the names of each variable from the benchmark and use them to construct prompts for the LLMs. Example prompts are shown in Suppl. A.1 (Table 14). We start with a simple prompt that asks, \"Does changing A cause a change in B?\", where A and B are the variable names. For a given pair (A, B), we ask the question in both directions and then take the mean accuracy. We choose the mean accuracy since it maps correctly to the accuracy of any method choosing between A â†’B or B â†’A. Specifically, the LLM-based method obtains an accuracy of 0 if it answers both questions incorrectly. If it answers one of the questions correctly (indicating that none cause each other or both cause each other), then it effectively chooses the output at random (A â†’B or B â†’A). We also report weighted accuracy, as recommended by Mooij et al. (2016) to avoid the effect of overcounting some similar pairs. Results. The second half of Table 2 shows the performance of different LLMs on the task. Similar to prior studies on capabilities of LLMs (Wei et al., 2022a), we observe the emergent behavior that only text-davinci-002 and above achieve a non-trivial accuracy than random chance. With GPT3.5 class models (text-davinci-003 and gpt-3.5-turbo), accuracy of LLM-based method reaches 83% and is competitive to the covariance-based causal discovery algorithms. This suggests that the ability of inferring causal direction emerges as the size of the model is increased. It may also depend on the specific training data and the use of reinforcement learning from human feedback (RLHF) for instruction finetuning. Following recent work showing the value of the prompt tuning for obtaining high-quality results (Wei et al., 2022b; Long et al., 2023), we employ two different kinds of prompts. In the first, we prepend the prompt with the message, \"You are a helpful assistant for causal reasoning\", with the intention of steering the LLMâ€™s output space towards more causally consistent answers. This simple message of asking gpt-turbo-3.5 (i.e., ChatGPT) to be an agent of causal reasoning increases the (weighted) accuracy by almost 5% to 86.9%, consistent with the impact of prompt engineering observed by (Long et al., 2023). The accuracy is higher than previously reported LLM-based accuracy on the task from Choi et al. (2022) that used the davinci-instruct-beta model (83% unweighted accuracy). Moreover, by changing the prompt to instead ask a single question that asks the LLM to output the more likely causal direction between A â†’B or A â†B while explaining its reasoning in a â€œstep-by-stepâ€ manner (see Suppl. A.1), the weighted accuracy improves to 9Model Acc. Wt. Acc. Slope (Marx & Vreeken, 2017) 0.75 0.83 bQCD (Tagasovska et al., 2020) 0.68 0.75 PNL-MLP (Zhang & Hyvarinen, 2012) 0.75 0.73 Mosaic (Wu & Fukumizu, 2020) 0.83 0.82 ada 0.50 0.50 text-ada-001 0.49 0.50 babbage 0.51 0.50 text-babbage-001 0.50 0.50 curie 0.51 0.52 text-curie-001 0.50 0.50 davinci 0.48 0.47 text-davinci-001 0.50 0.50 text-davinci-002 0.79 0.79 text-davinci-003 0.82 0.83 LMPrior (Choi et al., 2022) 0.83 - gpt-3.5-turbo 0.81 0.83 gpt-3.5-turbo (causal agent) 0.86 0.87 gpt-3.5-turbo (single prompt) 0.89 0.92 gpt-4 (single prompt) 0.96 0.97 Table 2: Accuracy of different versions of GPT on the TÃ¼bingen cause-effect pairs dataset. The best LLM performance outperforms the current state-of-the-art covariance-based approaches that rely on observational data of the two variables. Weighted accuracy weights individual pairs to account for overcounting due to some pairs sharing the same source dataset. The causal agent is gpt-3.5-turbo with system message set as â€œYou are a helpful assistant for causal reasoning.â€. LMPrior uses davinci-instruct-beta. 92.1%. Finally, using gpt-4 with this prompt, the weighted accuracy shoots up to 97%. This accuracy is substantially higher than that of the best-known covariance-based causal discovery method. Understanding LLM output. The combined single prompt allows us to inspect the LLM output, since it provides an explanation in addition to its final answer. For the examples below, we show the output from gpt-3.5-turbo. Let us look at the example, (age of abalone, length of abalone) where the LLM correctly outputs that changing the age of an abalone is more likely to change its length. Figure 3(a) shows the LLMâ€™s output. Its explanation gives the impression that it can reason about the problem using its knowledge to arrive at the correct answer: However, on another pair that arguably should involve similar reasoning, (age of abalone, diameter of abalone), the LLM outputs an incorrect answer and its explanation does not make much sense (Figure 3(b)). The fact that the LLM can make such errors is not surprising, since it is not explicitly optimizing for being consistent with prior knowledge or causal mechanisms. However, it is remarkable that such errors occur infrequently: The LLM made only 12 errors in the dataset of 108 pairs. On some other pairs where the LLMâ€™s output did not match the ground-truth, it reveals possible ambiguity in the variable names, that can be fixed by adding more context to the question. Consider the example of (ozone concentration, radiation) below where the LLM initially provides an incorrect answer. LLM : 1. Ozone is a gas in Earthâ€™s atmosphere that absorbs most of the Sunâ€™s ultraviolet (UV) radiation. 2. Chlorofluorocarbons (CFCs), pollutants that were once widely used in refrigerants and aerosol sprays, have been found to destroy ozone molecules in the atmosphere. 3. The destruction of the ozone layer in the upper atmosphere causes more UV radiation to reach the surface of Earth. 10SYSTEM : You are a helpful assistant for causal reasoning. USER : Which cause-and-effect relationship is more likely? A. changing the age of an abalone causes a change in its length. B. changing the length of an abalone causes a change in its age. Letâ€™s work this out in a step by step way to be sure that we have the right answer. Then provide your final answer within the tags <Answer>A/B</Answer>. GPT-3.5- TURBO : 1. Cause: Changing the age of an abalone Effect: a change in its length. 2. Cause: Changing the length of an abalone Effect: a change in its age. Based on the biological knowledge about abalones, it is more likely that changing the age of an abalone causes a change in its length. This is because the length of an abalone is known to in- crease as it ages, and not the other way around. Therefore, the correct answer is <Answer>A</Answer>. (a) Correct answer SYSTEM : You are a helpful assistant for causal reasoning. USER : Which cause-and-effect relationship is more likely? A. changing the age of an abalone causes a change in its diameter. B. changing the diameter of an abalone causes a change in its age. Letâ€™s work this out in a step by step way to be sure that we have the right answer. Then provide your fi- nal answer within the tags <Answer>A/B</Answer>. GPT-3.5- TURBO : Step 1: Determine which variable is the cause and which is the effect. In Option A, â€œageâ€ is the cause and â€œdiameterâ€ is the effect. In Option B, â€œdiameterâ€ is the cause and â€œageâ€ is the effect. Step 2: Consider which variable is more likely to cause a change in the other. Abalone growth is primarily determined by environ- mental factors such as water temperature, food avail- ability, and water quality. It is unlikely that changing the age of an abalone would cause its diameter to change significantly. On the other hand, changes in the environment could potentially impact the size of the abalone, and therefore, its diameter could change. Therefore, Option B is more likely: changing the diameter of an abalone causes a change in its age. <Answer>B</Answer> (b) Incorrect answer Figure 3: Probing reasoning of LLMs for deciding causal direction. Two example outputs from a LLM (gpt-3.5- turbo). The first gives the impression of the LLM performing causal reasoning to arrive at the correct answerâ€”that age may cause the length of abalone to increase. The second example asks a question that requires arguably the same causal knowledge but the LLM proceeds with an incoherent argument and provides an incorrect answer. 4. Increased UV radiation can lead to various negative effects such as skin cancer, cataracts, and damage to crops. From these facts, we can determine that a decrease in ozone concentration would lead to an increase in radiation. Therefore, Option A is more likely: <Answer>A</Answer> The correct answer according to the benchmark is that radiation causes ozone concentration. However, the LLM interpreted the question as about the ozone concentration in the stratospheric ozone layer, whereas the question was about ground-level ozone concentration measured in a city. After adding this context, the LLMâ€™s answer changes, USER : Wait, this question is not about stratospheric ozone concentration. Rather, Iâ€™m asking about the ground-level ozone concentration measured in a city and the corresponding ground-level radiation due to sunlight. GPT-3.5- TURBO :: I see, thank you for clarifying. In that case, the answer is likely option B: changing radiation causes a change in ground-level ozone concentration. This is because ground-level ozone is formed through a chemical reaction between nitrogen oxides (NOx) and volatile organic compounds (VOCs) in the presence of sunlight. This reaction is known as photochemical smog and occurs more frequently on hot, sunny days when there is more radiation. Therefore, an increase in radiation would likely lead to an increase in ground-level ozone concentration. 11<Answer>B</Answer> Finally, in some cases, the direction of the cause-effect relationship is not absolute and depends on other supporting information. For the pair (stock return of Hang Seng bank, stock return of HSBC Hldgs.), the LLM correctly determines that Hang Seng is a subsidiary of HSBC Hldgs. but its answer does not match the benchmarkâ€™s label. HSBC Hldgs. owns a majority stake in Hang Seng bank, so the benchmark answer indicates that any change in Hang Sengâ€™s stock return should also affect HSBC Hldgs. stock return. At the same time, since stock markets do not follow any fixed pattern, it is possible that a major change in HSBC Hldgs. stock return causes people to invest more or less in Hang Seng Bank and hence affect its stock return, as the LLM output argues. LLM : To determine which cause-and-effect relationship is more likely, we need to consider the nature of the two companies and their relationship with each other. Hang Seng Bank and HSBC Holdings are two separate companies but they are both major players in the Hong Kong financial market. Hang Seng Bank is a subsidiary of HSBC Holdings and is listed on the Hong Kong Stock Exchange. HSBC Holdings is a multinational banking and financial services company that operates in over 60 countries worldwide. Given that relationship and the fact that Hang Seng Bank is a subsidiary of HSBC Holdings, it is more likely that: <Answer>B</Answer>changing the stock return of HSBC Hldgs causes a change in the stock return of Hang Seng Bank. Creating a novel Tubingen dataset. It is possible that LLMs achieve high accuracy in predicting causal direction because Tubingen pairs is a popular dataset which may have been included in LLMsâ€™ training data. To test generalization of LLMs to causal pairs that were not a part of LLMsâ€™ training data, we build a new dataset. Since the training cutoff date for the models we use (GPT-3.5-turbo and GPT-4) is September 2021, we aim to collect cause-effect pairs that were published after the cutoff date. To do so, we use Google Book search to search for cause-effect pairs in books published on or after Oct 2021. We first categorize pairs from the original dataset into domains such as biology, engineering, medicine, and so on. For each of these categories, we search for appropriate books that describe a causal relationship in their text. Based on this procedure, we obtain a dataset with 67 variable pairs. For instance, the variable pair, â€œbattery capacityâ€, â€œambient temperatureâ€ was derived from the book, Battery Management System and Its Applications, published in 2022. For a sample list of variables, see Appendix A.3. The full dataset is available at https://github.com/py-why/pywhy-llm. On this dataset, we applied the exact same (single) prompt from the original benchmark. GPT-3.5-turbo and GPT-4 obtain 80.3% and 98.5% accuracy respectively, indicating that the capability to identify causal direction generalizes to variable pairs outside of popular datasets. 3.1.2 Neuropathic pain dataset We now consider a ground-truth causal graph constructed by medical experts on neuropathic pain diagnosis (Tu et al., 2019). It contains relationships between different nerves and the associated symptoms that patients express. Table 3 shows example of edges in the graph. Unlike the TÃ¼bingen benchmark which contains some commonly known variables, understanding the names of variables in this graph requires specialized medical knowledge. We evaluate LLMs on a pairwise causality task similar to the Tubingen benchmark. We do so by creating a pair for each edge in the graph and setup the task to determine the direction of the edge. That is, for all the edges present in the ground-truth graph, we ask an LLM to infer the correct direction of causality. We obtain a dataset of 475 edges. Table 4 shows the accuracy of different LLMs in determining the direction of edges. Similar to the TÃ¼bingen benchmark, smaller language models are unable to obtain accuracy substantially higher than random chance (50%). In addition, even text-davinci-003, the largest text completion model in our comparison set, is unable to obtain a high accuracy using the two-sided prompt. However, chat-based models, trained using human feedback, are able to distinguish causal 12Variable A Variable B Dir. Right L1 Radiculopathy Right adductor tendonitis â†’ Pharyngeal discomfort Right C3 Radiculopahty â† Right L5 Radiculopathy Lumbago â†’ Left PTA Left L4 Radiculopahty â† Left T3 Radiculopahty Toracal dysfunction â†’ DLS L5-S1 Right S1 Radiculopathy â†’ Left C3 Radiculopathy DLS C2-C3 â† Left C7 Radiculopathy Left medial elbow problem â†’ Right Ischias Right L5 Radiculopathy â† Right Morton trouble Right L5 Radiculopathy â† Table 3: Example cause-effect pairs from the Neuropathic pain diagnosis benchmark. â€˜Dir.â€™ refers to the ground-truth causal direction between the variables. Model Accuracy ada 40.1 text-ada-001 50.0 babbage 50.0 text-babbage-001 50.9 curie 50.0 text-curie-001 50.0 davinci 38.4 text-davinci-001 50.0 text-davinci-002 51.7 text-davinci-003 55.1 gpt-3.5-turbo 71.1 gpt-3.5-turbo (neuropathic pain expert) 75.1 gpt-4 78.4 gpt-4 (neuropathic pain expert) 84.3 text-davinci-003 (single prompt) 86.0 gpt-3.5-turbo (single prompt) 85.5 gpt-4 (single prompt) 96.2 Table 4: Accuracy of different versions of GPT on the inferring the edge directions of the Neuropathic pain diagnosis graph. As with the TÃ¼bingen dataset, LLMs like gpt-3.5-turbo obtain more than 85% accuracy on determining the direction of edges. The causal agent is gpt-3.5-turbo with a system message set as â€œYou are a helpful assistant for causal reasoning.â€ direction accurately. With the two-sided prompt, gpt-3.5-turbo achieves the highest accuracy of 75%. The choice of prompt also has a big impact. As with the TÃ¼bingen dataset, using single prompt per variable pair leads to a significant increase in accuracy: text-davinci-003 now matches gpt-3.5-turbo and both models obtain more than 85% accuracy. While the performance of these models is impressive, gpt-4 obtains an even higher accuracy (96%), yielding more than 457 correct responses out of 475. Understanding LLM output. We use the output of gpt-3.5-turbo (single prompt) to understand how LLMs obtain such high accuracies. On manual inspection of the output by gpt-3.5-turbo, we find that it correctly understand the medical terms in almost all cases. In the example below, the LLM provides the correct definitions of DLS T5-T6 and Left T6 Radiculopathy (without access to any such information in the prompt) and provides the correct causal direction. Suppl. A.2 provides the LLM output for this pair. However, even as it provides the correct definitions, it can provide erroneous output in unpredictable ways. Here it went from the correct definition of the same concept (DLS) to confusing it with the â€œT5-T6 region\" associated with DLS T5-T6 (see Suppl. A.2 for the full output). 13Moreover, the example in Table 5 adds evidence that LLMs may not be capable of consistent and coherent reasoning, even as they output correct answers for a majority of the causal questions. In the left panel, the LLM provides the correct reasoning for most of its answer but ultimately chooses a final answer inconsistent with its own reasoning (A instead of B). Unlike reasoning errors, fortunately self-consistency is an easier problem to verify using the LLM itself. We find that gpt3.5-turbo and earlier class of models are unable to check consistency of an output, but GPT-4 is able to. Therefore, it may be possible to pass all LLM outputs through a GPT-4 verifier to ensure self-consistency of the outputs. The right panel of Table 5 shows the verifier prompt and output from GPT-4 (the question and answer from gpt-3.5-turbo are not fully shown for brevity). Initial response Critiquing the response SYSTEM : You are an expert on neuropathic pain di- agnosis. USER : You are a helpful assistant to a neuropathic pain diagnosis expert. Which cause-and-effect relationship is more likely? A. Right C7 Radiculopathy causes DLS C6- C7. B. DLS C6-C7 causes Right C7 Radiculopa- thy. Letâ€™s work this out in a step by step way to be sure that we have the right answer. Then provide your final answer within the tags <An- swer>A/B</Answer>. GPT-3.5- TURBO : First, we need to understand what each term means. DLS stands for degenerative lumbar spondylosis, which is a condition where the spinal discs in the lower back degenerate and cause pain. C6-C7 refers to the sixth and seventh vertebrae in the cervical (neck) spine. Radiculopathy is a condition where a nerve root is compressed or damaged, causing pain, numbness, or weakness in the corresponding area of the body. Based on this information, it is more likely that option A is the correct cause-and-effect relationship. This is because radiculopathy is often caused by compression or damage to a nerve root, which could be the result of degen- erative changes in the spine (such as DLS). In this case, the degeneration in the lower back (DLS) could be causing compression or damage to the nerve root at the C7 level, resulting in right C7 radiculopathy. Therefore, the answer is <An- swer>A</Answer>. USER : Analyze the output from an AI assistant. Is the final answer (A) consistent with the reasoning provided by the assistant? Question: {{USERPROMPT-FROM-BEFORE}} AI assistant: {{ANSWER-FROM- BEFORE}} GPT-4- WEB : The final answer (A) is not consistent with the reasoning provided by the assistant. The reasoning suggests that DLS C6-C7 could cause Right C7 Radiculopathy, which corre- sponds to option B. The correct answer should be <Answer>B</Answer>. Table 5: Critiquing LLM output using another LLM instance. To increase robustness of an LLMâ€™s response, we can use GPT-4 as a critic. The left panel shows an incorrect reply from gpt-3.5-turbo wherein the reasoning is correct but the LLM outputs the incorrect option (A). We create a special â€œcritiqueâ€ prompt that asks gpt-4 to evaluate the response from an AI assistant for self-consistency. gpt-4 finds the logical inconsistency and provides the correct answer. 14In addition to self-consistency, when LLM outputs do not match the ground-truth, they can sometimes help critique the ground-truth itself. For instance, the ground-truth graph contains an edge from Left L5 Radiculopathy to Obesity. However, the LLM output provides the opposite direction: obesity can cause L5 Radiculopathy, which is corroborated by research that claims obesity as a risk factor for radiculopathy (Atchison & Vincent, 2012; TomiÂ´c et al., 2009). LLM : Step 1: Understand the terms - Neuropathic pain: pain caused by damage or disease affecting the nervous system - L5 Radiculopathy: a condition where the nerve root that exits the spine at the L5 level is compressed or irritated, causing pain, numbness, or weakness in the leg - Obesity: a medical condition characterized by excessive body fat that increases the risk of health problems Step 2: Analyze the cause-and-effect relationship - Option A: Left L5 Radiculopathy causes Left Obesity. This relationship does not make sense because L5 radiculopathy is a nerve condition that affects the leg, while obesity is a whole-body condition related to excessive body fat. It is unlikely that a leg nerve condition would cause obesity. - Option B: Left Obesity causes Left L5 Radiculopathy. This relationship is more plausible because obesity can put extra pressure on the spine and nerves, leading to nerve compression or irritation. It is possible that obesity could cause L5 radiculopathy. Step 3: Choose the more likely cause-and-effect relationship - Based on the analysis, Option B is more likely: Left Obesity causes Left L5 Radiculopathy. <Answer>B</Answer> 3.1.3 Other properties of pair-wise relationships In addition to asking the LLM about the existence and direction of a causal relationship between a pair of edges, we can also ask many other questions as well. For example, we can ask if a relationship may be confounded by other variables, or what variables may mediate or moderate the relationship. We can ask whether a causal relationship is monotonic, whether its effects are homogeneous or not with respect to other properties. Other useful properties to identify include the time-lag of a causal relationship, whether it is stable, and whether it is subject to spillover effects. Designing prompts for these questions should be a straightforward extension of the pairwise edge existence prompts. 3.2 Full graph generation We now extend the pairwise analysis to generating the full graph. Given a set of variables, a straightforward extension is to construct a list of all possible variable pairs and repeat the pairwise test for each pair. However, full graph generation provides additional challenges. First, the pairwise analysis assumed the existence of an edge and our goal was simply to determine its direction. In the graph generation problem, there are three possible options for each variable pair: A â†’B, A â†B, or no edge exists. Second, graph generation requires the ability to distinguish between direct and indirect causes, given the other nodes in the graph. For example, if the true relationship is A â†’B â†’C, then it may be correct to output both A â†’B and A â†’C in a pairwise task, but for graph generation, outputting A â†’C is an incorrect answer. Moreover, the decision depends on which variables are present in the input variable set for the graph generation task. For example, if B is not one of the variables in the input variable set, then A â†’C is a valid edge and should be included. Due to these reasons, it is unclear how well a simple extension of the pairwise task using LLMs will transfer to the full graph generation problem. For simple graphs over 3-4 variables, Long et al. (2023) show that LLMs can obtain promising accuracy in inferring the edges. Below we report the results of applying the pairwise LLM prompts on more complex, real-world datasets involving specialized knowledge: on pain diagnosis and on artic sea ice coverage. 15Model Precision Recall F1 Random 0.25 0.5 0.33 chatGPT (Tu et al., 2023) 1 0.12 0.21 text-davinci-003 0.59 0.68 0.63 gpt-3.5-turbo (single prompt) 0.66 0.71 0.68 gpt-4 (single prompt) 0.74 0.58 0.65 Table 6: Accuracy of different versions of GPT on the Neuropathic pain dataset. 3.2.1 Neuropathic pain dataset We continue our exploration with the neuropathic pain dataset. It has 221 variables, so the total number of possible variable pairs are C(221, 2) = 24310. Before conducting full graph generation, we utilize a 100-pair dataset provided by the authors of the dataset to evaluate LLMâ€™s capability to infer causal relationships (Tu et al., 2023). It has 50 variable pairs that form edges in the graph and 50 pairs that do not form an edge. We notice that many of the variable names are in Swedish, so we employ an LLM (gpt-4-web) to translate the names to English as a preprocessing step. Results. Table 6 shows the results of different LLM-based methods. As a baseline, using an algorithm that return either direction at random for each pair, we would obtain 0.5 recall and 0.25 precision on recovering the true edges, leading to an F1 score of 0.33. Using a simple prompt, â€œA causes B. R and L refer to the right and left sides of the body respectively. Answer with true or false. â€, Tu et al. (2023) use chatgpt-3.5-web to obtain an F1 score of 0.21. This LLM performance is worse than the random baseline which may indicate that LLMs are not useful for full graph generation. However, with a different prompt, we find that the same model can provide over 3X increase in the F1 score. We provide the LLMs with an identical prompt to the â€œsingleâ€ prompt from the pairwise task, with one addition: we add a third option, â€œC: No causal relationship existsâ€. With this prompt, gpt-3.5-turbo (API analogue of chatgpt-3.5-web) obtains 0.68 F1 score. This score is double the F1 score expected from a random guess baseline, indicating that the output from LLMs provide a non-trivial utility for inferring causal edges. Using a novel medical dataset. It is possible that the Neuropathic dataset may be a part of the training set for the LLMs. Therefore, we consider a causal DAG proposed by Mascaro et al. (2023) that was published online in February 20224, a few months after the training cutoff date of the LLMs considered. The DAG captures expert-provided relationships concering COVID-19 disease processes. We consider a subset of the full DAG containing 11 nodes, as reported by Mascaro et al. in their paper. Variables correspond to concepts in respiratory medicine, such as â€œAlveolar Epithelial infectionâ€ and â€œViremiaâ€. We apply the same prompt that we used for the Neuropathic dataset. GPT-4 obtains an F1 score of 0.73 which is comparable to the best-reported F1 on the Neuropathic dataset (F1=0.68). 3.2.2 Arctic sea ice dataset To evaluate LLMâ€™s ability to generate causal graphs, we now consider a dataset from a different field of science: arctic sea ice and atmospheric science. This dataset is on the drivers of arctic sea ice thickness (or coverage): what causes the arctic sea coverage to increase or decrease? In a recent publication, Huang et al. (2021) provide a domain knowledge-based graph with 12 variables and 48 edges. Variables in the graph include total cloud water path, sea level pressure, geopotential height, meridional and zonal wind at 10m, net shortwave and longwave flux at the surface. Importantly, the graph contains some double-sided edges. Since the number of variables is low (12), we are able to conduct an experiment to generate the full graph using LLMs. Huang et al. (2021) evaluate the accuracy of three recent discovery algorithms: temporal causality discovery framework (Nauta et al., 2019), NOTEARS (Zheng et al., 2018), and DAG-GNN (Yu et al., 2019). They evaluate the algorithms on the normalized Hamming distance (NHD) between the predicted graph Gâ€²and the ground-truth graph G. 4The first article describing the dataset was published on medRxiv in February 2022 and the dataset was released in the same month (URL: https://osf.io/bynr6/). The OSF project, itself, was created on 1st October 2021. 16Algorithm NHD No. of predicted edges Baseline NHD Ratio TCDF 0.33 9 0.39 0.84 NOTEARS (Static) 0.33 15 0.44 0.75 NOTEARS (Temporal) 0.35 7 0.38 0.92 DAG-GNN (Static) 0.32 23 0.49 0.65 DAG-GNN (Temporal) 0.34 16 0.44 0.77 gpt-3.5-turbo 0.33 62 0.76 0.43 gpt-4 0.22 46 0.65 0.34 Table 7: Normalized hamming distance (NHD) for different causal discovery algorithms. Since NHD depends on the number of predicted edges, we compare the ratio of NHD and baseline NHD across algorithms. A lower NHD ratio is better. LLM-based graph generation (gpt-3.5-turbo) obtains comparable NHD and the lowest NHD ratio compared to recent covariance-based discovery algorithms. For a graph with m nodes, the distance is given by âˆ‘m i,j=1 1 m2 1Gij Ì¸=Gâ€² ij , the number of edges that are present in one graph but not the other, divided by the total number of all possible edges. The distance is zero if Gâ€²outputs exactly the same edges as G. For reference, since the number of nodes and edges in our ground-truth graph are 12 and 48 respectively, if Gâ€²outputs 48 completely different edges compared to the 48 edges of G, the NHD between G and Gâ€² will be 0.66. However, if we output no edges at all, the NHD will be better than the algorithm, 0.33. Since the NHD depends on the number of edges returned by a discovery algorithm, we compare the ratio of NHD of the discovery algorithm and a â€œfloorâ€ baseline that outputs the same number of edges but all of them are incorrect. A lower ratio implies the multiple by which the graph discovery algorithm is better than the worst baseline returning the same number of edges. Table 7 shows the NHD metric for the three covariance-based causal discovery algorithms and knowledge-based graph generation with LLMs. For each algorithm, we also show the number of predicted edges and the NHD for a worst case algorithm that predicts the same number of edges. NHD for the three covariance-based algorithms is similar. However, the ratio of NHD for the algorithm and the baseline algorithm provides the relative improvement that the algorithm provides compared to the baseline. Using the ratio metric, we see that DAG-GNN (static) performs the best and has the lowest Ratio distance. For LLM-based graph generation, we use the same prompt as in the neuropathic pain dataset. Using gpt-3.5-turbo, we obtain a normalized hamming distance of 0.33, comparable to the three recently proposed covariance-based causal discovery algorithms. However, the LLM returns 62 edges, so its NHD ratio (0.43) is substantially better than covariance-based algorithms. Finally, gpt-4 obtains the lowest NHD among all algorithms and significantly outperforms state-of-the-art covariance-based discovery algorithms. It outputs almost the same edges as the ground-truth (46) with an NHD of 0.22, one-third less than other algorithms. Its NHD ratio is also the lowest. gpt-4 recovers over half (29) of the edges correctly with a precision of 0.63 (F1 score=0.57), indicating the competitiveness of LLM-based algorithms for causal generation. Measuring LLMâ€™s NHD metric on a novel dataset. In addition to the Arctic sea ice dataset, we consider a dataset that was created in 2023 by consultation with medical experts on Alzheimerâ€™s disease (Abdulaal et al., 2023). Since the causal graph was created after the training cutoff date of September 2021, it cannot be present in the training data for LLMs. Abdulaal et al. (2023) present an expert-validated causal graph of 11 nodes. Variables in the graph include age, ventricular volume, brain MRI, etc. They use the same prompt that we use for the Arctic sea ice dataset and report NHD of the obtained graphs using different LLMs. Table 16 in Appendix A.3 reports their results on the dataset. The results show that the graph generated by gpt-4 obtains the lowest NHD (0.14) compared to existing existing data-based methods such as NOTEARS (0.22) and DAG-GNN (0.37). Its NHD ratio is also the lowest (0.28). These results, similar to those obtained for the Arctic sea dataset, indicate the generalizability of LLMs like GPT-4 to for inferring full graph over novel datasets. Note that novel datasets may still contain concepts that existed before the LLMâ€™s training cutoff date. See Section 5.1 for a discussion on utility of LLMs with respect to novel scenarios. 17Which cause-and-effect relationship is more likely? A. changing SLOT1 causes a change in SLOT2. B. changing SLOT3 causes a change in SLOT4. Letâ€™s work this out in a step by step way to be sure that we have the right answer. Then provide your final answer within the tags <Answer>A/B</Answer>. Figure 4: We probe the importance of individual words for getting a correct answer by redacting a random word from a question. We highlight words based on their importance for getting a correct result. A white background indicates that redacting the word did not reduce accuracy. A dark blue highlight indicates that redacting the word reduced accuracy the most. 3.3 Probing LLM behavior further To better characterize the validity and robustness of our experiments at the intersection of LLMs and building causal graphs, we report the results of our LLM probing experiments, described in Sec 2.4. Memorization Test Results: We provide the LLM with the first 3 columns of the dataset (the row ID and 2 variable names), and ask the model to complete the remaining 2 columns of the row (the source dataset name and the ground truth causal direction. We find that GPT-3.5 is able to correctly recall 58% of the remaining cells in the dataset, and recall 19% of the rows without error. Our memorization test with GPT-4 is able to recall 61% of remaining cells in the dataset and 25% of rows without error. These results indicate that the TÃ¼bingen dataset is certainly in GPTâ€™s training set and that GPT has had an opportunity to memorize the dataset. However, the gap between percentage of inputs memorized and LLMs accuracy is still significant and more work is needed to understand the role of memorization for the inputs that are not captured by our memorization test. As the neuropathic pain and the arctic sea ice coverage datasets are not available online in a tabular file format, we adapted our memorization test to ask for completions of randomized samples of JSON and code files 5. We provide the LLM with a contiguous section of text from a randomly chosen location within a file, and ask the model to complete the file from that point. Counting the number of characters that are correctly reproduced by the LLM, provides an indication of whether the dataset has been memorized by the model. Our results, detailed in Section E.1, indicate that the neuropathic dataset has been at least partially memorized by GPT-3.5 Turbo, but not GPT-4, and the arctic sea ice dataset has not been memorized by either GPT-3-5 Turbo or GPT-4. More generally, we expect knowledge-based causal graph generation to be founded partly on memorized information, but also dependent on the LLMâ€™s ability to process and transform seen causal relationships for use in multiple contexts. We can conceptually model the LLMâ€™s end-to-end efficacyP(Y |D)P(D), where P(D) represents the likelihood of a causal relationship being memorized by the LLM, and P(Y |D) the likelihood that the relationships can be correctly transformed. Knowing that our benchmark dataset is memorized implies that our experiments can only measuring P(Y |D), but not measure P(D). That is, when evaluating an LLM on an existing benchmark, its performance on the benchmark is a test of how well the LLMâ€™s ability to process and transform its knowledge into the necessary causal relationship assuming that it has been trained using some representation of the underlying information. Redaction Test: To characterize whether or not the LLM is attending to the correct words and concepts in a question when answering our pair-wise causal edge inference questions, we use a redaction test. Our redaction test shows what words are important for correctly labeling a causal relationship between two features. Figure 4. shows our results for experiments in gpt-3.5-turbo, averaged across 357 random redaction probes over our TÃ¼bingen experiment. If the LLM is correctly reading the question, we expect that it will pay a lot of attention to key instructions and to the options themselves. Here we see that some of the key instruction words that indicate we are asking about causal relationships, such as â€™changingâ€™, and â€™causesâ€™ as well as instructions for how to format the final answer in tags are 5For the neuropathic dataset, we use the JSON representation available on the public web at https://observablehq.com/@turuibo/ the-complete-causal-graph-of-neuropathic-pain-diagnosis , and for the arctic sea ice dataset, we use a python file that embeds a 12x12 binary matrix representation of the causal graph, https://github.com/big-data-lab-umbc/cybertraining/ blob/2179fb771dcb69d4f302f693b30e057c366f6be1/year-3-projects/team-6/NOTEARS/compute_dist_matrix_ between_hyperparam_STATIC_AND_TEMPORAL.py. We omit tests for novel datasets created after LLM training cut off dates. 18critically important, each affecting the accuracy of the result in the most redactions. The SLOT phrases are important but redundant (SLOT1 and SLOT4 are identical and SLOT2 and SLOT3 are identical). Therefore the LLM seems more robust to their individual redactions in this test, each individual redaction affecting the accuracy of the result, but less often. While most unimportant words do not affect the accuracy of the result much, we do see the redaction of number of â€™minorâ€™ words having a surprisingly strong effect, possibly indicating a general sensitivity of the LLM to grammatical correctness. 4 LLMs for Token Causality and Causal Judgments In the previous section, we saw how LLMâ€™s ability to capture common sense and domain knowledge allows for a knowledge-based approach to building a causal graph. In this section, we study how LLMsâ€™ can bring these capabilities to enable a systematized approach to token causality challenges. Token causality is motivated by problems of attribution and assigning responsibility in real world applications, such as legal reasoning, machine failure debugging, and root- cause analysis for system regressions. For example in tort law, the core problem of deciding how much blame for a plaintiffâ€™s injury is attributed to a defendantâ€™s action relative to other potentially mitigating events is fundamentally an token causal inference task. Causal inference researchers have attempted to use formal causal models to define token causality in a way that is consistent with how humans naturally attribute cause and related concepts of responsibility, blame, and explanations to events and their outcomes (Halpern, 2016; Kueffner, 2021). This task has proven exceedingly difficult, because human judgments of causality depend on elements of background context that are difficult to formalize in an SCM. Types of background context that are fundamental to human causal judgments but difficult to formalize in an SCM include: â€¢ Causal frame: The set of candidate causal events that are relevant to a particular outcome event. SCMs require relevant causes be included as endogenous variables, but in practice humans rely on domain knowledge and common sense to set the causal frame after the outcome occurs (Icard & Goodman, 2015). For example, there are a multitude of potential causes of forest fires, but in the case of a particular forest fire, upon learning lightning struck and that a hiker was carelessly smoking a cigarette during fire season, we know these are relevant causal events. We would also know whether to ignore â€œdrought\" or â€œlack of controlled burn\" if they are not relevant in the case of this particular fire. â€¢ Necessary causality: Whether a candidate cause needed to happen for the outcome to occur. â€¢ Sufficient causality: Whether a candidate causeâ€™s occurance would have led to the outcome event if other causal events had occurred differently. â€¢ Normality: The degree to which causal events align with statistical norms or prescriptive norms (social, moral, or legal norms) (Phillips et al., 2015; Kominsky et al., 2015). When agents violate norms, they are typically judged to be more of a cause of resulting outcomes (Phillips et al., 2015; Kominsky et al., 2015). Human causal judgments depend highly on whether candidate cause is a norm violation, or whether it is more or less normal than other causal events. â€¢ Other human factors: Other human factors include bias towards action, handling intention and epistemic state, and how bad outcomes are interpreted. When the candidate cause is an agentâ€™s behavior, humans tend to ascribe more causality actions (e.g., tossing a cigarette) than to lack of actions (e.g, not doing a controlled burn) (Henne et al., 2017). In addition, when the candidate cause is an agentâ€™s behavior, whether the agent acted with intention and knew what they were doing (Nadelhoffer, 2006; Knobe, 2003) matters. Finally, human causal judgments also depend on whether the outcome is undesirable (e.g., causing a forest fire vs. causing a reforestation initiative) (Kominsky et al., 2015). Sidestepping the challenge of formalizing these concepts into a causal model, LLMs offer an opportunity to capture the necessary and relevant background context for an event directly from natural language description of an event. 19Premise Counterfactual Question Multiple-choices answers A woman does not order Chinese food. What would have happened if she had ordered Chi- nese food? The woman would have become less hungry.;The woman would have become very hungry.;That is not possible. A woman sees a fire. What would have happened if the woman had touched the fire? She would have been burned.;She would not have been burned.;That is not possible.;She would have seen fire. A bird lands in a forest. What would have happened if a plane had landed in the forest? The plane would have crashed.;Everything would have been fine.;The plane would have landed safe and sound.;In a forest you will find lots of planes. A plant grows in a planter. What would have happened if the planter grows in the plant? That is not possible.;It would have grown quicker.;The plant would have suffered.;The planter would have cultivated the plant. A mortician prepares a corpse. What would have happened if the mortician had prepared a dinner? He would have had a delicious dish.;Morticians can- not prepare dinners.;The dinner would have been buried.;The mortician would have killed the corpse. An oil tanker sails across an ocean. What would have happened if the oil tanker had broken up in an ocean? There would have been environmental pollution. That is not possible.;The oil tanker would have continued to carry oil.;The oil tanker would have been saved.; A car crashes into a tree. What would have happened if the car had parked beneath the tree? Nothing special would have happe ned.;The car would have been hit by the tree.;That is not possi- ble.;I think it would have crashed into the tree. A child draws a picture. What would have happened if the child had erased the picture? The picture would not have been visible.;The pic- ture would have been visible.;That is not possible. A craftsman builds a house. What would have happened if the house had built a craftsman? That is not possible.;The house would have been built faster.;Everything would have been fine.;The craftsman would have hands. A doctor washes their hands at work. What would have happened if the doctor hadnâ€™t washed their hands? The patients could get an infection.;The patients could get better.;That is not possible. Table 8: Example scenarios from the CRASS counterfactual reasoning benchmark. The task is to select the best answer choice for the counterfactual question, given a premise. Given that an LLM is trained on text narratives written by humans, the subjective elements of causal judgments may be encoded as part of the LLMâ€™s internal representations. In this section, we first review the concept of counterfactual reasoning and show that LLMs can answer counterfactual questions about an event with accuracy close to that of humans on a benchmark task. We then present an example scenario to illustrate the challenges of modeling real-world events using token causality and introduce the two key counterfactual concepts used in formal models of token causality: necessity and sufficiency. Using 15 different vignettes commonly used in token causality literature, we provide evidence that LLM can capture the necessary background context and reason about necessity and sufficiency of token causes. Finally, we show how this capability transfers to a causal judgment benchmark task from Big Bench (Suzgun et al., 2022), specifically designed to test LLMs on their ability to infer the normality of a candidate causes. 4.1 Building block of token causality: Counterfactual reasoning Counterfactual reasoning is the process of considering hypothetical situations or alternate realities by altering specific elements or conditions of an actual event or situation (Kahneman & Miller, 1986; Byrne, 2005). Such reasoning is a key element of token causality. To determine the causes of an event, it is important to simulate alternative worlds where an event may not have happened and reason about the consequences. For example, a naive approach to token causality is to use a definition based on counterfactual dependence: an event A is a cause of another event B if B would not have happened without A. Independent of token causality, counterfactual argument making is a desirable skill for a LLM, as it would assist users in decision-making, planning, and offer insights that may not explicitly apparent from the original context. With this motivation, Frohberg & Binder (2022) provide a benchmark called CRASS (Counterfactual Reasoning Assessment) for evaluating the ability of LLMs to answer counterfactual conditional questions, a subset of which is included in the 20Model Accuracy GPT-3 (Frohberg & Binder, 2022) 58.39 T0pp (Sanh et al., 2021) 72.63 text-davinci-003 83.94 gpt-3.5-turbo 87.95 gpt-4 92.44 Human annotators 98.18 Table 9: Accuracy of different LLMs on the CRASS counterfactual reasoning benchmark. gpt-4 achieves 92% accuracy, significantly higher than the previous reported accuracy on this benchmark and within six percentage points of human annotatorsâ€™ accuracy. BIGBench collection of datasets (Srivastava et al., 2022).6 The benchmark contains 275 instances, where each instance has the LLM select from multiple choice answers to a counterfactual conditional question such as â€œA woman opens a treasure chest. What would have happened if the woman had not opened the treasure chest?â€. Table 8 lists some instances from the dataset. Our memorization test indicates the CRASS dataset has not been memorized (see Section E). The authors report GPT test-davinci-003 as the best performing (zero-shot) LLM on this task, with a top-1 accuracy of 58.39%. Another language model (T0pp) (Sanh et al., 2021), which was finetuned on a multi-task setup of classification tasks, obtains 72% accuracy. This is contrasted with 98.18% accuracy human baseline, indicating that counterfactual predictions remain a challenging task for LLMs. We revisit the claim using LLMs released after the benchmarkâ€™s publication. As shown in Table 8, each problem instance has a premise, counterfactual question and a list of answer options. We construct a prompt for the LLM by concatenating the premise and the counterfactual question, and then presenting the answer options as A, B, C, D (for an example prompt, see Suppl. B.1). For gpt-3.5-turbo and gpt-4, we additionally provided a system message, â€œYou are a helpful assistant for counterfactual reasoning.â€ Table 9 shows the accuracy of different LMs. Remarkably, GPT 3.5 version models show substantial improvement in accuracy over GPT 3. gpt-3.5-turbo obtains an accuracy of 87.95. gpt-4 improves it further to 92.44%, which is 20 percentage points higher than the previous best accuracy. Comparing to the human accuracy on this task, gpt-4 is within six percentage points of average human accuracy. The results indicate that large language models of GPT-3.5 and 4 series represent a substantial jump in LLM ability to generate counterfactual arguments. Understanding LLM output. We now try to analyze some outputs from gpt-4 to understand its counterfactual argument-making capabilities. For full LLM outputs of the prompt discussed here, see Suppl. B.1.As the high accuracy indicates, gpt-4 is able to simulate different scenarios based on the text prompt and answer what-if scenarios. In many cases, when gpt-4â€™s answer does not match the benchmark, it reveals ambiguity in the text used to describe the scenario. In one of the cases, the LLM is asked what will happen if a man catches a water balloon, given that he has caught a ball. The benchmark answer is that he will get wet whereas the LLM output correctly argues that he may get wet or not, depending on whether the balloon bursts as he catches it. Similarly, another question asks what will happen if a man gets very nervous. The benchmark answer indicates that he will pass out, but the LLM output correctly mentions that, â€œPassing out due to extreme nervousness is possible, but not guaranteed. and concludes that the man would not pass out. We suspect that given enough clarifying context, the LLM should be able to correctly answer such questions. That said, there are a few instances where the LLM does not capture context that people may easily determine through common sense. For instance, one of the questions asks what would happen if a man walks on a bed instead of walking on the street. â€œWalking on a bedâ€ conveys that the man is likely indoors, but the LLM incorrectly imagines a scenario where the man is still walking to the same destination, but now he is walking over a bed, which makes it incorrectly 6The full dataset was released on Github on August 2021, so it is after the training cutoff for text-davinci-003 model. But it is a month before the cutoff for gpt-3.5-turbo. A smaller subset of inputs was released as a part of BIG-Bench in May 2021 with a canary string for LLMs to avoid including it in their training data. 21conclude that the man â€œwill get lateâ€ (see Suppl. B.1). These results indicate the importance of minimizing ambiguity when interacting with an LLMâ€“wherever possible, adding small additional details (e.g., â€œwalking on his bed at homeâ€) can increase the chances of a correct answer. Results on a novel counterfactual dataset. In addition to the CRASS benchmark, we consider a counterfactual dataset that was created after the LLMs training cutoff date of September 2021. Li et al. (2023) develop a new synthetic dataset that was first uploaded on Github in 2022. We consider only the counterfactual variant of their dataset. It includes counterfactual statements of the kind: premise, hypothesis, conclusion. For each premise, there are two statements (one correct, one incorrect). We reformat the dataset to match CRASS format. That is, we create a multiple-choice question prompt as: premise; statement1, statement2. For example, here is an instance from the dataset: â€œIf Charles Darwin had worked on curing people, A. Darwin would have written a paper on Aspirin; B. Darwin would have written a paper on species.â€ Li et al. measure whether a model ranks the correct answer (A) higher than the incorrect answer (B). We follow the exact same prompt text used for the CRASS dataset and evaluate GPT-4 (see Table 17 in Appendix A.3). GPT-4 obtains an accuracy of 88.6%, which is significantly higher than the best-reported accuracy from Li et al. (71.3% for GPT-3). This indicates that models like GPT-4 can generalize counterfactual arguments to new scenarios. 4.2 Inferring token causality: Necessary and sufficient causes Given the promising performance of LLMs on a counterfactual inference task, we now investigate whether LLMs can infer whether events are necessary or sufficient for a focal event. 4.2.1 Motivating Example: Necessary and sufficient causes To motivate, we present the following example story of a bottle of beer spilling at a party, a variation on the story presented in Henne & Oâ€™Neill (2022). Since the story was published in 2022, it cannot have been in the training set of either GPT 3 or GPT 4 models. PROMPT : Mike, Jack, Susan, and Peter are at a party. There is an open bottle of beer on the table. Mike is moving through the room and accidentally bumps against the table. Jack saw that the bottle was about to fall, and he reached out to catch the bottle. But just as Jack was about to catch the bottle, Peter bumped into Jack, which caused Jack to miss the bottle, which then fell to the ground. As a result, the bottle of beer fell to the floor, spilling the beer. Right after the bottle hit the floor, Susan tripped and fell onto the table. Let us unwrap the story progressively to bring out the two key concepts of token causality: necessity and sufficiency. Necessary causality: The property of an event, C, that if it had not occurred, then the outcome event, E, would not have occurred. So we start by evaluating whether the LLM can make simple statements about necessary causality in the context of the beer bottle spilling story. First start by ask the LLM to provide a one sentence counterfactual explanation as to why Mike caused the spill. PROMPT : Mike, Jack, Ben, Susan, and Peter are at a party. There was an open bottle of beer on the table. Mike is moving through the room and accidentally bumps against the table. As a result, the bottle of beer fell to the floor, spilling the beer. Did Mike cause the beer to spill? Provide a one sentence counterfactual explanation as to why or why not. Limiting the response to one sentence discourages more verbose responses. LLM : Yes, Mike caused the beer to spill; if Mike hadnâ€™t bumped into the table, the beer bottle would not have fallen and spilled. 22The LLM zeroes in on the necessary causality as the explanation. More generally, we can prompt the LLM to invoke the necessary cause principle and output the token cause for an event. In the above example, Mikeâ€™s bumping into the table was necessary for the beer to spill. Moreover, it was also sufficient to cause the beer to spill. But there are many cases where a causal event is a necessary but insufficient cause of an outcome. Sufficient causality: The property of an event, C, that if it had occurred, then the outcome event, E, would have occurred. Formal models provide unambiguous definitions of sufficient causality; in Pearlâ€™s counterfactual notation, the probability a causal event C=1 is a sufficient cause is P(EC=1 = 1|C = 0, E= 0) (Pearl, 2009b). In natural language this reads as â€œin cases where C was 0 and E was 0, what is the probability that E would have been 1 if C had been 1?â€ But this phrasing awkward to express in situations where C and E were both indeed 1, not 0. So we turn to cognitive science literature to find a more natural text expression of sufficient cause. We use a variant ofrobust sufficient causality (Hitchcock, 2012; Woodward, 2006; Hitchcock, 2012): Robust sufficient causality: Given that causal event C occurred, outcome event E would have occurred even if other events that contributed to Bâ€™s occurrence had not occurred. To elicit a statement about robust sufficiency, we prompt the LLM to state what other causal events lead to the spill, and then predict a counterfactual about whether the spill would have occurred those events had not happened. This is a more challenging task than the necessary cause prediction, because it requires the LLM to solve a causal frame problem decide which other causal events to, some of which may their interactions. PROMPT : Based on the information in the passage, what other events directly contributed to the fall of the beer and what would have happened if those events hadnâ€™t happened? LLM : Other than Mike bumping into the table, the presence of the open beer bottle on the table directly contributed to the fall of the beer; if the bottle had not been placed on the table or had been properly closed, it would not have fallen and spilled even if Mike had bumped into the table. This response highlights the capability of the LLM to set the causal frame, i.e., select causal events relevant to the inference problem. We did not specify which causal events were eligible for consideration. For example, the LLM could have included the presence of the table as a causal event, but it did not. This is in contrast to formal models such as SCMs where all the variables must be articulated in advance of running an inference query on the model. We conjecture that, as a probability model, the LLM is selecting probable causal abstractions based on both the prompt and statistical regularities in the training data. This allows its causal argument generation to adapt based on the scenario; if Mike bumping the table is the only causal event in consideration, it is sufficient, but that may change if other causal events are present. This highlights the flexibility of LLMâ€™s in causal analysis relative to SCMs and other formal models. 4.2.2 Evaluating LLMs on inferring necessary and sufficient causes When the principle of necessity and sufficiency yields the same cause, token cause of an event can be easily determined. However, in many real-world scenarios, they may output different answers. For example, in the beer spilling example above, suppose there were two beer bottles on the table. Then the presence of any beer bottle on the table is a sufficient cause for beer spilling, but not necessary. Even if one of the bottles was not placed on the table, the other one would have spilled. Therefore, inferring necessary and sufficient causes typically requires applying the definitions of necessity and sufficiency and using formal reasoning (Halpern, 2016). Below we investigate whether LLMs can generate causal arguments directly using the natural language description of an event. We evaluate on a collection of 15 vignettes (example scenarios) from Kueffner (2021). Table 10 shows sample vignettes. These vignettes are widely used to discuss and critique token causality definitions in the literature and span challenging scenarios across seven different types: symmetric overdetermination, switch, late preemption, early preemption, double preemption, bogus preemption, short circuit, and other miscellaneous examples. For example, 23Vignette Type Input Context Event Actor Nec. Suff. Overdetermination Alice (AF) and Bob (BF) each fire a bullet at a window, simultane- ously striking the window, shattering it (WS). window shatter- ing Alice No Yes Switch Alice pushes Bob. Therefore, Bob is hit by a truck. Bob dies. Otherwise, Bob would have been hit by a bus, which would have killed him as well. Bobâ€™s death Alice No Yes Late preemption Alice (AF) and Bob (BF) each fire a bullet at a window. Aliceâ€™s bullet hits the window first (AH). The window shatters (WS). Bobâ€™s bullet arrives second and does not hit the window (BH). window shatter- ing Alice No Yes Early preemption Suppose Alice reaches out and catches a passing cricket ball. The next thing on the ballâ€™s trajectory was a solid brick wall that would have stopped the ball. Beyond that there was a window. window being in- tact Alice No Yes Double preemption Alice intends to fire a bullet at a window (AI). Bob intends to prevent Alice from hitting the window (BI). Bob tries to stop Alice (BSA). Bob is stopped by Carol (CSB). Alice fires a bullet (AF), hits the window (AH) and shatters it (WS). The window shatters (WS). window shatter- ing Alice Yes No Bogus preemption Alice intents to put lethal poison into Carolâ€™s water. However, Alice does not put lethal poison into Carolâ€™s water ( Â¬AP). Bob puts an antidote into Carolâ€™s water (BA). The water is lethal (L), if the poison is added without the addition of an antidote. If Carol would consumes the lethal water she would die (CD). Carol consumes her water (CC). Carol does not die (Â¬CD). Carolâ€™s survival Alice No Yes Short circuit Carol is alive (CA). Alice puts a harmless antidote in Carolâ€™s water (AA). Adding ant idote to the water, protects it against poison (WS - â€™water saveâ€™). If Alice puts the antidote into Carolâ€™s water, B ob will poison the water (BP) Adding poison to an unprotected water makes it toxic (WT). If Carol would drink toxic water she would die (i.e. inhibiting CS). Carol consumes her water and survives (CS). Carolâ€™s survival Alice No Yes Miscellaneous If there is hot weather, flowers will die. Watering prevents the flowers to die in hot weather. The neighbor does not water the flowers in her yard. The flowers die. flowersâ€™ death neighborâ€™s inac- tion Yes Yes Table 10: Example vignettes for evaluation of inferring necessary and sufficient causes, categorized by their type based on the different ways in which potential causes can interact to yield the final outcome. Each vignette tests two questions: â€œIs {Actor} a necessary cause of {Event}?â€ and â€œIs {Actor} a sufficient cause of {Event}?â€. symmetric overdetermination is a scenario where multiple processes (causes), all of which producing the same outcome (event), terminate at the same time. Suppl. C.1 provides a description for each type. For each vignette, our goal is check whether an LLM can correctly identify whether a cause is necessary, sufficient, or both. To do so, we use the following prompt template, where PRINCIPLE is replaced by either â€œminimal changeâ€ (corresponding to necessary cause) or â€œmultiple sufficient causesâ€ (corresponding to sufficient causes). In each example, we provide two options: one of them is the necessary cause and the other the sufficient cause. Interestingly, the prompt itself was constructed using the gpt-3.5-turbo (see Suppl. C.2 for details). An example prompt for the early preemption vignette is shown below. SYSTEM : You are an expert in counterfactual reasoning. Given an event, use the principle of [PRINCIPLE] to answer the following question. PROMPT : Suppose Alice reaches out and catches a passing cricket ball. The next thing on the ballâ€™s trajectory was a solid brick wall that would have stopped the ball. Beyond that there was a window. Is Alice a [NECESSARY/SUFFICIENT] cause of window being intact? After your reasoning, provide the final answer within the tags <Answer>Yes/No</Answer>. Using this prompt, we evaluate all 15 example scenarios on two LLMs: gpt-3.5-turbo and GPT-4. For example, on this vignette, both LLMs correctly answer that Alice is not a necessary cause; even without her action, the brick wall would have stopped the ball.For evaluating sufficiency, gpt-4 correctly answers that Alice is a sufficient cause of the window being intact, but gpt-3.5-turbo answers incorrectly (see responses in Suppl. C.4.1). Tables 11 and 12 summarize the results over all vignettes. We see a significant difference between the accuracy of gpt3.5-turbo and gpt-4, indicating a marked change in ability between the two models. gpt-3.5-turbo fails to capture the nuances of necessity and sufficiency definitions and obtains an accuracy near random guess. However, gpt-4 is accurate 24Vignette Type Necessary Sufficient gpt-3.5-turbo Overdetermination âœ“, âœ“ X, âœ“ Switch X,X âœ“,X Late preemption X X Early preemption X, âœ“, X X, X, âœ“ Double preemption âœ“ âœ“ Bogus preemption âœ“ X, Short circuit X X Miscellaneous X, âœ“, âœ“, X âœ“, âœ“, X, âœ“ Total Accuracy 46.6% 46.6% gpt-4 Overdetermination âœ“, âœ“ âœ“ , âœ“ Switch âœ“, âœ“ âœ“ , âœ“ Late preemption âœ“ âœ“ Early preemption âœ“, âœ“,âœ“ âœ“ , Double preemption âœ“ X Bogus preemption âœ“ âœ“ Short circuit X X Miscellaneous âœ“,X, âœ“,âœ“ âœ“ ,âœ“, âœ“,âœ“ Total Accuracy 86.6% 86.6% Table 11: Accuracy of gpt-3.5-turbo and gpt-4 on inferring necessary or sufficient cause on 15 standard vignettes. The vignettes are divided into eight types (e.g., Early Preemption type has three vignettes). Each (âœ“/X) corresponds to a correct/incorrect answer on a single vignette. gpt-3.5-turbo fails at the task (worse than random chance) but gpt-4 can infer necessary and sufficient cause with high accuracy. for most vignette types. This is remarkable because the LLM was not explicitly provided definitions of necessary and sufficient causes. As with the case of causal graph generation, however, the high accuracy also comes with unpredictable failure modes. For example, even as gpt-4 use the correct sufficiency principle for most vignettes, on the short circuit vignette (see Table 10), gpt-4 uses only counterfactual dependence principle to answer the question and fails to reason about the sufficieny of cause. The input prompt and incorrect response are shown in Suppl. C.4.2. Since the above vignettes or their variants have been quoted in many publications, there is a concern that LLMs may have memorized them and may not be producing new inferences about the answers. We therefore report results on the â€œlab-vignettesâ€ dataset, a new dataset we created that adapts the base vignettes to a laboratory setting. We do so by keeping the same type of scenario (e.g., short circuit), but changing the situation to that in a laboratory with reagents, test tubes, mixtures and crystals. Suppl. C.3 provides details on this dataset. On this new, unseen dataset, we find a similar pattern when comparing models. gpt-4 obtains significantly higher accuracy than gpt-3.5-turbo. While gpt-4 retains its overall accuracy, we find an interesting pattern: for both models, necessity is an easier concept to answer than sufficiency. gpt-4 obtains over 92% accuracy on deciding necessity of cause compared to 78% on deciding its sufficiency. This may be because necessity always involves comparing the output under a counterfactual world where only the cause is flipped; whereas sufficiency can be nuanced, involving counterfactual flips to all relevant other variables (and deciding what those variables should be). Overall, our results indicate the capability of LLMs to understand a scenario in natural language and output token causes according to a pre-specified definition, but also show the lack of robustness due to unpredictable failures. 25Vignette Type Necessary Sufficient gpt-3.5-turbo Overdetermination âœ“, âœ“ X, âœ“ Switch X, âœ“ âœ“ ,X Late preemption X âœ“ Early preemption âœ“, X X, X Double preemption âœ“ âœ“ Bogus preemption âœ“ X Short circuit X X Miscellaneous âœ“, âœ“, âœ“, X âœ“, X, X, âœ“ Total Accuracy 64.2% 42.8% gpt-4 Overdetermination âœ“, âœ“ âœ“ , âœ“ Switch âœ“, âœ“ X, âœ“ Late preemption âœ“ âœ“ Early preemption âœ“, âœ“ X Double preemption âœ“ âœ“ Bogus preemption âœ“ âœ“ Short circuit âœ“ âœ“ Miscellaneous âœ“,X, âœ“,âœ“ âœ“ ,âœ“, X,âœ“ Total Accuracy 92.8% 78.5% Table 12: Testing dataset memorization issues with a novel â€œlab-vignettesâ€ dataset. The average accuracy of gpt-4 stays the same as in the std vignettes, indicating that gpt-4â€™s capabilities to infer necessary and sufficient cause can generalize to new data. Inferring necessary cause (93%) emerges as an easier task than inferring sufficient cause (78%). 4.2.3 Assessing responsibility Using LLMs to produce causal arguments allows for a more nuanced understanding of causality, taking into account factors such as intention, moral obligation, and epistemic state that are difficult to capture using formal SCM approaches. We present a study of these capabilities in Supplementary D. 4.3 Evaluating LLMs ability to infer normality Normality is a key element of evaluating token-causality. When evaluating the role an event had in causing some outcome, it is difficult to use a formal model to quantify the normality of that event. LLMs enable us to rely on common-sense knowledge to quantify how normal/abnormal an event was relative to an outcome. In this section, we describe an experiment designed to quantify the role normality plays in predicting the outcome to token causality problems. For this analysis we used vignettes from the causal judgment benchmark in BIG-Bench Hard (Suzgun et al., 2022). The causal judgment benchmark collects examples from vignettes similar to those in Kueffner (2021), but are in more general language and provide more background information. Each of the vignettes ends with a yes/no question about whether a particular event caused the outcome. The benchmark provides ground truth labels for each question, collected from human annotators. The vignettes vary factors that drive human causal judgments, such as whether the causal event was necessary, sufficient, normal, whether it was causation by omission, whether it lead to a harmful outcome, etc. It is an ideal benchmark for evaluating how well LLMs would do in addressing practical token causality problems, as opposed to more contrived examples. 26The benchmark has proven resilient to saturation as new generations of LLMs are released. For example, while several benchmarks in BIG-Bench Hard saw a dramatic increases in accuracy between GPT3 and GPT4, causal judgment had a small increase (61.5% in text-davinci-003 and has an accuracy of 67.6% in GPT-4-32K (AML-BABEL, accessed 2023-04-25). Top human performance is at 100% accuracy. Constructing a benchmark for normality. We filtered out the vignettes that ask whether an agent intentionally caused an outcome and only focus on examples that ask if an event or agent caused an outcome. We do this because assessing intention is a separate task from ascribing causality, and because many intention examples were contrived (e.g., trolley problems) such that normality does not apply. We then manually reworded each vignette in order to mitigate against the possibility of the vignettes having appeared in the training. Each vignette was specifically designed to include sets of the factors that drive human causal judgments (necessity, normality, etc.). So we reword the vignette by applying a minimalist lexical substitution of words in the vignette so that we avoid changing those factors. To illustrate, the following is a vignette from the benchmark, which originally appeared in Knobe & Fraser (2008): The receptionist in the philosophy department keeps her desk stocked with pens. The administrative assistants are allowed to take the pens, but faculty members are supposed to buy their own. The administrative assistants typically do take the pens. Unfortunately, so do the faculty members. The receptionist has repeatedly emailed them reminders that only administrative assistants are allowed to take the pens. On Monday morning, one of the administrative assistants encounters Professor Smith walking past the receptionistâ€™s desk. Both take pens. Later that day, the receptionist needs to take an important message... but she has a problem. There are no pens left on her desk. Did Professor Smith cause the problem? We reworded this vignette as follows, with the main changes in bold: The executive assistant in the marketing department keeps notepads in his cabinet. The office assistants are allowed to take the notepads, but managers are supposed to buy their own. The office assistants typically do take the notepads. Unfortunately, so do the managers. The executive assistant has repeatedly emailed them reminders that only office assistants are allowed to take the notepads. On Tuesday morning, one of the office assistants encounters Manager Johnson walking past the executive assistantâ€™s cabinet. Both take notepads. Later that day, the executive assistant needs to take an important note... but he has a problem. There are no notepads left in his cabinet. Did Manager Johnson cause the problem? We then manually label each example as â€œnormal\" or â€œabnormal\". In most cases these labels were not subjective assessments on our part. Each element in the causal judgment data set includes a citation to the source of the passage. In many of these studies, normality was explicitly varied across sections of a vignette, and thus we could confirm our labels from their source papers. However, in some cases, we had to make subjective best-guess assessments. Prompting the LLM. We follow a two-step process. We first prompt the LLM to extract the causal event in question. PROMPT : The last sentence of the following passages is a question. The question either ask if an outcome event happened because a certain causal event occurred. When the question asks specifically about â€œintention,â€ the causal event is not the outcome, it is the action that lead to the outcome. State the causal event being asked about. Do not include the outcome event in the statement. Do not include other causal events in the passage. Passage: The executive assistant in the marketing department keeps notepads in his cabinet. The office assistants are allowed to take the notepads, but managers are supposed to buy their own. The office assistants typically do take the notepads. Unfortunately, so do the managers. The executive assistant has repeatedly emailed them reminders that only office assistants are allowed to take the notepads. On Tuesday morning, one of the office assistants encounters Manager Johnson walking past the executive assistantâ€™s cabinet. Both take notepads. Later that day, the executive assistant needs to take an important note... but he has a problem. There are no notepads left in his cabinet. Did Manager Johnson cause the problem? 27LLM : Manager Johnson took notepad from the executive assistantâ€™s desk. We then ask the LLM to evaluate the normality of the causal event in question. Inspired by both model-based and subjective notions of normality, we construct the following prompt. PROMPT : Given a causal event that causes an outcome event, the causal event is \"abnormal\" if any of the following is true: â€¢ The causal eventâ€™s occurrence was unexpected, unlikely, surprising, rare, or improbable â€¢ The causal event was an agentâ€™s action or lack of action that non-accidentally, knowingly, or negligently violated social, legal, or ethical norms. The opposite of â€œabnormal\" is â€œnormal\". A causal event is â€˜normal\" if its occurrence was expected, likely, probable, and not surprising. In cases where the causal event is an agentâ€™s action or lack of action, the action or lack of action can also be considered \"normal\" if it intentionally compliant with social, legal and ethical norms. The normality of a causal event is independent of whether the outcome event was bad or and whether the outcome event was normal. Please read the following passage and statement about a causal event from the passage. Provide your step-by-step rationale as to why the causal event is abnormal or normal. Then, return the word â€œnormalâ€ if you think the causal event is normal or â€œabnormalâ€ if you think the causal event is abnormal. Passage: [PASSAGE] Event: Manager Johnson took a notepad from the executive assistantâ€™s cabinet. LLM : Manager Johnsonâ€™s action of taking a notepad from the executive assistantâ€™s cabinet is abnormal because it violates the established norm that managers are not allowed to take notepads from the executive assistantâ€™s cabinet. Despite the executive assistant sending reminders about this rule, Manager Johnson still took a notepad, making his action unexpected and unethical. In defining normality, we attempted to capture both statistical norms (with keywords â€œexpected,â€ â€œlikely,â€ â€œprobable,â€ â€œfrequent,â€ â€œunsurprising,â€ and their opposites) and prescriptive norms. Violation of prescriptive norms can depend on intention, so we use keywords â€œnonaccidentallyâ€, â€œknowingly,â€ â€œnegligently,â€ and â€œintentionally.â€ We also ask the model to provide a chain-of-thought â€œstep-by-stepâ€ rationale for its answer. Results. Initial results show accuracy in inferring normality was 69.2% on gpt-3.5-turbo and 71.1% on gpt-4, which are comparable to performance on the core benchmark task. Table 13 shows three examples of correct and incorrect answers across answers by GPT-3.5-turbo and GPT-4. Table 13 demonstrates the capability of LLMs to parse normality but also provides intuition for how the LLMs can get normality wrong. While results did improve between versions of gpt-3.5-turbo and gpt-4, there were some cases were gpt-4 was wrong when gpt-3.5-turbo was right, although its rationale when it was wrong was typically more persuasive. 4.4 Discussion Formal approaches to token causality have struggled to represent background common sense elements of a causal judgment, such as necessity, sufficiency, normality and responsibility. We show through analysis of the CRASS (Frohberg & Binder, 2022) and causal judgment (Ghazal et al., 2017) benchmarks and related vignettes from Kueffner (2021) that LLMs perform well in generating causal arguments with these elements. Relatively lower performance for direct and chain-of-thought answering on the causal judgment benchmark highlight an opportunity to research and engineer systems that guide LLMs in assembling these core elements into solutions for practical problems in token causality questions. 28Passage and Question Correct Answer gpt-3.5-turbo rationale GPT-4 rationale Megan and Sarah work for the same organization. They each need to use a printer for tasks sometimes. Unfortu- nately, the printer isnâ€™t very reliable. If two people print at the same time, it usually jams. So the organization decided to institute an official rule. It declared that Megan would be the only one permitted to use the printer in the morn- ings and that Sarah would be the only one permitted to use the printer in the afternoons. As expected, Megan used the printer the next day at 9:00 am. But Sarah decided to ignore the official rule. She also used the printer at 9:00 am. The printer jammed immediately. Did Sarah cause the printer to jam? Sarah using the printer at 9:00 am is an ab- normal event be- cause she fol- lowed policy. Using the printer at 9:00 am is consistent with the official policy, making this causal event normal. (incorrect) Sarahâ€™s decision to use the printer at 9:00 am despite the official policy is abnormal, as it intentionally violates the companyâ€™s policy put in place to prevent the printer from jamming. This action is unexpected and goes against the established social and workplace rules. (correct) The executive assistant in the marketing department keeps notepads in his cabinet. The office assistants are allowed to take the notepads, but managers are supposed to buy their own. The office assistants typically do take the notepads. Unfortunately, so do the managers. The executive assistant has repeatedly emailed them reminders that only office assistants are allowed to take the notepads. On Tuesday morning, one of the office assistants encounters Manager Johnson walking past the executive assistantâ€™s cabinet. Both take notepads. Later that day, the executive assistant needs to take an important note... but he has a problem. There are no notepads left in his cabinet. Did Manager Johnson cause the problem? The office as- sistant taking the notepad is a normal event because they followed policy. Taking notepads is ex- pected behavior among both office assistants and managers according to the rules set by the exec- utive assistant. Therefore, the causal event of both the office assistant and Manager Johnson taking notepads is normal. (in- correct - for managers, this is a norm violation) Taking a notepad from the ex- ecutive assistantâ€™s cabinet is abnormal for the office assis- tant in this situation because they contributed to the prob- lem of the executive assistant having no notepads left, even though it is typically allowed for them to take notepads. However, it is important to note that the managers action of taking a notepad is even more abnormal and against the established rules. (incor- rect - the normality of the office assistant taking the pen is independent of the outcome) Mark was about to go pick up his daughter from a daycare in the city. In the parking lot, he stopped to chat with friends. Because of this delay in picking up his daughter, Markâ€™s friend, who waited 15 min for Mark to arrive, took the child home in his car. On the way home, the friendâ€™s car was hit by a reckless driver. Markâ€™s daughter, the only seriously injured victim, suffered severe arm injuries. Did Mark cause his daughterâ€™s injury? While stopping to talk to friends is normal, do- ing so to the point of being late and causing others to wait is a social norm vio- lation. The cause of Markâ€™s being late is therefore abnormal. Stopping to talk with friends in a parking lot is a common and ex- pected behavior, making this causal event quite normal. (incorrect) Marking stopping to talk with friends in the parking lot is a normal event as peo- ple often engage in casual conversations when they en- counter acquaintances. It is not unexpected, unlikely, or a violation of social norms. (incorrect) Table 13: Comparative assessments of normality between gpt-3.5-turbo and gpt-4. Stories taken from the BIG-Bench causal judgments task. 5 A New Frontier for Causality Our results on evaluating large language models for causal DAG generation, counterfactual reasoning, and token causality demonstrates that they bring significant new capabilities across a wide range of causal tasks. We believe that these capabilities signal the beginnings of a new frontier and will transform the practice and research of causality. We start by summarizing key takeaways on how to interpret the new capabilities that LLM bring to causality. We then provide implications for practitioners and future research directions at the intersection of LLMs and causality. What is new with causality and LLMs? â€¢ LLMs provide access to domain knowledge that was, heretofore, only available via human domain experts. LLMs can provide this domain knowledge when explicitly asked, and also implicitly, e.g., as we see in background knowledge necessary for token causality vignettes. â€¢ LLMs provide a flexible, natural language-based interaction mechanism for conducting causal analysis that can work alongside existing tools, thus democratizing access to causal tools and knowledge. 29â€¢ LLMs offer a new capability to extract the key primitives of an token causality question (necessity, sufficiency, normality, etc.). This opens up the possibility of building systems based on formal token causality research for practical scenarios for perhaps the first time. It is important to recognize that answering causal questions require iterative steps in practice. LLMs can potentially en- able a wide and effective adoption of causality in practice and break the barriers between different research communities related to causality. What is not changing with causality and LLMs? â€¢ Especially for high-risk and high-value tasks, people have long relied on rigorous analyses in various formal frameworks to ensure correctness of their decision-making. The need for rigorous, well-documented, and verifiable analyses has not changed. This implies that LLM-driven applications must have tight integration with more formal approaches to causal reasoning. LLMs should be able to fluidly connect their language generation with the use of formal tools, including re-framing or translating natural language scenarios to formal descriptions, and ensuring their natural language responses are in accordance with the conclusions of formal analyses. 5.1 Implications for Causality Practitioners Augmenting human expertise with LLMs. LLMs can enable effective human-AI interaction that reduces human burden during causal analyses, while ensuring trustworthiness and human verifiability. One of the key reasons causal methods are not widely deployed is because using them requires expertise in expressing the causal assumptions formally through a graph and then verifying them through robustness checks. LLMs can act as an assistant in both these processes, augmenting the capability of a practitioner and helping reduce friction in starting a causal analysis. Given the challenging task of algorithmic causal discovery, most practitioners start their causal analysis by constructing a graph manually based on their domain knowledge. Rather than creating a graph from scratch, we recommend that practitioners input their dataset metadata to an LLM and obtain an initial version of the graph that they can iterate on. At least for medium sized graphs, our results indicate that LLMs can match state-of-the-art accuracy in learning real-world graphs, as long as there is descriptive metadata available for each variable. Doing so can save time since editing a graph can be faster than coming up with all edges. Alternatively, given that LLMs obtain their best accuracy in pairwise causal discovery tasks, another way is use LLMs to critique human-generated graphs. LLMs can be used to iteratively critique the presence or absence of edges and help the analyst discover any missing edges. Apart from graph creation, the other challenging task is to validate the output of any causal analysis, since the output depends on the initial causal assumptions (e.g., the conditional independence assumptions implied by the graph). Here LLMs can act as useful assistants and help the analyst through chat conversation towards finding any flaws in the research and/or planning a robustness test. LLMs can also help in suggesting specific robustness checks for a causal analysis, such as the variables to use as negative controls. In Suppl. F.2, we present a case study of a conversation with an LLM to identify negative controls for an observational study of vaccine efficacy. Applying LLMs to novel scenarios. We evaluated LLMs on a combination of known, popular datasets (that are likely to be in LLMsâ€™ training data) and datasets developed after LLMâ€™s training cutoff date (that are not present in the LLMsâ€™ training data). On both kinds of datasets, we obtain similarly high accuracies across different causal tasks, indicating that the presence of the specific dataset in the LLMsâ€™ training data is not a prerequisite for doing well on a causal task. In our experiments with datasets outside the LLMs training data, we expect that the LLMâ€™s capabilities are a result of transforming relevant domain knowledge extracted from other, older texts in the training data. We do not expect an LLM to construct causal assumptions or causal graphs on new concepts beyond what is included in its training corpus. That said, even analyses of novel causal relationships generally occur in the context of other, better understood causal mechanisms. Therefore we expect practitioners to benefit from using LLMs for inferring at least a subset of cause-effect relationships in real-world datasets. For scenarios entirely outside an LLMâ€™s training corpus, the use of retrieval augmented generation presents an opportunity for future research. 30The correctness of causal assumptions is critical for the validity of causal analyses, e.g., of numerical data for scientific discovery or other novel scenarios. Until now, the sole advice for ensuring the correctness of causal assumptions was to advise practitioners to take care and consult with domain experts. The construction of causal assumptions or causal graphs and the criticism of practitionerâ€™s assumptions based on LLMâ€™s output presents a significant advance in technological support for this critical task. LLM + Causal tools: A new pipeline for causal analysis While LLMs can help with framing and validating causal assumptionsâ€”the steps in a causal analysis where human expertise is centralâ€”they are not capable of replacing the tools that do statistical analysis for estimating causal quantities. These tools (e.g., DoWhy, EconML, Ananke) implement principled algorithms from graphical causal models and the potential outcomes literature and are a workhorse for a causal analyst. We envision that LLMs can enable an end-to-end pipeline for causal inference where the code control seamlessly transfers between an LLM and causal tools. Practitioners can construct pipelines where graph creation is done in LLM-assisted interfaces while the final output graph is passed in to a statistical inference tool. Given a graph, an LLM may also be used to generate the code for conducting downstream analysis with a specified causal tool. Based on recent research (Schick et al., 2023), it may also be possible to augment LLMs and enable them to call out relevant causal tools and return the answer in a conversational chat interface. Suppl. F.1 presents a case study conversation with GPT-4 asking it to create sample code for synthetic data generation and causal analysis using the PyWhy open-source libraries. LLMs as a fluid conversational interface, merging covariance- and logic-based reasoning. In practice when tackling complex causal challenges, people strategically alternate between different modes and methods for causal reasoning. People may begin with a logical reasoning approach to frame a high-level problem, then toggle to a covariance based approach to solve a subtask. They may switch to a mental simulation to consider counterfactuals, or validate consequences against logical assumptions. And they may continue interchanging complementary analytical methods repeatedly before honing in on their final answer. This kind of fluid iteration between modes of thinking to date required humans to translate their premises and knowledge across frameworks. Now, LLMs may be able to do more of that. If so, this may allow a merging of the many kinds of causality into a single higher-level abstraction, allowing people to ask and have answered a much broader and higher-level set of causal questions than could have been addressed through computational methods previously. A first step towards this goal can be to include the information about variables into the problem of generating causal DAGs. 5.2 Ethical Implications and Societal Impacts Causal modeling is integral to AI interpretability and fairness. It provides explanations behind an AIâ€™s predictions in individual cases, enhancing interpretability and fairness in AI decision-making processes. However, without stringent understanding and control over how LLMs formulate causal arguments, thereâ€™s a risk of these models being manipulated to produce seemingly valid yet misleading explanations for its explanations. Such misrepresentations could falsely attest to the fairness of a decision or rationalize its basis, potentially leading to misplaced trust and unjust outcomes under the guise of algorithmic neutrality and causal rigor. The field of causal inference has focused on theoretical and empirical analysis of real-world causal mechanisms, including in high-stakes areas like policy-making and healthcare. Despite its numerical basis, this field is not immune to subjective influences. For instance, a causal analysis of how â€™raceâ€™ affects â€™wealthâ€™ relying on rigorous causal identification and high quality data would still depend on a definition of race â€“ a nebulous concept impossible to randomize in an experiment and which varies across cultures and history. One of the strengths of traditional causal inference analysis is that the subjective elements of the analysis are explicit in the specification of the model, such as the choice of variable definition and the edges in a DAG. When these assumptions and biases are explicit, they can be scrutinized. But with LLMs, these assumptions and biases, shaped by diverse and sometimes highly problematic internet data, are hidden in the black box of learned weights in a transformer architecture. Even these weights are often concealed behind proprietary boundaries. This lack of transparency could lead to the misconception that LLMsâ€™ causal 31claims are unbiased due to their algorithmic nature. This is concerning given claims that LLMs learn world models (i.e., causal models). Indeed, an LLM can be fine-tuned to reflect reflect any viewpoint (e.g., vaccine skepticism). It is imperative that in researching the causal capabilities of LLMs, we avoid creating a veneer of causal infallibility. 5.3 New research questions on causality and LLMs Our work is a proof of concept that LLMs hold the potential to provide domain knowledge and enable natural and flexible interactions for causal tasks. It will require further advances to enable effective human-AI interaction to realize this potential. Knowledge-based causal DAG generation. We acknowledge that our formulation of knowledge-based pairwise edge inference is different from the standard formulation. However, given that LLMs demonstrate non-trivial performance of inferring causal relationships based on variable names alone, we believe that this can have substantial implications on research on algorithmic causal discovery and, more generally, causal model specification. Specifically, these results beg the question of how we can reformulate the problem of specifying a causal DAG that best leverages large language models and existing knowledge in large amounts of texts. Graphical causal models require the modeler to specify a causal graph. They do this using domain knowledge about which variables in the domain cause witch â€“ i.e., the modeler leverages meta-information about the variables to construct the graph. In contrast, the canonical problem formulation of causal discovery aims to address this task with algorithms that attempt to reverse engineer the causal graph using covariance in observational or experimental data. The attractiveness of this approach is that relying on statistical evidence in the data helps avoid subjective model misspecification errors. However, this approach completely ignores meta-information about variables. Our results indicate that this may be a mistake since many datasets provide both data values and meta-information that can be useful for inferring at least a part of the full graph. One promising direction is to consider the output of an LLM as a prior for a causal discovery algorithm. The relevant question is to what extent having an LLM-based prior would increase the accuracy of causal discovery algorithms, where (Choi et al., 2022) present some promising results. Another, more provocative direction is to recognize that current discovery algorithms struggle on real-world datasets and we can aim for redefining the causal discovery problem such that it includes the meta-information about variables and the existing knowledge encoded through LLMs (Zhiheng et al., 2022). Developing new discovery algorithms that make use of both meta-information and covariance in the data can be a fruitful research direction, wherein the LLM may be used as a prior, as a critic during learning, and/or as a post-processor. In addition to building new algorithms, LLMs can also facilitate building benchmarks for causal discovery algorithms. As illustrated from our results, causal discovery benchmarks are usually created by close collaboration with domain experts, with a strong assumption that domain experts know the groundtruth causal graphs. However, this assumption does not hold because 1) Possible edges grow superlinearly with the number of variables, so it is unlikely for a human expert to go over all possible edges reliably; 2) More importantly, domain experts may not be able to precisely provide the causal link between two variables given their knowledge. LLMs can help identify potentially missing edge and correct mislabeled edge, or at least point out relations that require additional checking. LLM-guided effect inference. Identifying the correct adjustment set (e.g., using the backdoor criterion) is one of the most important problems for causal effect inference. However, the validity of the backdoor criterion depends on access to the true graph. LLMsâ€™ capability for generating causal DAG can be useful for effect inference and opens up a research question on how to use LLMs to infer valid backdoor sets. In addition to discovering the full graph, techniques that utilize partial graph structure (Entner et al., 2013; Cheng et al., 2022) to identify backdoor sets may also benefit from the ability of knowledge-based DAG generation with LLMs to infer specific edges. Similarly, it may be interesting to see if LLMs can be used to suggest potential instrumental variables for a causal task, given metadata about observed variables. A second promising direction is in utilizing LLMâ€™s domain knowledge to help build robustness and validation checks for a given causal analysis. Typically, this involves an analyst selecting appropriate robustness checks given a problem and then reasoning over the background context to identify the correct variables to use for the check. Can LLMs be 32used for determining which robustness tests may be applicable for a given problem and which specific variables are most suited to be used in those tests? Systematizing token causality and attribution. The ability to support token causal inference is one of the most potentially disruptive capabilities of LLMs. In domains ranging from law to intelligence analysis, analysts have to first summarize key elements of text data from multiple sources, then synthesize explanations about the degree to which events contributed to other events, i.e., explanations of why and how things happened. While it is well known LLMs excel at the former task, using LLMs for token causality presents a major opportunity to impact the latter. Using LLMs for token causality also has potential impact to root cause analysis and credit assignment in domains ranging from engineering to reinforcement learning. Formal causal models for inferring token causality questions have struggled formalizing the many elements of common sense background knowledge that rely on when judging token causality (Halpern, 2016; Icard et al., 2017; Knobe, 2003; Henne et al., 2017). Weâ€™ve argued that LLMs can work with these background concepts directly in natural language, sidestepping the challenge of shoehorning them into a formal model. The BIG-Bench causal judgment dataset (Suzgun et al., 2022) is ideal for testing LLMs ability to answer practical token causality questions. LLMs have performed less well on this benchmark relative to best human performance as compared to the rapid saturation of other benchmarks. However, answering these types of questions involves reasoning over commonsense background knowledge concepts, such as necessity, sufficiency, and normality. Our analysis shows that LLMs do better at answering questions about these basic elements than answering high-level token causal judgment questions directly. A promising direction of research is developing methods that guide LLMs to use these token causality primitives to answer higher level token causal judgment questions, perhaps using formal token causality theory as a guide. Human-LLM collaboration. A recurring theme throughout the paper is the promise of human-AI collaboration in causal problems. There are interesting research questions on how best to facilitate such human-LLM collaboration for building graphs. The interaction will likely be iterative for the maximum impact: LLMs may suggest graph edges, take feedback from a human, and also can give feedback on a manually generated graph. Given that building a graph is a common impediment to doing causal analyses, an improvement in this task will have important consequences for the widespread adoption of causal data analysis. Understanding and improving how LLMs generate causal arguments. Why LLMs demonstrate such causal capabilities, as well as the limits of these capabilities is not yet well understood. The LLMs certainly do well in interpolating between causal language in its training data to answer interventional and causal language. The question is whether they are limited to such interpolation Willig et al. (2023) or if they can learn a â€œworld model\" (i.e., a catholic causal model) Hao et al. (2023). According to the causal hierarchy theorem (Bareinboim et al., 2022), an LLM can only make interventional and counterfactual inferences outside of its training data with specific interventional and counterfactual inductive biases. Questions of whether an LLM entails such causal inductive biases, what exactly they might be, and for what types of queries can they be relied upon will require further study. At the same time, we see unpredictable failure modes, even in tasks where LLMs obtain high accuracy. In summary, understanding the nature of causal reasoning in LLMs and how to improve their robustness is a key future question. 6 Conclusion Human domain knowledge has always been a central piece in causal analysis. In this paper, we studied the capabilities of large language models and found that LLMs can provide value by mimicking that domain knowledge, as a result of being trained on vast amounts of human-generated text. This mimicking is a result of a complex training process so it is not explainable and neither predictable: an LLM can fail on some queries while successfully building a causal arguments in others. What is remarkable is how few times that such errors happen: our evaluation finds that on average, LLMs can outperform state-of-the-art causal algorithms in graph discovery and counterfactual inference, and can systematize nebulous concepts like necessity and sufficiency of cause by operating solely on natural language input. 33From a research perspective, these results open up more questions than they answer, and we provide a list of research questions at the intersection of LLMs and causality. At the same time, due to the demonstrated capabilities of LLMs, we foresee a marked impact of LLMs on the practice of causal analysis. We outlined how LLMs can help reduce the burden of human expertise in tasks like building the causal graph, effect inference, and attribution. Another contribution of LLMs is in bridging the divide between covariance- and logic-based causal analysis. By providing a flexible natural language interface for answering causal queries, LLMs can serve to unify these two branches of causal analysis and allow analyses that seamlessly straddle both to answer real-world causal questions. Acknowledgments We thank all the people who gave us feedback on earlier versions of this work, as well as the thought-provoking questions and comments during seminars that helped refine our ideas. We also thank the anonymous reviewers whose comments helped improve the paperâ€™s contributions. References Ahmed Abdulaal, Nina Montana-Brown, Tiantian He, Ayodeji Ijishakin, Ivana Drobnjak, Daniel C Castro, Daniel C Alexander, et al. Causal modelling agents: Causal graph discovery through synergising metadata-and data-driven reasoning. In The Twelfth International Conference on Learning Representations, 2023. AML-BABEL. Aml-babel benchmark: Causal judgement. https://aml-babel.com/benchmark/ datasets/bigbench-hard/causal_judgement, accessed 2023-04-25. James W Atchison and Heather K Vincent. Obesity and low back pain: relationships and treatment. Pain Management, 2(1):79â€“86, 2012. Elias Bareinboim, Juan D Correa, Duligur Ibeling, and Thomas Icard. On pearlâ€™s hierarchy and the foundations of causal inference. In Probabilistic and causal inference: the works of judea pearl, pp. 507â€“556. 2022. Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pp. 610â€“623, 2021. Sebastian Bordt, Harsha Nori, Vanessa Rodrigues, Besmira Nushi, and Rich Caruana. Elephants never forget: Memorization and learning of tabular data in large language models, 2024. URL https://arxiv.org/abs/ 2404.06209. SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. Kailash Budhathoki, Dominik Janzing, Patrick Bloebaum, and Hoiyi Ng. Why did the distribution change? In Arindam Banerjee and Kenji Fukumizu (eds.), Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume 130 of Proceedings of Machine Learning Research, pp. 1666â€“1674. PMLR, 13â€“15 Apr 2021. Ruth M. Byrne. The Rational Imagination: How People Create Alternatives to Reality. MIT Press, Cambridge, MA, 2005. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. arXiv preprint arXiv:2202.07646, 2022. Winston Chang. Connecting counterfactual and physical causation. In Proceedings of the Annual Meeting of the Cognitive Science Society, volume 31, 2009. 34Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021. Debo Cheng, Jiuyong Li, Lin Liu, Kui Yu, Thuc Duy Le, and Jixue Liu. Toward unique and unbiased causal effect estimation from data with hidden variables. IEEE Transactions on Neural Networks and Learning Systems, 2022. Hana Chockler and Joseph Y Halpern. Responsibility and blame: A structural-model approach. Journal of Artificial Intelligence Research, 22:93â€“115, 2004. Kristy Choi, Chris Cundy, Sanjari Srivastava, and Stefano Ermon. Lmpriors: Pre-trained language models as task- specific priors. arXiv preprint arXiv:2210.12530, 2022. Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and Prithviraj Sen. A survey of the state of explainable ai for natural language processing, 2020. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171â€“4186, 2019. Doris Entner, Patrik Hoyer, and Peter Spirtes. Data-driven covariate selection for nonparametric estimation of causal effects. In Artificial intelligence and statistics, pp. 256â€“264. PMLR, 2013. JÃ¶rg Frohberg and Frank Binder. CRASS: A novel data set and benchmark to test counterfactual reasoning of large language models. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pp. 2126â€“2140, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology. org/2022.lrec-1.229. Tobias Gerstenberg, Noah Goodman, David Lagnado, and Josh Tenenbaum. From counterfactual simulation to causal judgment. In Proceedings of the annual meeting of the cognitive science society, volume 36, 2014. Ahmad Ghazal, Todor Ivanov, Pekka Kostamaa, Alain Crolotte, Ryan V oong, Mohammed Al-Kateb, Waleed Ghazal, and Roberto V Zicari. Bigbench v2: the new and improved bigbench. In 2017 IEEE 33rd International Conference on Data Engineering (ICDE), pp. 1225â€“1236. IEEE, 2017. Clark Glymour, Kun Zhang, and Peter Spirtes. Review of causal discovery methods based on graphical models. Frontiers in genetics, 10:524, 2019. Joseph Y Halpern. Actual causality. MiT Press, 2016. Joseph Y Halpern and Christopher Hitchcock. Graded causation and defaults. The British Journal for the Philosophy of Science, 2015. Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992, 2023. Daniel Murray Hausman. Causal relata: Tokens, types, or variables? Erkenntnis, 63(1):33â€“54, 2005. Mary Hegarty. Mechanical reasoning by mental simulation. Trends in cognitive sciences, 8(6):280â€“285, 2004. 35Jan Hellner. Causality and causation in law. Scandinavian studies in law, 40:111â€“134, 2000. Paul Henne and Kevin Oâ€™Neill. Double prevention, causal judgments, and counterfactuals. Cognitive science, 46(5): e13127, 2022. Paul Henne, Ãngel Pinillos, and Felipe De Brigard. Cause by omission and norm: Not watering plants. Australasian Journal of Philosophy, 95(2):270â€“283, 2017. Miguel A HernÃ¡n and James M Robins. Causal inference, 2010. Christopher Hitchcock. Portable causal dependence: A tale of consilience. Philosophy of Science, 79(5):942â€“951, 2012. Marius Hobbhahn, Tom Lieberum, and David Seiler. Investigating causal understanding in llms. In NeurIPS ML Safety Workshop, 2022. Patrik Hoyer, Dominik Janzing, Joris M Mooij, Jonas Peters, and Bernhard SchÃ¶lkopf. Nonlinear causal discovery with additive noise models. Advances in neural information processing systems, 21, 2008. Yiyi Huang, MatthÃ¤us Kleindessner, Alexey Munishkin, Debvrat Varshney, Pei Guo, and Jianwu Wang. Benchmarking of data-driven causality discovery approaches in the interactions of arctic sea ice and atmosphere. Frontiers in Big Data, 4, 2021. ISSN 2624-909X. doi: 10.3389/fdata.2021.642182. URL https://www.frontiersin.org/ articles/10.3389/fdata.2021.642182. Thomas Icard and Noah D Goodman. A resource-rational approach to the causal frame problem. In CogSci, 2015. Thomas F Icard, Jonathan F Kominsky, and Joshua Knobe. Normality and actual causal strength. Cognition, 161:80â€“93, 2017. Guido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical sciences. Cambridge University Press, 2015. Marc Jeannerod. Neural simulation of action: a unifying mechanism for motor cognition. Neuroimage, 14(1): S103â€“S109, 2001. Daniel Kahneman and Dale T. Miller. Norm theory: Comparing reality to its alternatives. Psychological Review, 93(2): 136â€“153, 1986. Marcus Kaiser and Maksim Sipos. Unsuitability of notears for causal graph discovery when dealing with dimensional quantities. Neural Processing Letters, 54(3):1587â€“1595, 2022. Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. Gpt-4 passes the bar exam.Available at SSRN 4389233, 2023. Joshua Knobe. Intentional action and side effects in ordinary language. Analysis, 63(3):190â€“194, 2003. Joshua Knobe and Ben Fraser. Causal judgment and moral judgment: Two experiments. Moral psychology, 2:441â€“447, 2008. Joshua Knobe and Scott Shapiro. Proximate cause explained. The University of Chicago Law Review, 88(1):165â€“236, 2021. Jonathan F Kominsky, Jonathan Phillips, Tobias Gerstenberg, David Lagnado, and Joshua Knobe. Causal superseding. Cognition, 137:196â€“209, 2015. Eliza Kosoy, David M Chan, Adrian Liu, Jasmine Collins, Bryanna Kaufmann, Sandy Han Huang, Jessica B Hamrick, John Canny, Nan Rosemary Ke, and Alison Gopnik. Towards understanding how machines can learn causal overhypotheses. arXiv preprint arXiv:2206.08353, 2022. 36Konstantin Raphael Kueffner. A comprehensive survey of the actual causality literature. 2021. Peter Lee, Sebastien Bubeck, and Joseph Petro. Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine. New England Journal of Medicine, 388(13):1233â€“1239, 2023a. Peter Lee, Carey Goldberg, and Isaac Kohane. The AI Revolution in Medicine: GPT-4 and Beyond. Pearson, 1 edition, 2023b. ISBN ISBN-13: 9780138200138. Jiaxuan Li, Lang Yu, and Allyson Ettinger. Counterfactual reasoning: Testing language modelsâ€™ understanding of hypothetical scenarios. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 804â€“815, 2023. Marc Lipsitch, Eric Tchetgen Tchetgen, and Ted Cohen. Negative controls: a tool for detecting confounding and bias in observational studies. Epidemiology (Cambridge, Mass.), 21(3):383, 2010. Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. Evaluating the logical reasoning ability of chatgpt and gpt-4, 2023. Tania Lombrozo. Causalâ€“explanatory pluralism: How intentions, functions, and mechanisms influence causal ascriptions. Cognitive psychology, 61(4):303â€“332, 2010. Stephanie Long, Tibor Schuster, Alexandre PichÃ©, ServiceNow Research, et al. Can large language models build causal graphs? arXiv preprint arXiv:2303.05279, 2023. Gary Marcus. How come gpt can seem so brilliant one minute and so breathtakingly dumb the next?, 2022. URL https://garymarcus.substack.com/p/how-come-gpt-can-seem-so-brilliant . Alexander Marx and Jilles Vreeken. Telling cause from effect using mdl-based local and global regression. In 2017 IEEE international conference on data mining (ICDM), pp. 307â€“316. IEEE, 2017. Steven Mascaro, Yue Wu, Owen Woodberry, Erik P Nyberg, Ross Pearson, Jessica A Ramsay, Ariel O Mace, David A Foley, Thomas L Snelling, Ann E Nicholson, et al. Modeling covid-19 disease processes by remote elicitation of causal bayesian networks from medical experts. BMC Medical Research Methodology, 23(1):76, 2023. Joris M. Mooij, Jonas Peters, Dominik Janzing, Jakob Zscheischler, and Bernhard SchÃ¶lkopf. Distinguishing cause from effect using observational data: Methods and benchmarks. Journal of Machine Learning Research, 17(32): 1â€“102, 2016. URL http://jmlr.org/papers/v17/14-518.html. Michael S Moore. Causation and responsibility: An essay in law, morals, and metaphysics. Oxford University Press on Demand, 2009. Thomas Nadelhoffer. Desire, foresight, intentions, and intentional actions: Probing folk intuitions. Journal of Cognition and Culture, 6(1-2):133â€“157, 2006. Meike Nauta, Doina Bucur, and Christin Seifert. Causal discovery with attention-based convolutional neural networks. Machine Learning and Knowledge Extraction, 1(1):312â€“340, 2019. Nhan Nguyen and Sarah Nadi. An empirical evaluation of github copilotâ€™s code suggestions. In Proceedings of the 19th International Conference on Mining Software Repositories , MSR â€™22, pp. 1â€“5, New York, NY , USA, 2022. Association for Computing Machinery. ISBN 9781450393034. doi: 10.1145/3524842.3528470. URL https://doi.org/10.1145/3524842.3528470. Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730â€“27744, 2022. 37Judea Pearl. Causal inference in statistics: An overview. 2009a. Judea Pearl. Causality: Models, Reasoning, and Inference. Cambridge University Press, 2 edition, 2009b. Judea Pearl. Causality. Cambridge university press, 2009c. Jonas Peters, Dominik Janzing, and Bernhard SchÃ¶lkopf. Elements of causal inference: foundations and learning algorithms. The MIT Press, 2017. Jonathan Phillips, Jamie B Luguri, and Joshua Knobe. Unifying moralityâ€™s influence on non-moral judgments: The relevance of alternative possibilities. Cognition, 145:30â€“42, 2015. Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021. Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can- cedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023. Amit Sharma, Vasilis Syrgkanis, Cheng Zhang, and Emre KÄ±cÄ±man. Dowhy: Addressing challenges in expressing and validating causal assumptions. arXiv preprint arXiv:2108.13518, 2021. Amit Sharma, Hua Li, and Jian Jiao. The counterfactual-shapley value: Attributing change in system metrics. arXiv preprint arXiv:2208.08399, 2022. Shohei Shimizu, Patrik O Hoyer, Aapo HyvÃ¤rinen, Antti Kerminen, and Michael Jordan. A linear non-gaussian acyclic model for causal discovery. Journal of Machine Learning Research, 7(10), 2006. Sanchit Sinha, Hanjie Chen, Arshdeep Sekhon, Yangfeng Ji, and Yanjun Qi. Perturbing inputs for fragile interpretations in deep natural language processing, 2021. Steven A Sloman and David Lagnado. Causality in thought. Annual review of psychology, 66:223â€“247, 2015. Gregory T Smith. On construct validity: issues of method and measurement. Psychological assessment, 17(4):396, 2005. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, AdriÃ  Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022. Mirac Suzgun, Nathan Scales, Nathanael SchÃ¤rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, , and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Natasa Tagasovska, ValÃ©rie Chavez-Demoulin, and Thibault Vatter. Distinguishing cause from effect using quantiles: Bivariate quantile causal discovery. In Hal DaumÃ© III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning , volume 119 of Proceedings of Machine Learning Research , pp. 9311â€“9323. PMLR, 13â€“18 Jul 2020. URL https://proceedings.mlr.press/v119/tagasovska20a.html. Svetlana TomiÂ´c, Silva Soldo-ButkoviÂ´c, Biserka KovaË‡c, Dario Faj, Stjepan JuriÂ´c, Sanja MiÅ¡eviÂ´c, Lidija KneÅ¾eviÂ´c, and Darko VukaÅ¡inoviÂ´c. Lumbosacral radiculopathyâ€“factors effects on itâ€™s severity. Collegium antropologicum, 33(1): 175â€“178, 2009. Ruibo Tu, Kun Zhang, Bo Bertilson, Hedvig Kjellstrom, and Cheng Zhang. Neuropathic pain diagnosis simulator for causal discovery algorithm evaluation. Advances in Neural Information Processing Systems, 32, 2019. 38Ruibo Tu, Chao Ma, and Cheng Zhang. Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis. arXiv preprint arXiv:2301.13819, 2023. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022a. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b. Moritz Willig, Matej ZeË‡ceviÂ´c, Devendra Singh Dhami, and Kristian Kersting. Can foundation models talk causality? arXiv preprint arXiv:2206.10591, 2022. MORITZ Willig, MATEJ ZEË‡CEVI Â´C, DEVENDRA SINGH DHAMI, and KRISTIAN KERSTING. Causal parrots: Large language models may talk causality but are not causal. 2023. James Woodward. Sensitive and insensitive causation. The Philosophical Review, 115(1):1â€“50, 2006. Pengzhou Wu and Kenji Fukumizu. Causal mosaic: Cause-effect inference via nonlinear ica and ensemble method. In International Conference on Artificial Intelligence and Statistics, pp. 1157â€“1167. PMLR, 2020. Yue Yu, Jie Chen, Tian Gao, and Mo Yu. Dag-gnn: Dag structure learning with graph neural networks. In International Conference on Machine Learning, pp. 7154â€“7163. PMLR, 2019. Cheng Zhang, Stefan Bauer, Paul Bennett, Jiangfeng Gao, Wenbo Gong, Agrin Hilmkil, Joel Jennings, Chao Ma, Tom Minka, Nick Pawlowski, and James Vaughan. Understanding causality with large language models: Feasibility and opportunities, 2023. Kun Zhang and Lai-Wan Chan. Extensions of ica for causality discovery in the hong kong stock market. In Neural Information Processing: 13th International Conference, ICONIP 2006, Hong Kong, China, October 3-6, 2006. Proceedings, Part III 13, pp. 400â€“409. Springer, 2006. Kun Zhang and Aapo HyvÃ¤rinen. Causality discovery with additive disturbances: An information-theoretical perspective. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2009, Bled, Slovenia, September 7-11, 2009, Proceedings, Part II 20, pp. 570â€“585. Springer, 2009. Kun Zhang and Aapo Hyvarinen. On the identifiability of the post-nonlinear causal model. arXiv preprint arXiv:1205.2599, 2012. Xun Zheng, Bryon Aragam, Pradeep K Ravikumar, and Eric P Xing. Dags with no tears: Continuous optimization for structure learning. Advances in neural information processing systems, 31, 2018. LYU Zhiheng, Zhijing Jin, Rada Mihalcea, Mrinmaya Sachan, and Bernhard SchÃ¶lkopf. Can large language models distinguish cause from effect? In UAI 2022 Workshop on Causal Representation Learning, 2022. Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models, 2023. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022. 39A Causal graph generation: Additional details Model version. For all experiments, we use gpt-3.5-turbo-0613 and gpt-4-0613 model versions. A.1 Example prompts for the Tubingen benchmark Two prompts per pair Template: - Does changing {A} cause a change in {B}? Please answer in a single word: yes or no. - Does changing {B} cause a change in {A}? Please answer in a single word: yes or no. Examples: - Does changing the altitude cause a change in temperature? Please answer in a single word: yes or no. - Does changing the temperature cause a change in altitude? Please answer in a single word: yes or no. - Does changing a ballâ€™s speed at the beginning of a ball track cause a change in its speed at the end of the track? Please answer in a single word: yes or no. - Does changing a ballâ€™s speed at the end of a ball track cause a change in its speed at the beginning of the track? Please answer in a single word: yes or no. Single prompt Template: - Which cause-and-effect relationship is more likely? A. changing {A} causes a change in {B}. B. changing {B} causes a change in {A}. Letâ€™s work this out in a step by step way to be sure that we have the right answer. Then provide your final answer within the tags <Answer>A/B</Answer>. Example: - Which cause-and-effect relationship is more likely? A. changing the altitude causes a change in temperature. B. changing the temperature causes a change in altitude. Letâ€™s work this out in a step by step way to be sure that we have the right answer. Then provide your final answer within the tags <Answer>A/B</Answer>. Table 14: Two kinds of prompt templates for the TÃ¼bingen benchmark. The first asks two questions per variable pair whereas the second (â€œSingle promptâ€) asks a single question to orient each pairwise edge. A.2 Example LLM outputs for the Neuropathic pain dataset An example of a correct output from gpt-3.5-turbo. LLM : First, we need to understand the terms involved in the cause-and-effect relationship. Left T6 Radiculopathy refers to a condition where there is damage or irritation to the nerve root that exits the spinal cord at the level of T6 on the left side of the body. 40DLS T5-T6 refers to degenerative changes in the discs, ligaments, and/or bones at the level of T5-T6 in the spine. Now, letâ€™s consider the two options: A. Left T6 Radiculopathy causes DLS T5-T6. This would mean that the damage or irritation to the nerve root at T6 on the left side of the body somehow leads to degenerative changes in the discs, ligaments, and/or bones at the level of T5-T6. While it is possible that nerve damage could contribute to degenerative changes in the spine, it is more likely that the opposite is true. B. DLS T5-T6 causes Left T6 Radiculopathy. This option suggests that the degenerative changes in the discs, ligaments, and/or bones at the level of T5-T6 are causing the Left T6 Radiculopathy. This is a more likely cause-and-effect relationship, as degenerative changes in the spine can lead to nerve root irritation or compression. Therefore, the more likely cause-and-effect relationship is: <Answer>B</Answer>DLS T5-T6 causes Left T6 Radiculopathy. An example of an incorrect output. LLM : First, we need to understand what DLS T5-T6 and Right T6 Radiculopathy are. DLS stands for degenerative lumbar spondylosis, which is a condition where the spinal discs in the lumbar region of the spine degenerate over time. T5-T6 refers to the thoracic vertebrae in the middle of the spine. Radiculopathy is a condition where there is compression or irritation of a nerve root, which can cause pain, numbness, or weakness in the area of the body that the nerve supplies. Based on this information, it is more likely that option B is the correct cause-and-effect relationship. This is because radiculopathy is typically caused by compression or irritation of a nerve root, which can then lead to pain or other sym ptoms in the area of the body that the nerve supplies. In this case, the Right T6 Radiculopathy is causing symptoms in the area of the body that corresponds to the DLS T5-T6 region of the spine. Therefore, the correct answer is <Answer>B</Answer>. A.3 Novel datasets created after LLMs training cutoff date Novel Tubingen dataset. Table 15 shows the novel pairs constructed from recently published books. Novel Alzheimerâ€™s dataset. Table 16 shows graph discovery accuracy for gpt-3.5-turbo and gpt-4 compared to data-based discovery algorithms such as DAG-GNN and NOTEARS. Novel Counterfactual dataset. Table 17 shows results of using LLMs like GPT-4 on the novel counterfactual dataset from Li et al. (2023). We compare GPT-4 to accuracy obtained by previous models evaluated by Li et al. (2023). B Evaluation: LLMs and counterfactual reasoning B.1 Details on CRASS counterfactual evaluation For computing accuracy, the LLM obtains a score of 1 if it outputs the correct answer option and 0 if it outputs an incorrect option. If the LLM output does contain an answer among the options A/B/C, we consider as if the LLM has skipped the question and assign a score according to the probability of obtaining the correct answer under random chance (0.33). B.1.1 Example prompt Here is an example prompt. 41Category Var1 Var2 Causal Direction Book Title Year Biology high temperature and high shear rate protein denaturation Right Sustainable Protein Sources: Advances for a Healthier Tomor- row 2023 Biology amount of algae in coral color of coral Right Amaze your brains with 900+ Cool sci- ence Facts 2023 Finance credit card score on-time payments Left Bounce Back: The Ul- timate Guide to Finan- cial Resilience 2023 Medicine Kidney damage Cirrhosis Left Introduction to Global Health 2022 Science grain size reduction metal strength Right Materials Chemistry 2023 Civil Engineering building flexibility bearing displacement right Advances in Struc- tural Dynamics 2023 Medicine inflammatory bowel diseases Mesenteric venous thrombosis Right Understanding Crime Through Forensic Sci- ences 2023 Health Oral contraceptive pills weight gain Right Balanced Life: Your Ultimate Weight Man- agement Guide 2023 Soil Science Usage of Nitrogen fertilizer Soil pH Right Soils for nutrition: state of the art 2022 Technology battery capacity ambient temperature Left Battery Management System and Its Appli- cations 2022 Table 15: A sample of 10 pairs from the Novel Tubingen dataset. Algorithm NHD No. of Predicted Edges Baseline NHD Ratio NOTEARS 0.22 10 0.32 0.69 DAG-GNN 0.37 20 0.44 0.83 gpt-3.5-turbo 0.21 21 0.38 0.55 gpt-4 0.14 25 0.48 0.28 Table 16: Results for the novel Alzheimerâ€™s dataset from Abdulaal et al. (2023). For data-based algorithms, we report the results for the best configuration. For LLMs, we report results with temperature=0. Algorithm Accuracy BERT 34.2 RoBERTa 61.4 MPNET 66.9 GPT-2 53.7 GPT-3 71.3 gpt-4 88.6 Table 17: Accuracy results for the novel counterfactual dataset. Results for models other than gpt-4 are from Table 2 in Li et al. (2023). 42PROMPT : A woman sees a fire. What would have happened if the woman had touched the fire? A: She would have seen fire. B: That is not possible. C: She would not have been burned. D: She would have been burned. Provide your answer within the tags, <Answer>A/B/C/D</Answer>. B.1.2 Example responses from gpt-4 Here we list three example responses from gpt-4 that do not match the benchmark answer. In the first two examples, the answer from gpt-4 can be considered correct. SYSTEM : You are a helpful assistant for counterfactual reasoning. USER : A man catches a ball. What would have happened if the man had caught a water balloon? A: He would have caught the ball and not the balloon. B: He would have got wet. C: He would have died. D: He would have remained dry. Letâ€™s work this out in a step by step way to be sure that we have the right answer. Then provide your final answer within the tags, <Answer>A/B/C/D</Answer>. GPT-4: Step 1: Identify the difference between the two scenarios. In the first scenario, the man catches a ball. In the second scenario, the man catches a water balloon. Step 2: Determine the consequences of the difference. Catching a ball typically does not result in getting wet or any other significant consequences. However, catching a water balloon may cause it to burst, potentially getting the man wet. Step 3: Evaluate the possible outcomes. A: He would have caught the ball and not the balloon. (This outcome does not address the counterfactual scenario where the man catches a water balloon.) B: He would have got wet. (This outcome is possible if the water balloon bursts upon being caught.) C: He would have died. (This outcome is highly unlikely and not a reasonable consequence of catching a water balloon.) D: He would have remained dry. (This outcome is possible if the man catches the water balloon without it bursting.) Step 4: Choose the most likely outcome. The most likely outcome is that the man would have either gotten wet or remained dry, depending on whether the water balloon burst or not. Since we cannot determine with certainty whether the water balloon would burst, we cannot definitively choose between options B and D. <Answer>Uncertain between B and D</Answer> SYSTEM : You are a helpful assistant for counterfactual reasoning. USER : A man does not get very nervous. What would have happened if he had gotten very nervous? A: That is not possible. B: The man would have passed out. C: The man would not have passed out. 43Letâ€™s work this out in a step by step way to be sure that we have the right answer. Then provide your final answer within the tags, <Answer>A/B/C</Answer>. GPT-4: Step 1: Identify the counterfactual situation - The man gets very nervous. Step 2: Consider the possible outcomes of the counterfactual situation. a) The man passes out due to extreme nervousness. b) The man does not pass out but experiences other symptoms of nervousness. Step 3: Evaluate the likelihood of each outcome. a) Passing out due to extreme nervousness is possible, but not guaranteed. b) It is more likely that the man would experience other symptoms of nervousness without necessarily passing out. Final Answer: <Answer>C</Answer> Below we show an example where people may easily understand that the man is walking on his bed indoors, but the LLM incorrectly assumes that he continues walking outdoors on a bed towards the same destination. SYSTEM : You are a helpful assistant for counterfactual reasoning. USER : A man walks on a street. What would have happened if a man had walked on a bed? A: He would have been late. B: Nothing special would have happened. C: He would have arrived on time. Letâ€™s work this out in a step by step way to be sure that we have the right answer. Then provide your final answer within the tags, <Answer>A/B/C</Answer>. GPT-4: Step 1: Consider the difference between walking on a street and walking on a bed. - Walking on a street is a normal activity and allows for efficient movement from one place to an other. - Walking on a bed is unusual and would likely slow down the personâ€™s movement due to the soft and unstable surface. Step 2: Evaluate each option based on this difference. A: He would have been late. - This option suggests that walking on a bed would have slowed the man down, making him late for his destination. B: Nothing special would have happened. - This option implies that walking on a bed would not have had any significant impact on the manâ€™s journey. C: He would have arrived on time. - This option suggests that walking on a bed would not have affected the manâ€™s arrival time at his destination. Step 3: Determine the most likely outcome. - Given that walking on a bed is an unusual and inefficient way to move, it is most likely that the man would have been slowed down and potentially arrived late at his destination. <Answer>A</Answer> C Evaluation: Necessary and Sufficient Causes C.1 Classes of token causality vignette scenarios For a discussion of the different scenario classes, see Chapter 3 in Kueffner (2021). Below we provide a brief summary. 1. Symmetric Overdetermination: Multiple processes that lead to the same outcome, complete at the same time. 442. Switch: An action chooses between multiple processes, each of which lead to the same outcome. Thus, the action becomes immaterial for the outcome. 3. Late preemption: Two processes are running in parallel. Both would produce the same outcome, but one completes before the other. 4. Early preemption: Two processes would produce the same outcome, but one completes before the other has started. 5. Double preemption: A focal process leads to an event. An action that would have stopped the focal process, is itself stopped by another process. 6. Bogus preemption: An action stops a focal process, but the focal process itself is inactive wrt its effect on the outcome. 7. Short circuit: An original action makes the focal process inactive. An action is taken to prevent this inactive process; it triggers the process but the process is unable to complete because of the original action. C.2 Prompt generation for necessary and sufficient causes Given the complicated definitions of token causality (see, for example, definitions of â€œactual causality\" in Chapter 2 of Halpern (2016)), it is non-trivial to express them in natural language. We thus turned to the LLM itself to suggest prompts to use. Below we describe a method to generate an appropriate prompt for a given task, based on Zhou et al. (2022). For generating a prompt that instructs an LLM to identify the necessary cause, we provide a meta-prompt that gives a few demonstrations of the task (vignettes as input and the necessary cause as output) and then asks the LLM to generate the instruction for that task. We then use a paraphrased version of the LLMâ€™s instruction to create our prompt. Specifically, we provide three example vignettes with the scenario and correct token cause and ask the LLM to output the instructions that must have led to the output in each example. After a few executions, the LLM provided a reasonable principle called â€œminimal rewritingâ€. We tweaked the wording to the more common â€œminimal changeâ€ principle and use it in our prompt. Here is the meta-prompt and its response from gpt-3.5-turbo. META -PROMPT : I taught my friend some concepts in counterfactual reasoning. Based on my teaching, they provided the following output, given an input scenario and candidate outputs. Input: A window shatters when it is hit by a bullet. Alice and Bob each fire a bullet at a window, simultaneously striking the window, shattering it. Candidate outputs: Both Alice and Bob, Any of Alice or Bob Output: Both Alice and Bob Input: Alice, Bob, Carol, Dave and Eve all fire at a window. The window shatters after three hits. Candidate outputs: All five individuals, any individual, the first three individuals who fired Output: The first three individuals who fired Input: Alice fills a sink with water. At each time interval Alice adds another drop of water. At one point the sink overflows. Candidate outputs: All droplets added to sink, Only the last droplet added to sink Output: Only the last droplet added to sink What logic was the friend using to select the correct output? GPT-3.5- TURBO : : It seems that your friend was applying the principle of minimal rewriting to select the correct output. The principle of minimal rewriting is a criterion used in counterfactual reasoning to evaluate which changes to the initial situation are necessary to obtain a given outcome. 45Vignette Type Input Context Event Actor Nec. Suff. Overdetermination There is a fire in the chemistry lab. A can of water would douse the fire. Agents X and Y both spray a can of water each, dousing the fire. fire being doused Agent X No Yes Switch Reagent X is added to a mixture, which leads to an explosion and kills Sam. Otherwise, Reagent Y in Samâ€™s pocket would have in- fected him and killed him as well. Samâ€™s death Reagent X No Yes Late preemption Any of Reagent X or Reagent Y can be added to a mixture to convert it into a crystal. Reagent X is added first and the mixture turns to crystal. Reagent Y is added later and but does not mix since the crystal is already formed. crystal formation Reagent X No Yes Early preemption Sam reaches out and catches the falling test tube containing a con- taminating reagent. The next thing on the test tubeâ€™s trajectory was a soft foam mattress which would have prevented the test tube from reaching the floor. Beyond that there was the floor. floor being con- taminated Sam No Yes Double preemption Sam intends to add reagent X to a mixture that would convert it to a crystal. Riya intends to prevent Sam from adding the reagent. Riya tries to stop Sam from adding the reagent, but Riya is stopped by Frank. Sam adds the reagent and the crystal is formed. crystal formation Sam Yes No Bogus preemption Sam intents to put Reagent X in a mixture to make it explode. How- ever, Sam does not put the reagent in the mixture. Bob puts an anti-explosion Reagent Y into the mixture. The mixture would explode if Reagent X is added without the addition of Reagent Y . Mixture does not explode. mixture not ex- ploding Sam No Yes Short circuit A test tube contains a mixture. Sam puts sand in the mixture. Adding sand to the mixture protects it against explosion. Only if Sam puts sand in the mixture, Riya will add Reagent Y to the mixture. Adding Reagent Y to a mixture makes it explode, unless the mixture already contains sand. There is no explosion. avoiding explo- sion Sam No Yes Miscellaneous Sam is heating Reagent X in a test tube. If the heat is not stopped at the right time, the test tube would explode. Sam does not stop the heat. The test tube explodes. explosion Samâ€™s inaction Yes Yes Table 18: â€œlab-vignettesâ€: Examples of novel vignettes for evaluation of inferring necessary and sufficient causes. Each vignette is associated with two questions: â€œIs {Actor} a necessary cause of {Event}?â€ and â€œIs {Actor} a sufficient cause of {Event}?â€ In this case, your friend was likely evaluating which minimal changes to the input scenario would lead to the candidate outputs. By comparing the input scenario to the candidate outputs, your friend identified which changes were necessary to obtain the desired outcome, and selected the output that required the least amount of change. For example, in the first scenario, your friend identified that both Alice and Bob firing their bullets simultaneously was the minimal change needed to shatter the window. Therefore, they selected the output â€œAlice and Bobâ€ as the correct answer, as it required the least amount of rewriting. Similarly, in the second scenario, your friend identified that the minimal change to shatter the window was the first three individuals firing. Therefore, they selected â€œThe first three individuals who firedâ€œ as the correct output. Overall, your friends approach demonstrates a good understanding of counterfactual reasoning and the principle of minimal rewriting. We follow the same procedure for creating a prompt for identifying the sufficient cause. One of the LLM outputs mentions the â€œmultiple sufficient causesâ€ principle, which we use in our prompt. C.3 â€œLab-vignettesâ€: A novel token causality benchmark To avoid LLM memorization issues, we construct a new dataset for inferring necessary and sufficient causes, based on the available dataset. Specifically, we perturb the details in the original dataset and adapt it for scenarios in a chemistry lab, involving reagents, mixtures and crystals. Table 18 shows example vignettes from this dataset. We apply our memorization test to validate that our newly created dataset cannot be recreated by the LLM. 46C.4 Example responses from LLMs C.4.1 Correct response on an Early Preemption vignette We show the answers from gpt-3.5-turbo and GPT-4 on the example prompt in main text. For necessary cause, both models yield the correct answer. GPT-3.5- TURBO : : If Alice had not caught the passing cricket ball, the ball would have hit the solid brick wall and stopped there. Therefore, the window would not have been hit by the ball and would have remained intact. Hence, Alice is not a necessary cause for the window being intact. <Answer>No</Answer> GPT-4: : To determine whether Alice is a necessary cause for the window being intact, we can apply the principle of minimal change. Letâ€™s consider the counterfactual situation where Alice does not catch the ball. In the counterfactual situation, the ball would continue on its trajectory and hit the solid brick wall. As the brick wall would have stopped the ball, it would not have continued towards the window. Therefore, the window would still be intact even if Alice had not caught the ball. <Answer>No</Answer> For sufficiency, however, only GPT-4 provides the correct answer (Aliceâ€™s action is sufficient). GPT-3.5- TURBO : : No, Alice is not a sufficient cause for the window being intact. While Alice catching the ball prevented it from hitting the brick wall, there could have been other factors that prevented the ball from breaking the window, such as the ball losing momentum or the window being made of sturdy material. Therefore, there are multiple sufficient causes for the window being intact. <Answer>No</Answer>. GPT-4: : Using the principle of multiple sufficient causes, we can reason that there are multiple potential causes that could result in the window remaining intact. In this scenario, Alice catching the ball is one sufficient cause, as it prevents the ball from hitting the window. Another sufficient cause would be the solid brick wall, which would have stopped the ball if Alice hadnâ€™t caught it. Given that there are multiple sufficient causes, Aliceâ€™s action of catching the ball is not the sole determinant of the window remaining intact. However, her action is one of the sufficient causes that contributed to the outcome. <Answer>Yes</Answer> C.4.2 Unpredictable failure mode: An example Using the short circuit vignette, we show an example of gpt-4â€™s unpredicable failure modes. It applies the correct reasoning for most prompts, but applies an incorrect reasoning (assessing necessity only instead of sufficiency) for the following prompt. We show the input prompt and gpt-4â€™s answer below. PROMPT : You are an expert in counterfactual reasoning. Given an event, use the principle of minimal change to answer the following question. Input: Carol is alive (CA). Alice puts a harmless antidote in Carolâ€™s water (AA). Adding antidote to the water, protects it against poison (WS - â€™water saveâ€™). If Alice puts the antidote into Carolâ€™s water, Bob will poison the water (BP) Adding poison to an unprotected water makes it toxic (WT). If Carol would drink toxic water she would die (i.e. inhibiting CS). Carol consumes her water and survives (CS). Is Alice a necessary cause for Carolâ€™s survival? After your reasoning, provide the final answer within the tags <Answer>Yes/No</Answer>. GPT-4: : â€œWe can set up the causal chain as follows: 471. Alice puts the antidote in Carolâ€™s water (AA). 2. Antidote in the water protects it against poison (WS). 3. If Alice puts the antidote into Carolâ€™s water, Bob will poison the water (BP). 4. Adding poison to unprotected water makes it toxic (WT). 5. If Carol drinks toxic water, she would die (not CS). To determine if Alice is a sufficient cause for Carolâ€™s survival, letâ€™s consider a counterfactual scenario: If Alice had not put the antidote in Carolâ€™s water, would Carol have survived? In this scenario, we have: 1. Alice does not put the antidote in Carolâ€™s water (not AA). 2. Antidote in the water does not protect it against poison (not WS). 3. Bob does not poison the water (not BP). 4. The water remains nontoxic (not WT). 5. Carol consumes her water and survives (CS). In this counterfactual scenario, Carol still survives even if Alice does not put the antidote in her water. This indicates that Aliceâ€™s action was not a sufficient cause for Carolâ€™s survival, as Carol would have survived even without Aliceâ€™s intervention. <Answer>No</Answer> D Assessing Responsibility So far we looked at necessary or sufficient causes, which are binary: an event is either a cause or not a cause of another event. In natural language, the causal judgments we make are relative, e.g., we might ascribemore causality to â€œsmoking in the woods\" than to â€œpresence of dry leaves in Summer.â€ Formal token causality models capture this with graded causation, nonbinary measures of token causation that enable comparisons between causal events Halpern & Hitchcock (2015). In natural language we might use the simple concept of â€œresponsibility.â€ Responsibility is a natural way to rank causal events in everyday language, e.g., â€œsmoking in the woods was more responsible for the forest fire than the presence of dry leaves in Summer.â€ Halpern (2016); Chockler & Halpern (2004) provide an SCM-based definition of responsibility; in simple terms, they define responsibility of event C for event E is 1/(N+1), where N is the minimal number of other causal events contributing to E that have had not happen to make C a necessary cause of E. But mapping the SCM definition of responsibility back to plain language is nontrivial, just as it was with model-based P(EC=1 = 1|C = 0, E= 0)-definition of sufficiency back to plain language was challenge. This SCM definition is purely a measure of causal contribution. But in everyday language, â€œresponsibilityâ€ combines a degree of causal contribution with the concepts of intention, epistemic state, moral obligation, etc. mentioned earlier. So for this case study, we attempted to scope the definition of responsibility to focus on causal contribution. Responsibility: The relative degree to which an event causally contributes to a particular outcome event, relative to other events that caused that outcome. Given two causal events that cause the same outcome event, one causal event can be more, less, or equally responsible relative to the other. This definition of responsibility does not depend on whether the causal event is a personâ€™s action. D.0.1 Inferring responsibility: Motivating Example Taking our cue from the LLMâ€™s ability to define relevant variables on the fly, our definition allows the LLM to provide a relative ranking of causes, rather than quantify an absolute responsiblity value. Further, we make it clear that responsibility is the same regardless of whether the causal event was a personâ€™s action; we hope that if the definition of responsibility is the same for both inanimate object and people, then we cue the LLM to focus on causal contribution. Moreover, we cue the definition to ignore intention, morality, epistemic state, etc., or at the very least to limit those factors to levels that people usually ascribe inanimate objects when they are anthropormorphized in everyday language 48(e.g. â€œcheated by fateâ€, â€œthe universe is telling you something,â€ â€œmy phone is acting up,â€ etc.) We then prompted the LLM to use this definition for the beer spilling example. PROMPT : In the context of this dialog, define â€œresponsibilityâ€ as the relative degree to which an event causally contributes to a particular outcome event, relative to other events that caused that outcome. Given two causal events that cause the same outcome event, one causal event can be more, less, or equally responsible relative to the other. This definition of responsibility does not depend on whether the causal event is a personâ€™s action. Based on this definition of responsibility, make a one sentence statement about how responsible is Mikeâ€™s bumping of the table is for the spilling of the beer. LLM : Based on this definition of responsibility, Mikeâ€™s bumping of the table is highly responsible for the spilling of the beer, as it was the immediate and direct cause that led to the beer bottle falling and spilling. We now introduce two challenging scenarios. These are based on overdetermination and double preemption, two of the scenario types we saw in Section 4.2 for deciding necessary and sufficient causes. Overdetermination. Overdetermination is one example where a simple counterfactual dependence definition of actuality falls short relative to human judgments. To illustrate we introduce a new information to the beer spilling story, that Susan bumped into the table shortly after Mike and with enough force to spill the beer had it still been there. With this additional information, Mikeâ€™s bump is no longer a necessary cause of the spill; if he hadnâ€™t bumped the table, the beer still would have spilled. Despite the new lack of counterfactual dependence, humans still judge that Mike caused the spill. Analogously, if two children throw a rock at a window, most people judge that they both children are window breakers, regardless of whose rock struck first and broke the glass. First we make sure the LLM can assess that counterfactual dependence is no longer present. PROMPT : You said â€œif Mike hadnâ€™t bumped into the table, the beer bottle would not have fallen and spilled.â€ Call this statement A. Suppose that Susan tripped and fell onto the table in the instant after Mike bumped into the table and the bottle started falling. If the bottle had not already fallen, Susanâ€™s fall would have caused the bottle to fall. Assuming scenario A was true before, can statement A still be true? Provide a one sentence explanation why or why not. LLM : No, scenario A can no longer be considered true because if Susan fell onto the table in the instant after Mike bumped it, Susanâ€™s fall would have been the determining factor causing the beer bottle to fall and spill, regardless of Mikeâ€™s initial bump. This result aligns with our intuition. Now, we expect that, given that there are two causes of the beer spilling, the LLM should be able downgrade Mikeâ€™s level of responsibility relative to before. PROMPT : How, if at all, does this new information change your assessment of the responsibility of Mikeâ€™s table bump? LLM : With this new information, the responsibility of Mikeâ€™s table bump for the spilling of the beer is reduced, as Susanâ€™s fall would have also led to the same outcome, making both events contributing causal factors to the beer spill. Double prevention. Double prevention describes another case where humans ascribe causality to events that are counterfactually independent of the outcome. In double prevention, we have an initial causal event C that causes the outcome. As before C is an event that causes outcome E. Now we introduce a new causal event chain Aâ†’Bâ†’E. If B occurs E does not occur. If A occurs B does not occur. Thus A prevents B from preventing E. In our beer spilling example, Mike bumps the table (event C), which causes the beer bottle to fall (event E). Jack sees the bottle falling and tries to catch it (event B) and thus prevent event E. but Peter bumps into Jack (event A), which causes Jack to miss the bottle . 49In this case, both A and E have counterfactual dependence; the outcome would not have occurred if A had not occurred. But humans tend to judge that Mikeâ€™s bumping the table caused the beer to spill and that Peterâ€™s bumping into Jack, the double preventer, did not Chang (2009); Lombrozo (2010); Moore (2009). In the following prompt, we extend the beer spilling scene to include a double prevention scenario. In this case, Mike is bumps the table (event A), Jack tried to prevent it (event B), but Peter interferes with Jackâ€™s attempt (event C). We expect that the LLM should assign more responsibility to Mike than Peter. PROMPT : Suppose that Jack saw that the bottle was about to fall, and he reached out to catch the bottle. But just as Jack was about to catch the bottle, Peter bumped into Jack, which caused him to miss the bottle, which then fell to the ground. Who is more responsible for the bottle falling to the ground? Mike or Peter? LLM : In this scenario, Peter is more responsible for the bottle falling to the ground, as his bumping into Jack directly interfered with Jackâ€™s attempt to catch the bottle and prevent it from falling. While Mikeâ€™s bumping the table initially set the events in motion, Peterâ€™s action had the most immediate impact on the outcome. Here, we observe a failure of the ChatGPT4 to capture the nuance of double prevention at least by human standards. E Probing LLM Behaviors Further E.1 Memorization Tests The system message used for a memorization test of row based datasets, including the TuÂ¨bingen and CRASS datasets are: SYSTEM : You are an AI assistant that has read many sources of text from the internet. I am looking at text from the <DATASET NAME AND DESCRIPTION>dataset, published by <AUTHORS>as <PUBLICATION INFO>. Here is the README for the dataset: <README> I am going to list some sample rows of data, and I want you to complete each row as best as possible. I am testing your memory. USER : pair0005 Age ASSISTANT : pair0005 Age Length Abalone ->&nbsp; To test for memorization of datasets where the original file is not split into rows or lines, we sample a random N-character long section from the original dataset, and ask the LLM to complete the original file from there. The system message used for a memorization test of JSON based dataset, where the original file is not split into rows or lines, are: SYSTEM : You are an AI assistant that has read many sources of text from the internet. I am looking at text from the <DATASET NAME AND DESCRIPTION>dataset, published by <AUTHORS >as <PUBLICATION INFO >. Here is the README for the dataset: <README > I am going to provide a random 500 character subsection from the JSON-formatted data, and I want you to complete the next 500 characters of the data. I am testing your memory. USER : <500 characters from data > ASSISTANT : ... The results of our memorization tests are presented in table 19. Our memorization test shows the TÃ¼bingen dataset is in the training dataset and has been at least partially memorized by GPT-3.5 and GPT-4. The neuropathic pain dataset 50% Columns Memorized Dataset GPT-3.5 GPT-4 TÃ¼bingen Cells 58.9% 61% Rows 19.8% 25% Dataset Avg number of characters recovered Sample section length (num chars) GPT-3.5 Turbo GPT-4 Neuropathic 100 17 25 200 19 34 400 136 22 Arctic Sea Ice 100 2 2 200 4 10 400 34 3 CRASS 100 14 2 200 10 7 400 10 7 Table 19: Our memorization tests results show the TuÂ¨bingen dataset is in the training dataset for GPT-3.5 and GPT-4, the neuropathic dataset maybe partially memorized, but the other datasets seem to not be memorized. We run a cell and row based memorization test for the tabular TuÂ¨bingen dataset and an unstructured memorization test for the other datasets. The unstructured test counts the average number of characters the LLM is able to reproduce following a randomly sampled section of the given length. seems to be partially memorized by GPT-3.5 Turbo, but results with GPT-4 are unclear. The Arctic Sea Ice and CRASS datasets do not show signs of having been memorized, as the small number of recovered tokens are related to language or formatting that can be predicted from the sample text. E.2 Perturbation Test Example We give a brief illustration of a perturbation experiment in Figure 5, which shows a GPT 3.5 model giving an erroneous answer to an arithmetic problem, even when correctly solving subparts of the problem when prompted. We might wonder whether the LLM has learned a pattern that the final answer must match its initial answer, instead of a more correct pattern that the final answer must match the preceding arithmetic expression. With LLMs, we can run a perturbation experiment to test this hypothesis, intervening on the first answer to study its influence on the final answer. By redacting then perturbing each of the tokens in the prompt, one by one, we can determine which tokens are leading to the error and better understand reliabilities. Once we have a target token or word, we can perturb its value systematically to identify how it influences the final answer. Table 20 shows that the perturbation to the first answer of Figure 5 is often directly reflected in the final result. Interestingly, as the perturbation value increases, the final answer is more likely to snap to the correct result instead. F Supplementary Discussion F.1 Example of LLM-assisted Causal Analysis Code Generation Here, we present, without comment, a brief case study asking GPT-4 to generate python code for a demonstration causal analysis. 51Session 1: Q: What is 8 * 8 + 5 * 12? A: 104 Q: Please show your work A: 8 * 8 = 64 5 * 12 = 60 64 + 60 = 104 Session 2: Q: What is 8 * 8 + 5 * 12? A: 110 Q: Please show your work A: 8 * 8 = 64 5 * 12 = 60 64 + 60 = 110 Figure 5: LLM completions are highlighted in yellow. Session 1: In a multi-turn interaction, GPT3.5 first gives an erroneous response to a math problem. Prompted to show its work, it correctly solves the sub-parts of the problem, but again gives the wrong final answer. Session 2: we probe for the influence of the first wrong answer on the final wrong answer by replaying an interventional-conversation and asking GPT3.5 to complete only the final answer. USER : could you give me a jupyter notebook that creates synthetic data for a causal problem (please make a simple DAG and data generator); then calls CMUâ€™s causal-learn package to learn a dag; then calls DoWhy and EconML to estimate a specific treatment effect using the learned graph, and validates with sensitivity analyses I cannot directly provide you with a Jupyter Notebook file, but I can give you a step-by-step guide on how to create one yourself. In this example, weâ€™ll create a simple synthetic dataset with 3 variables, where X1 -> X2 -> Y and X1 -> Y , and use causal-learn, DoWhy, and EconML to estimate the treatment effect of X1 on Y . 1. Install the required packages: 1 !pip install numpy pandas causal-learn dowhy econml 2. Import the necessary libraries: 1 import numpy as np 2 import pandas as pd 3 from causal_learn import causal_graph 4 from dowhy import CausalModel 5 from econml.metalearners import TLearner 6 from sklearn.linear_model import LinearRegression 3. Generate synthetic data: 1 np.random.seed(42) 2 n_samples = 1000 3 4 # Generating X1 (treatment variable) 5 X1 = np.random.normal(0, 1, n_samples) 6 7 # Generating X2 (mediator variable) 8 X2 = 0.5 * X1 + np.random.normal(0, 1, n_samples) 9 52Perturbation Result 100 100 101 101 102 124 103 124 104 104 105 104 106 104 107 107 108 124 109 109 110 110 111 111 112 112 113 124 114 114 115 115 116 124 117 124 118 124 119 119 120 120 121 124 122 122 123 124 124 124 125 124 126 124 127 127 128 124 129 124 130 130 131 124 132 124 133 124 134 124 135 124 + 11 = 135 136 124 137 124 138 124 139 124 Table 20: Perturbing the first answer from Figure 5, and observing the result shows that the first answer strongly influences the final answer 10 # Generating Y (outcome variable) 11 Y = 2 * X1 + 0.8 * X2 + np.random.normal(0, 1, n_samples) 12 13 # Create a dataframe 14 data = pd.DataFrame({â€™X1â€™: X1, â€™X2â€™: X2, â€™Yâ€™: Y}) 534. Learn the causal graph using the causal-learn package: 1 graph = causal_graph.CausalGraph(df=data) 2 graph.estimate_graph() 3 print(graph.graph_) 5. Use DoWhy to create a causal model: 1 # Create a causal model using the learned graph 2 causal_model = CausalModel( 3 data=data, 4 treatment=â€™X1â€™, 5 outcome=â€™Yâ€™, 6 graph=graph.graph_.to_string() 7 ) 8 9 # Identify causal effect 10 identified_estimand = causal_model.identify_effect() 6. Estimate the treatment effect using EconML: 1 # Train a T-learner using linear regression 2 t_learner = TLearner(LinearRegression(), LinearRegression()) 3 t_learner.fit(Y=data[â€™Yâ€™], T=data[â€™X1â€™], X=data[[â€™X2â€™]]) 4 5 # Estimate the treatment effect 6 treatment_effect = t_learner.const_marginal_effect(data[[â€™X2â€™]]) 7. Validate the results using sensitivity analyses: 1 # Perform sensitivity analysis using DoWhy 2 refutation_results = causal_model.refute_estimate( 3 identified_estimand, 4 treatment_effect, 5 method_name=â€™random_common_causeâ€™ 6 ) 7 8 print(refutation_results) This example demonstrates how to create synthetic data for a causal problem, learn a DAG using the causal-learn package, estimate treatment effects using DoWhy and EconML, and perform sensitivity analyses. Note that the actual treatment effect and learned treatment effect may differ due to noise in the data and the assumptions made by the methods used. F.2 Example of LLM-assisted Identification of Negative Controls Here, we present, without comment, a brief case study asking GPT-4 to generate negative and positive controls. We add list formatting for readability. USER : In a causal observational study of vaccine efficacy, we are matching people who have voluntarily gotten a vaccine with individuals who have not yet done so. We are matching on attributes intended to capture all confounding effects. To help validate our analysis, what are some examples of negative controls where we might expect to see zero treatment effect? GPT-4: In a causal observational study, negative controls are variables or outcomes for which you do not expect to see a treatment effect. By including negative controls in your analysis, you can help validate your matching process 54and assess the presence of unmeasured confounding. Here are some examples of negative controls where you might expect to see zero treatment effect in a study of vaccine efficacy: Hair color: The vaccine should have no impact on the natural hair color of individuals, so you should expect no difference in hair color distribution between the vaccinated and unvaccinated groups. Blood type: There should be no relationship between receiving a vaccine and an individualâ€™s blood type. So, the distribution of blood types should be similar in both groups. Handedness: The vaccine should not affect whether a person is right-handed, left-handed, or ambidextrous, so you should expect no difference in handedness between the vaccinated and unvaccinated groups. Taste preferences: Vaccination should not influence an individualâ€™s taste preferences for certain foods or flavors, so no difference should be observed in this regard between the two groups. Birth month: There should be no relationship between receiving a vaccine and the month a person was born. Therefore, the distribution of birth months should be similar for vaccinated and unvaccinated individuals. Past unrelated medical procedures: Previous medical procedures unrelated to vaccination, such as dental work or orthopedic surgeries, should not be affected by vaccination status. As such, there should be no difference in the prevalence of these procedures between the groups. Favorite color: An individualâ€™s favorite color should not be influenced by vaccination status. Consequently, the distribution of favorite colors should be similar for both groups. Including these negative controls in your analysis can help to evaluate whether your matching process is successful in capturing all confounding effects. If you observe a treatment effect in these negative controls, it may indicate that there are unmeasured confounding factors or issues with your matching process. USER : In what subpopulations would we expect to see zero treatment effect on disease prevention? (for the disease being vaccinated against) GPT-4: There are certain subpopulations where you might expect to see zero or limited treatment effect on disease prevention for the disease being vaccinated against: Individuals with pre-existing immunity: People who have already been infected with the disease and developed immunity might not show additional benefits from vaccination. However, the vaccine may still help to boost their immunity or protect against different strains of the disease. Individuals with specific immune deficiencies: Some people have immune system disorders or are taking immunosuppressive medications that may prevent them from mounting an effective immune response to the vaccine. In such cases, the vaccine may not provide any additional protection against the disease. Non-responders to the vaccine: A small proportion of individuals might not develop an adequate immune response to the vaccine due to genetic factors or other unknown reasons. These non-responders may not experience any protection from the vaccine. Individuals vaccinated after infection: If someone receives the vaccine after already being infected with the disease, the vaccine may not have a chance to provide any additional protection. This is because the immune response generated by the vaccine might not have enough time to develop before the disease progresses. Individuals vaccinated with expired or improperly stored vaccines: If a vaccine has expired or has been stored improperly, it may lose its potency and not induce the desired immune response. In such cases, individuals receiving these vaccines may not experience any protection against the disease. Itâ€™s important to note that even if a subpopulation does not experience a treatment effect from the vaccine, the vaccine can still be beneficial for the overall population. Widespread vaccination can lead to herd immunity, which can indirectly protect individuals in these subpopulations by reducing the overall prevalence of the disease. USER : Are there any subpopulations in which we would expect a known but non-zero effect? to help validate our observational causal analysis? 55GPT-4: Yes, there are subpopulations where you might expect a known but non-zero effect of vaccination on disease prevention. These groups can serve as positive controls to help validate your observational causal analysis. Some examples include: Age groups: Vaccine efficacy can vary by age group. For instance, older adults or very young children may have a less robust immune response to vaccination compared to young adults. You may observe a reduced but non-zero effect in these age groups compared to others. Individuals with certain chronic conditions: People with specific chronic medical conditions like diabetes, cardiovascular diseases, or respiratory illnesses might have a weaker immune response to vaccines. As a result, you may see a lower but non-zero vaccine effect in these subpopulations. Pregnant individuals: Pregnancy can cause temporary changes in the immune system, which might lead to a reduced response to vaccines. Consequently, you may observe a smaller but non-zero effect in pregnant individuals compared to non-pregnant individuals. Smokers: Smoking is known to have adverse effects on the immune system, potentially leading to a reduced immune response to vaccines. In this subpopulation, you may find a lower but non-zero vaccine effect compared to non-smokers. Individuals receiving a lower vaccine dose or a different vaccination schedule: Some subpopulations might receive a lower vaccine dose or a different vaccination schedule, which can result in varying levels of protection. You may see a reduced but non-zero effect in these cases. By comparing the treatment effect in these subpopulations to the overall population or other subpopulations with expected higher vaccine efficacy, you can help validate your causal analysis. If your analysis can correctly capture these known differences in treatment effects among various subpopulations, it provides evidence that your methodology is robust and can potentially account for confounding factors. USER : if we have multiple observations per person over time. I.e., we know whether they are infected with the given disease on any given day after vaccination, then does that suggest any time-bound negative or positive controls? GPT-4: Yes, when you have multiple observations per person over time (longitudinal data), you can use time-bound negative or positive controls to help validate your causal analysis. Here are some examples: Time-bound Negative Controls: Pre-vaccination period: Compare the infection rates before vaccination between the future vaccinated and unvaccinated groups. Since neither group has received the vaccine at this stage, you should expect no difference in infection rates due to vaccination. Short time window post-vaccination: Analyze the infection rates in a very short time window (e.g., within a few days) after vaccination. The vaccineâ€™s protective effect usually takes some time to develop, so you should expect minimal to no treatment effect in this short period. Time-bound Positive Controls: Peak immune response period: Identify the time window in which the vaccineâ€™s immune response is expected to be at its peak based on clinical trial data or previous research. Compare the infection rates between the vaccinated and unvaccinated groups during this period, and you should expect to see a substantial treatment effect. Waning immunity period: Investigate the infection rates during a period when the vaccineâ€™s protective effect is known to wane or decrease. During this time, you might still expect to see a treatment effect, but it could be smaller compared to the peak immune response period. Post-booster vaccination: If booster shots are administered to maintain or enhance immunity, you can compare infection rates between vaccinated individuals who have received boosters and those who have not (or unvac- cinated individuals). You should expect to see a positive treatment effect in those who received the booster shots. 56Using these time-bound controls can help you assess the validity of your causal analysis, as you expect specific patterns in the treatment effect over time. If your analysis can capture these expected temporal patterns, it provides evidence that your methodology is reliable and able to account for time-varying confounding factors. 57",
      "references": [
        "Causal modelling agents: Causal graph discovery through synergising metadata-and data-driven reasoning.",
        "Aml-babel benchmark: Causal judgement.",
        "Obesity and low back pain: relationships and treatment.",
        "On pearlâ€™s hierarchy and the foundations of causal inference.",
        "On the dangers of stochastic parrots: Can language models be too big?",
        "Elephants never forget: Memorization and learning of tabular data in large language models",
        "Sparks of artificial general intelligence: Early experiments with gpt-4.",
        "Why did the distribution change?",
        "The Rational Imagination: How People Create Alternatives to Reality.",
        "Quantifying memorization across neural language models.",
        "Connecting counterfactual and physical causation.",
        "Evaluating large language models trained on code",
        "Toward unique and unbiased causal effect estimation from data with hidden variables.",
        "Responsibility and blame: A structural-model approach.",
        "Lmpriors: Pre-trained language models as task- specific priors.",
        "A survey of the state of explainable ai for natural language processing",
        "Bert: Pre-training of deep bidirectional transformers for language understanding.",
        "Data-driven covariate selection for nonparametric estimation of causal effects.",
        "CRASS: A novel data set and benchmark to test counterfactual reasoning of large language models.",
        "From counterfactual simulation to causal judgment.",
        "Bigbench v2: the new and improved bigbench.",
        "Review of causal discovery methods based on graphical models.",
        "Actual causality.",
        "Graded causation and defaults.",
        "Reasoning with language model is planning with world model.",
        "Causal inference",
        "Portable causal dependence: A tale of consilience.",
        "Investigating causal understanding in llms.",
        "Nonlinear causal discovery with additive noise models.",
        "Benchmarking of data-driven causality discovery approaches in the interactions of arctic sea ice and atmosphere.",
        "A resource-rational approach to the causal frame problem.",
        "Normality and actual causal strength.",
        "Causal inference in statistics, social, and biomedical sciences.",
        "Neural simulation of action: a unifying mechanism for motor cognition.",
        "Norm theory: Comparing reality to its alternatives.",
        "Unsuitability of notears for causal graph discovery when dealing with dimensional quantities.",
        "Gpt-4 passes the bar exam.",
        "Intentional action and side effects in ordinary language.",
        "Causal judgment and moral judgment: Two experiments.",
        "Proximate cause explained.",
        "Causal superseding.",
        "Towards understanding how machines can learn causal overhypotheses.",
        "A comprehensive survey of the actual causality literature.",
        "Benefits, limits, and risks of gpt-4 as an ai chatbot for medicine.",
        "The AI Revolution in Medicine: GPT-4 and Beyond.",
        "Counterfactual reasoning: Testing language modelsâ€™ understanding of hypothetical scenarios.",
        "Negative controls: a tool for detecting confounding and bias in observational studies.",
        "Evaluating the logical reasoning ability of chatgpt and gpt-4",
        "Causalâ€“explanatory pluralism: How intentions, functions, and mechanisms influence causal ascriptions.",
        "Can large language models build causal graphs?",
        "How come gpt can seem so brilliant one minute and so breathtakingly dumb the next?",
        "Telling cause from effect using mdl-based local and global regression.",
        "Modeling covid-19 disease processes by remote elicitation of causal bayesian networks from medical experts.",
        "Distinguishing cause from effect using observational data: Methods and benchmarks.",
        "Causation and responsibility: An essay in law, morals, and metaphysics.",
        "Desire, foresight, intentions, and intentional actions: Probing folk intuitions.",
        "Causal discovery with attention-based convolutional neural networks.",
        "An empirical evaluation of github copilotâ€™s code suggestions.",
        "Capabilities of gpt-4 on medical challenge problems.",
        "Training language models to follow instructions with human feedback.",
        "Causal inference in statistics: An overview.",
        "Causality: Models, Reasoning, and Inference.",
        "Causality.",
        "Elements of causal inference: foundations and learning algorithms.",
        "Unifying moralityâ€™s influence on non-moral judgments: The relevance of alternative possibilities.",
        "Multitask prompted training enables zero-shot task generalization.",
        "Toolformer: Language models can teach themselves to use tools.",
        "Dowhy: Addressing challenges in expressing and validating causal assumptions.",
        "The counterfactual-shapley value: Attributing change in system metrics.",
        "A linear non-gaussian acyclic model for causal discovery.",
        "Perturbing inputs for fragile interpretations in deep natural language processing",
        "Causality in thought.",
        "On construct validity: issues of method and measurement.",
        "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.",
        "Challenging big-bench tasks and whether chain-of-thought can solve them.",
        "Distinguishing cause from effect using quantiles: Bivariate quantile causal discovery.",
        "Lumbosacral radiculopathyâ€“factors effects on itâ€™s severity.",
        "Neuropathic pain diagnosis simulator for causal discovery algorithm evaluation.",
        "Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis.",
        "Emergent abilities of large language models",
        "Chain of thought prompting elicits reasoning in large language models.",
        "Can foundation models talk causality?",
        "Causal parrots: Large language models may talk causality but are not causal.",
        "Sensitive and insensitive causation.",
        "Causal mosaic: Cause-effect inference via nonlinear ica and ensemble method.",
        "Dag-gnn: Dag structure learning with graph neural networks.",
        "Understanding causality with large language models: Feasibility and opportunities,",
        "Extensions of ica for causality discovery in the hong kong stock market.",
        "Causality discovery with additive disturbances: An information-theoretical perspective.",
        "On the identifiability of the post-nonlinear causal model.",
        "Dags with no tears: Continuous optimization for structure learning.",
        "Can large language models distinguish cause from effect?",
        "Agieval: A human-centric benchmark for evaluating foundation models,",
        "Large language models are human-level prompt engineers."
      ],
      "meta_data": {
        "arxiv_id": "2305.00050v3",
        "authors": [
          "Emre KÄ±cÄ±man",
          "Robert Ness",
          "Amit Sharma",
          "Chenhao Tan"
        ],
        "published_date": "2023-04-28T19:00:43Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Behavioral benchmark study of large language modelsâ€™ ability to generate correct causal arguments across both type causality (causal DAG/edge discovery) and token causality (counterfactuals, necessary/sufficient causes, normality judgments). Shows that GPT-3.5/4 can infer causal direction from variable metadata (names/context) with very high accuracyâ€”often surpassing state-of-the-art covariance-based causal discovery on standard benchmarksâ€”and can answer counterfactual and token-causality primitives with strong performance. Provides evidence that performance is not solely dataset memorization via post-cutoff datasets and memorization tests; documents prompt sensitivity and unpredictable failure modes; argues LLMs can augment human causal workflows by supplying domain knowledge and natural-language interfaces, and motivates hybrid LLM+data causal pipelines. Releases code/datasets (pywhy-llm).",
        "methodology": "Treat LLMs as black-box generators of causal statements/arguments given natural-language prompts. For pairwise causal direction: prompt LLMs with variable names (two-question or single forced-choice Aâ†’B vs Bâ†’A with chain-of-thought) and score accuracy/weighted accuracy. For full-graph generation: extend prompts to three-way decisions (Aâ†’B, Bâ†’A, no edge) over variable pairs and evaluate against ground-truth graphs using F1 or normalized Hamming distance (NHD). For token causality: evaluate counterfactual multiple-choice answering (CRASS) and probe necessary/sufficient cause judgments on vignette suites using prompts that operationalize â€˜minimal changeâ€™ (necessity) and â€˜multiple sufficient causes/robust sufficiencyâ€™ (sufficiency). For robustness/validity: conduct memorization tests (row/character continuation) to estimate benchmark exposure; create novel post-training-cutoff datasets (book-mined causal pairs; COVID DAG subset; Alzheimerâ€™s DAG; synthetic counterfactual dataset; lab-adapted vignettes) to test generalization; perform redaction-based sensitivity analysis to identify prompt tokens affecting accuracy; use LLM-as-critic (e.g., GPT-4 verifying GPT-3.5 outputs) to catch self-inconsistency errors.",
        "experimental_setup": "Models: multiple OpenAI GPT variants (small to GPT-4; key results with gpt-3.5-turbo-0613 and gpt-4-0613; temperatures typically 0 for graph tasks). Tasks/benchmarks: (1) Pairwise causal discovery on TÃ¼bingen Causeâ€“Effect Pairs (108 pairs; report unweighted and Mooij et al. weighted accuracy; compare to Mosaic and other causal discovery baselines). (2) Novel â€˜post-cutoffâ€™ causal pairs mined from books published â‰¥ Oct 2021 (67 pairs) to test generalization. (3) Neuropathic pain expert graph (Tu et al.)â€”pairwise direction over 475 true edges; plus a 100-pair edge/non-edge subset for full-graph edge classification; Swedish-to-English translation via GPT-4-web preprocessing; metrics: accuracy (pairwise), precision/recall/F1 (edge existence). (4) Full-graph generation on Arctic sea-ice/atmospheric science expert graph (12 nodes, 48 edges; includes bidirected edges) with metric NHD and NHD ratio vs worst-case baseline with same edge count; compare to TCDF, NOTEARS (static/temporal), DAG-GNN (static/temporal). Also cite external evaluation on an Alzheimerâ€™s expert graph created in 2023 (11 nodes; NHD comparisons). (5) Counterfactual reasoning on CRASS (275 multiple-choice items; physics/logic/commonsense; compare to prior GPT-3/T0pp and human baseline; accuracy metric). Also evaluate on a counterfactual dataset released in 2022 (Li et al.) reformatted to multiple-choice. (6) Necessary/sufficient cause on 15 canonical actual-causality vignettes (Kueffner) and a novel â€˜lab-vignettesâ€™ set to reduce memorization; metrics: accuracy per necessity and sufficiency. (7) Normality judgment: reworded BIG-Bench Hard causal judgment vignettes; two-step prompting (extract causal event, then classify normal/abnormal) with accuracy reported. Validation/robustness methods: prompt variants (system messages like â€˜causal reasoningâ€™/â€˜counterfactual reasoningâ€™), redaction tests, memorization tests, post-cutoff datasets, and LLM-critic self-consistency checking.",
        "limitations": "LLMs operate primarily on text/metadata rather than observed data values, so they can produce plausible but wrong causal claims when variable names are ambiguous or context is missing; they may ignore key confounders/sample-size/selection issues in conversational settings (illustrated by ad A/B example). Performance is highly prompt-sensitive; small wording changes and chain-of-thought instructions can materially change accuracy. Benchmarks may be partially memorized (e.g., substantial memorization signal on TÃ¼bingen), complicating construct validity; novel datasets reduce but do not eliminate exposure risks because underlying concepts may still be in training data. LLMs exhibit unpredictable failure modes: incoherent reasoning, self-inconsistency between rationale and chosen option, errors on specific vignette types (e.g., short-circuit sufficiency), and only moderate performance on normality judgments (~70%). Graph-generation evaluation relies on expert â€˜ground truthâ€™ graphs that themselves may contain errors/ambiguities; LLM disagreements sometimes plausibly critique labels but also create uncertainty about evaluation. No formal guarantee of causal correctness or identifiability; outputs are not verifiable without external formal analysis and data. Ethical risks: LLM-generated causal explanations can create misleading veneer of rigor and may reflect hidden biases from training data; black-box nature and proprietary models reduce transparency.",
        "future_research_directions": "Develop hybrid algorithms combining knowledge-based LLM priors/critics with covariance-based causal discovery and effect estimation (e.g., use LLM outputs as structural priors, constraints, or post-processing; integrate metadata + data in new discovery formulations). Create robust, standardized prompting/verification pipelines (self-consistency checks, multi-agent critique, tool-assisted validation) to reduce prompt sensitivity and catch failures. Extend LLM assistance to downstream causal tasks: identifying adjustment sets/backdoor criteria under partial graphs, suggesting instruments, proposing/validating robustness checks (negative/positive controls, sensitivity analyses), and translating natural-language scenarios into formal SCM/DAG specifications. Build better benchmarks: post-cutoff, domain-specific, and adversarial/ambiguity-controlled datasets; leverage LLMs to help audit/correct expert graphs and to generate scalable benchmark instances while tracking memorization. Improve token-causality systems by decomposing causal judgment into primitives (necessity, sufficiency, normality, responsibility) and composing them with formal theory; study persistent gaps on normality and human-factor judgments. Investigate theoretical limits and inductive biases required for interventional/counterfactual generalization (causal hierarchy considerations), and study when LLMs â€˜world modelsâ€™ support reliable causal inference. Explore retrieval-augmented and tool-using LLMs for truly novel domains and for grounding causal claims in evidence; study humanâ€“LLM collaboration interfaces for iterative graph building and assumption auditing with traceable provenance and bias controls.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Unlocking the Capabilities of Thought: A Reasoning Boundary Framework to Quantify and Optimize Chain-of-Thought",
      "full_text": "From explained variance of correlated components to PCA without orthogonality constraints Marie Chaventâˆ— Guy Chaventâ€  Abstract Block Principal Component Analysis (Block PCA) of a data matrix A, where loadings Z are determined by maximization of âˆ¥AZâˆ¥2 over unit norm orthogonal loadings, is difficult to use for the design of sparse PCA by â„“1 regularization, due to the difficulty of taking care of both the orthog- onality constraint on loadings and the non differentiable â„“1 penalty. Our objective in this paper is to relax the orthogonality constraint on loadings by introducing new objective functions expvar(Y ) which measure the part of the variance of the data matrix A explained by correlated components Y = AZ. So we propose first a comprehensive study of mathematical and numerical properties of expvar( Y ) for two existing definitions Zou et al. [2006], Shen and Huang [2008] and four new definitions. Then we show that only two of these explained variance are fit to use as objective function in block PCA formulations for A rid of orthogonality constraints. Keywords: PCA, sparsity, dimension reduction, explained variance, orthogo- nality constraints, block optimization. âˆ—UniversitÂ´ e de Bordeaux, CNRS, INRIA, Bordeaux INP, IMB, UMR 5251, Talence, France e-mail : marie.chavent@u-bordeaux.fr (corresponding author) â€ Retired, collaborator to the SERENA Project Team, INRIA-Paris 2 rue Simone Iff, 75589 Paris, France arXiv:2402.04692v1  [stat.ML]  7 Feb 20241 Introduction Many linear data analysis methods construct new variables that â€œbestâ€ summa- rize the columns of a n Ã— p data matrix A where n observations are described by p numerical variables. The m < pnew variables are the columns of Y = AZ where Z is the p Ã— m loading matrix. These new variables are for instance principal components in PCA (Principal Component Analysis), canonical com- ponents in CA (Canonical Analysis) or PLS components in PLS (Partial Least Squares) regression. When the components are orthogonal (which is the case for PCA), it is usual to assess the quality of the components by measuring the part of the variance of A explained by Y = AZ with : expvar(AZ) = âˆ¥AZâˆ¥2 F , where the subscript F denotes the Frobenius norm. But this definition fails when the components are correlated. Correlated components appear when sparsity is introduced into the loading matrix Z to select the important original variables. The loading vectors and the components are no longer necessarily orthogonal. Two definitions have already been pro- posed to measure the explained variance of correlated components : theadjusted variance of Zou et al. [2006] and the total variance of Shen and Huang [2008]. Because there is no single definition, we introduce a set of three conditions to be satisfied by any explained variance definition and we propose a comprehensive study of mathematical and numerical properties of these two existing definitions together with four new definitions. We prove first that the total variance of Shen and Huang [2008] called sub- space explained variance hereafter, satisfies only two of three conditions. This lead us to propose other definitions by projection or by normalization on a set X = [x1, ..., xm] of orthonormal vectors which â€œpoints in the same directionâ€ as the components Y = [ y1, ..., ym]. Such orthonormal vectors X can be as- sociated to Y by three different rules: QR or UP (polar) decomposition of Y , or maximization of the explained variance. This leads to five definitions ( two normalized and three projected explained variance) : - the QR and the UP normalized explained variances (QRnormVar and UPnormVar), - the QR projected explained variance (QRprojVar) which is the adjusted variance of Zou et al. [2006], - the UP projected explained variance (UPprojVar), - the optimal projected explained variance (optprojVar). We prove that the five above definitions satisfy also the two first compatibility conditions, but that only the three projected variances, satisfy the third one, and hence are proper explained variances. 1Then we investigate both theoretically and numerically the size of the six explained variances, the existence of order between them, and their ability to rank in the same order - or not - different sets of components Y , which is for what they have been introduced at the origin. Finally, we study the ability of the three proper definitions of explained variance to provide a substitute to the classical Block PCA formulation : maximizing âˆ¥AZâˆ¥2 F under the orthonormality constraint ZT Z = Im, (1) by replacing (1) by a Explained Variance block PCA formulation : max âˆ¥zjâˆ¥=1,j=1...m expvar(AZ) , (2) which is rid of orthogonality constraints on loadings Z = [z1 . . . zm], and hence particularly suited as starting point for the design of sparse PCA algorithms.. As a conclusion of this study, we single out the optimal projected explained variance of Y = AZ : expvar(Y ) = max XT X=Im X j=1...m âŸ¨yj , xjâŸ©2 , (3) which possesses, after introduction of weights, the desired properties in the sense that it is easy to compute and differentiate, and admits as unique maximizer the SVD solution of PCA Zâˆ— = [v1 . . . vm] made of the m first right singular vectors of A. This will be the starting point for the design of an efficient group-sparse block PCA algorithm in the companion paper [Chavent and Chavent, 2023]. The paper is organized as follows: In Section 2 we motivate the need for an explained variance definition, and establish necessary conditions to be satisfied by any definition of the variance explained by a set of non-necessarily orthogonal components. Section 3 establishes the properties of the subspace explained variance (total variance) of Shen and Huang [2008]. Section 4 is devoted to the definition of three projected (including the adjusted variance of Zou et al. [2006] and the optimal variance (3)) and two normalized explained variances, together with the study of their mathematical properties. Sections 5 and 5.3 present numerical comparisons of the size of the variance explained by the six definitions, and of their ability to rank in the same order (or not) non necessarily orthogonal components. Finally, Section 6 compares the ability of the three projected variance definitions to discriminate the singular value solution of PCA as their unique maximizer, which makes them fit to use as Block PCA objective function. It should be noted that we have implemented the six explained variance definitions in R [R Core Team, 2021] in the package sparsePCA available at https://github.com/chavent/sparsePCA. 22 Defining variance explained by components when loadings are non orthogonal We set us from now on in the context of Principal Component Analysis (PCA), where one seeks a small number m of uncorrelated components yj = Azj, j= 1 . . . mby combining the p columns of a data matrix A, each containing n samples of a centered variable, with m unit norm loading vectors zj, j= 1 . . . m, in such a way that the components yj retain the largest possible part of the total variance of A. We denote by expvar(Y ) = the part of the variance of A explained by Y = AZ (4) and by âˆ¥Aâˆ¥2 F = X i,j=1...m a2 i,j the (total) variance of A, where âˆ¥ â€¢ âˆ¥F is the Frobenius norm of A . At this point, expvar(Y ) is a still loosely defined quantity outside of the solution of PCA given by : Z = Vm, Y = Um Î£m = AVm , (5) where Um and Vm contain them first left and right singular vectors and Î£m is the diagonal matrix of them first singular values of the singular value decomposition (SVD) of A : A = UÎ£V T with UT U = Ir , V T V = Ir , Î£ = diag(Ïƒ1, . . . , Ïƒr) = r Ã— r matrix with Ïƒ1 â‰¥ Ïƒ2 â‰¥ Â·Â·Â· â‰¥Ïƒr > 0 , where r is the rank of A, and the columns u1 . . . ur of U and v1 . . . vr of V are the left and right singular vectors of A. The principal components yj = Azj = Ïƒjuj are orthogonal, and hence un- correlated, so the sum of their variance âˆ¥yjâˆ¥2 represents the part of the total variance âˆ¥Aâˆ¥2 F of A explained by these m principal components. So we see that the variance explained by the principal components Y is : expvar(Y ) = âˆ¥Y âˆ¥2 F = X j=1...m âˆ¥yjâˆ¥2 = X j=1...m Ïƒ2 j â‰¤ X j=1...r Ïƒ2 j = âˆ¥Aâˆ¥2 F . (6) However, when sparsity constraints are introduced into loading vectors like in sparse PCA for instance, non orthogonal loadings and components are gener- ated and one has to face the problem of defining expvar( Y ) for possibly non orthogonal components. Alas, formula (6) for expvar( Y ) is strictly limited to the case of orthogonal components and loadings, as we see now. Consider first the case of non orthogonal components : take for example for Z an orthonormal basis of span {Vm} but different from Vm. Then components Y = AZ are not anymore orthogonal, and hence correlated, so the sum of their variances (the total variance of Y ) is too optimistic, and one expects that expvar(Y ) < âˆ¥Y âˆ¥2 F , (7) 3which shows that âˆ¥Y âˆ¥2 F is not a satisfying definition of expvar( Y ) in this case. Then take for example a data matrix A with three singular values 3, 2, 1 and hence a total variance of 14. Then chose for Z two linearly independant unit vectors close to the first right singular vector v1. Then : âˆ¥AZâˆ¥2 F = âˆ¥Y âˆ¥2 F = âˆ¥y1âˆ¥2+âˆ¥y2âˆ¥2 = âˆ¥Az1âˆ¥2 | {z } â‰ƒÏƒ2 1=9 + âˆ¥Az2âˆ¥2 | {z } â‰ƒÏƒ2 1=9 â‰ƒ 18 > 9 + 4 + 1| {z } Ïƒ2 1+Ïƒ2 2+Ïƒ2 3 = 14 = âˆ¥Aâˆ¥2 F , which violates property expvar(Y ) â‰¤ âˆ¥Aâˆ¥2 F implied by (4), so once again âˆ¥Y âˆ¥2 F is not suited as a definition of expvar( Y ). A last motivation for the search of definitions of expvar( Y ) : rather than solving the classical Block PCA formulation (1), why not solve the explained variance Block PCA formulation (2) by maximizing expvar( Y ) under the sole unit norm constraint on the loadings Z ? Sparse block PCA formulations based on such explained variance objective function eliminate the difficulty caused by the orthogonality constraints on the loadings, and, by construction, rule directly the balance between sparsity and explained variance. So we address first in this paper the problem of defining the part of the variance of A explained by components Y = AZ, under the sole condition that : âˆ¥zjâˆ¥ = 1, j= 1 . . . m ,rank(Z) = rank(Y ) = m â‰¤ r . (8) In absence of a sound definition for the explained variance of correlated compo- nents, we define a set of hopefully reasonable necessary conditions to be satisfied by any such definition : â€¢ Condition 1: when Y, Zhappen to be the SVD solution of PCA given by (5), expvar(Y ) has to provide the exact value given by (6) : expvar(Y ) = X j=1...m Ïƒ2 j â‰¤ X j=1...r Ïƒ2 j = âˆ¥Aâˆ¥2 F for m = 1 . . . r .(9) â€¢ Condition 2: when Y, Zsatisfy only (8), the components are not orthog- onal anymore, and one expects that, because of the correlation between the components, the part of the variance of A explained by Y is smaller than that of the PCA solution : expvar(Y ) â‰¤ X j=1...m Ïƒ2 j . (10) â€¢ Condition 3: expvar(Y ) has to take into account the possible correlation of the components as expected in (7) : expvar(Y ) â‰¤ X j=1...m âˆ¥yjâˆ¥2 = âˆ¥Y âˆ¥2 F , with equality only if the components are orthogonal. 4Any explained variance definition expvar( AZ) which satisfies these conditions achieves its PCA maximum valueP j=1...r Ïƒ2 j (Condition 2) for Z = Vm (Condi- tion 1), and hence provides a block formulation for PCA without orthogonality constraints on loadings, which can be used to derive sparse PCA algorithms. This point os view will be developped in Section 6 below. We propose now definitions for the explained variance of the components Y = AZ associated to any matrix Z of m â‰¤ r linearly independant - but not necessarily orthogonal - unit norm loading vectors zj, j = 1, ..., m. These definitions will include those introduces by Shen and Huang [2008] (see section 3 below) and Zou et al. [2006] (see section 4.2). 3 Subspace explained variance We start here from a reformulation of the explained variance (6) of the principal components Y = AZ, based on the subspace spanned by Z = Vm. Let P Vm denotes the orthogonal projection on this subspace. Then P Z = PVm = VmV T m , so that : expvar(Y ) = âˆ¥Y âˆ¥2 F = âˆ¥A PZâˆ¥2 F = âˆ¥A PVmâˆ¥2 F = X j=1...m Ïƒ2 j . (11) When Z satisfies (8) only, we proceed by analogy with (11), and define the subspace explained variance of Y = AZ by : expvarsubsp (Y ) def = âˆ¥APZâˆ¥2 F = tr \b Y T Y (ZT Z)âˆ’1) \t , (12) where P Z = Z(ZT Z)âˆ’1ZT denotes the projection matrix on the subspace spanned by Z. This shows that subspace explained variance is the Rayleigh quotient associated to A, Z. Note that with this definition, where the explained variance depends only of the subspace spanned by [z1 . . . zm], the normalization of the loadings zj is not necessary. Of course, we will still continue to represent loadings by unit norm vectors - but this is here only a convenience. The subspace explained variance coincides with the total explained variance introduced by Shen and Huang [2008, section 2.3 p. 1021], which they proved was increasing with the number of loadings, and bounded by the variance âˆ¥Aâˆ¥2 F of the data. The next lemma gives a complete picture of its properties : Lemma 3.1. (Subspace explained Variance) Let Z satisfy (8). Then the sub- space explained variance defined by (12) satisfies : expvarsubsp (Y ) =Ïƒ2 1+Â·Â·Â· +Ïƒ2 m = Max â‡” span{Z} = span{Vm} . (13) and hence satisfies conditions 1 and 2. However, it does not satisfy condition 3 : - when the components Y = AZ happen to be orthogonal : expvarsubsp (Y ) â‰¥ âˆ¥Y âˆ¥2 F with equality iff Z perm = Vm , (14) 5where â€œ perm = â€ denotes the equality of matrices up to a column permutation and multiplicatipn by Â±1, - when the loadings Z happen to be orthogonal : expvarsubsp (Y ) = âˆ¥Y âˆ¥2 F with non necessarily orthogonal components . (15) The proof is in Section 8.2 of the Appendix. This lemma shows that expvarsubsp (Y ) verifies only conditions 1 and 2, and overestimates the explained variance in two cases : - when components Y are orthogonal without pointing in the direction of the left singular vectors, inequality (14) implies expvar subsp (Y ) > âˆ¥Y âˆ¥2 F , which contradicts condition 3, - when loadings Z are orthogonal without pointing in the direction of the right singular vectors, then the components are correlated and (15) contra- dicts condition 3, which requires in that case that expvarsubsp (Y ) < âˆ¥Y âˆ¥2 F . We explore in the next section other directions in the hope of being able to comply with all conditions 1, 2 and 3. 4 Projected and normalized explained variances We start now from definition (6) of the explained variance in the case of PCA. A natural generalization would be : expvar(Y ) ? = X j=1...m âˆ¥yjâˆ¥2 = âˆ¥Y âˆ¥2 F = âˆ¥AZâˆ¥2 F , which, as we have seen in the Section 2, is not anymore an an acceptable defi- nition when components are correlated. However, this definition continues to make perfect sense for the variance explained by components as long as they are orthogonal, without pointing nec- essarily in the direction of left singular vectors. Hence a natural way to eliminate the redundancy caused by the orthogonality default of the components Y and to satisfy Condition 3 is to : 1. Step 1: choose a rule to associate to the components Y an orthonormal basis X of span{Y } that, loosely speaking, â€œpoints in the direction of the components Y â€, and, when the components Y happen to be orthogonal, points in the directions of Y itself. So the rule for the choice of the basis X associated to Y has to satisfy : XT X = Im , span{X} = span{Y } , < yj, xj > â‰¥ 0 âˆ€j = 1 . . . m , âŸ¨yj, ykâŸ© = 0 âˆ€j Ì¸= k =â‡’ xj = yj/âˆ¥yjâˆ¥ âˆ€j = 1 . . . m . (16) 6Examples of such rules are : QR decomposition : Y â‡ X = Q solution of Y = Q R , QT Q = Im where R is an upper triangular matrix , Polar decomposition : Y â‡ X = U solution of Y = U P , UT U = Im where PT = P âˆˆ I RmÃ—m , P â‰¥ 0 . (17) 2. Step 2: associate to Y orthogonal adjusted components Y â€² along the X axes, and define the variance explained by the components Y by : expvar(Y ) def = âˆ¥Y â€²âˆ¥2 F . Two ways for obtaining the adjusted components Y â€² are considered hereafter: by projection or by normalization as illustrated in Figure 1. y1 y2 yâ€²2 x1 x2 yâ€²1 E2 yâ€²1 yâ€²2 Figure 1: Illustration of projected and normalized explained variances : The ellipse E2 represents the image by A of all unit norm loadings. Let the X = [x1, x2] be the orthonormal vectors associated to the correlated components Y = [ y1, y2]. Then the extremities of the adjusted components Y â€² = [ yâ€² 1, yâ€² 2] obtained by projection are the two blue dots on the x1, x2 axes, and those obtained by normalization are the two green dots located at the intersection of the axes with E2. 4.1 Normalized explained variances We consider first in this section the case where the adjusted components Y â€² in step 2 are obtained by â€œnormalizationâ€ in the directions of the chosen orthonor- mal basis X of span{Y }. More precisely, the idea is to choose the abscissa of 7yâ€² j on the xj axis by requiring that yâ€² j is the image by A of some unit norm ad- justed loading zâ€² j. This is illustrated in Figure 1 : one associates to components Y = [y1, y2] the adjusted components Y â€² = [yâ€² 1, yâ€² 2] whose extremities are the points on the x1, x2 axes located on the ellipse image by A of the unit ball of the loading space. In order to determine the adjusted loadings zâ€² j, one computes first (non necessarily unit norm) loadings T = [t1, . . . , tm] by performing on the loadings zj the same linear combinations M : Y = XM , (18) that transformed Y into X, which leads to define T by : Z = T M . (19) Multiplying then by A left gives Y = AZ = AT Mand, comparison with (18) shows that X = AT and so xj = Atj, j= 1 . . . m. The adjusted components yâ€² j are then defined by : yâ€² j = Azâ€² j with zâ€² j = tj/âˆ¥tjâˆ¥ so that âˆ¥zâ€² jâˆ¥ = 1 , yâ€² j = xj/âˆ¥tjâˆ¥ , j = 1 . . . m , (20) and the normalized explained variance of Y estimated with X is defined by : expvarX norm(Y ) = âˆ¥Y â€²âˆ¥2 F = X j=1...m 1/âˆ¥tjâˆ¥2 , (21) where T = [t1, . . . , tm] is given by (19) and (18). Before specifying the rule of step 1 which associates an orthonormal basis X to the components Y , we give some properties of expvarX norm(Y ) which hold independently of the chosen basis X. Lemma 4.1. (Normalized explained variance) For any unit norm loadings Z and any basis X chosen according to the rule (16), the normalized explained variance of Y = AZ defined by(21) satisfies conditions 1 and 2 and : expvarX norm(Y ) â‰¤ expvarsubsp (Y ) â‰¤ Ïƒ2 1 + Â·Â·Â· + Ïƒ2 m . (22) Lemma 4.1 follows from Lemma 3.1 applied to the orthogonal components Y â€² : expvarX norm(Y ) def = âˆ¥Y â€²âˆ¥2 F â‰¤ expvarsubsp (Y â€²) â‰¤ Ïƒ2 1 + Â·Â·Â· + Ïƒ2 m , (23) which proves (22). We can now specify the rules for the choice of X to define two normalized explained variance satisfying conditions 1 and 2. QR normalized variance. Let X be defined by the QR-decomposition Y = XR of the components Y = AZ as recalled in (17) . Then (21) leads to another definition of variance : expvarQR norm(Y ) = X j=1...m 1/âˆ¥tjâˆ¥2 , Z = T R , Rupper triangular . (24) The normalized variance expvar QR norm(AZ) does not satisfy condition 3, as the counter example of Figure 2, left, shows. 8x1 y1 y2 yâ€²1 y2 y1 x1 x2x2 = E2 yâ€²2 yâ€²1 yâ€²2 Figure 2: Counterexamples for property 3. Left :Let X denote the basis asso- ciated to components Y by the QR-decomposition, and Y â€² be the correspond- ing normalized adjusted components. One sees that expvar QR norm(Y ) = âˆ¥Y â€²âˆ¥2 â‰¥ âˆ¥Y âˆ¥2, which violates property 3. Right :Let X denote the basis associated to components Y by the polar decomposition, and Y â€² be the corresponding nor- malized adjusted components. One sees that expvar QR norm(Y ) = âˆ¥Y â€²âˆ¥2 â‰¥ âˆ¥Y âˆ¥2, which violates property 3. UP normalized variance. Let X be defined by the UP-decomposition (polar decomposition) Y = XP of the components Y = AZ as recalled in (17). Then (21) defines another variance : expvarUP norm(Y )= X j=1...m 1/âˆ¥tjâˆ¥2 , Z = T P , P = (Y T Y )1/2 . (25) The normalized variance expvarUP norm(AZ) does not either satisfy condition 3, as the counter example of Figure 2, right, shows. So we explore in the next section another road in order to comply with all conditions 1,2 and 3. 4.2 Projected explained variances We consider in this section the case where the adjusted components Y â€² in step 2 are obtained by projection of the components Y on the chosen orthonormal basis X of span{Y }. The adjusted component yâ€² j is hence defined by : yâ€² j = âŸ¨yj , xjâŸ©xj , j = 1 . . . m . The so called projected explained varianceof Y estimated with X is then defined by : expvarX proj(Y ) def = âˆ¥Y â€²âˆ¥2 F = X j=1...m âŸ¨yj , xjâŸ©2 (26) 9Before specifying the rule of step 1 which associates an orthonormal basis X to the possibly correlated components Y , we give some properties of expvarX proj(Y ) which hold independently of the chosen basis X : Lemma 4.2.(Projected explained variances) For any unit norm loadings Z and any basis X chosen according to the rule (16), the projected explained variance of Y = AZdefined by (26) satisfies conditions 1, 2 and 3 and : expvarX proj(Y ) â‰¤ expvarsubsp (Y ) â‰¤ Ïƒ2 1 + Â·Â·Â· + Ïƒ2 m . (27) The proof is in Section 8.3 of the Appendix. We can now specify the rules for the selection of the orthonormal basis X, which give each, according to Lemma 4.2, a projected explained variance satisfying conditions 1 to 3. QR projected explained variance. We choose here to associate to the components Y , the orthonormal basis X = Q obtained by QR-decomposition of Y as recalled in (17).The vector x1 of the basis X = Q is chosen in the direction of the component with the larger norm, and the remaining components are projected on the orthogonal subspace to x1. Then x2 is determined by the same process applied in the orthogonal subspace, and so on. This reordering ensures that the basis X = Q associated to Y will point in the direction of Y primarily for the components of larger variance. The (order dependent) resulting QR projected explained variance is given by : expvarQR proj(Y ) = X j=1...m âŸ¨yj , xjâŸ©2 = X j=1...m r2 j,j. (28) It coincides with the adjusted variance introduced in Zou et al. [2006]. UP Projected explained variance. We choose now to associate to the components Y , the orthonormal basis X = U obtained by UP-decomposition (polar decomposition) of Y as recalled in (17). The basis X = U does its best to point in the same direction as the components Y , in that it maximizes the scalar product âŸ¨Y, XâŸ©F = P j=1...mâŸ¨xj, yjâŸ©. The (order independent) resulting UP projected explained variance is given by : expvarUP proj(Y ) = X j=1...m âŸ¨yj , xjâŸ©2 = X j=1...m p2 j,j . (29) Optimal projected explained variance. The idea here is to associate to the components Y , the basis X which gives the largest projected explained variance defined in (26). This choice satisfies obviously condition (16) and the so called optimal projected explained variance is defined by : expvaropt proj(Y ) = max XT X=Im X j=1...m âŸ¨yj , xjâŸ©2 , (30) 10The numerical computation of the optimal projected explained variance requires the maximization of the convex function X â‡ P j=1...mâŸ¨yj , xjâŸ©2 under the constraint XT X = Im. This can be done using the algorithm of JournÂ´ ee et al. [2010], which gives here : Xk+1 = polar \u0000 2 Y diag(XT k Y ) \u0001 , X 0 = U = polar(Y ) . (31) where 2Y diag(XT k Y ) is the gradient atXk of the functionX â‡ P j=1...mâŸ¨yj , xjâŸ©2. 5 Comparison of the explained variances In this section, the six variances explained by non orthogonal components (see Table 1), are compared theoretically and numerically. Name Notation Definition Short name subspace variance expvar subsp [12] subspVar QR normalized variance expvar QR norm [24] QRnormVar UP normalized variance expvar UP norm [25] UPnormVar QR projected variance expvar QR proj [28] QRprojVar UP projected variance expvar UP proj [29] UPprojVar optimal projected variance expvar opt proj [30] optprojVar Table 1: Summary of for the 6 variance definitions. 5.1 What we know We give first theoretical results on the relative magnitudes of the 6 explained variances : - The subspace explained variance is larger than any of the five other vari- ances (Lemmas 4.2 and 4.1). It is even larger than expected when com- ponents or loadings are orthogonal without being left and right singular vectors (see (14) and (15)). - The optimal projected variance is greater by definition than any other pro- jected variance, in particular greater than the QR and the UP projected variance. - There is no natural order between the QR and UP projected variances : when the components Y are of equal norm, the basis X which maximizes expvarX proj(Y ) is polar(Y ), which implies in particular that : expvaropt proj(Y ) = expvarUP proj(Y ) â‰¥ expvarQR proj(Y ) . (32) But the converse of the last inequality can hold when the norms of the components are very different : for m = 2, one checks that âˆ¥y2âˆ¥/âˆ¥y1âˆ¥ small enough implies that expvarUP proj(Y ) â‰¤ expvarQR proj(Y ). 11- There is no natural order between the QR and UP normalized variances : for components Y such that the basis X associated by QR-decomposition coincides with the m-first left singular vectors Um of A, one has, according to Lemma 4.1 : expvarQR norm(Y ) = X j=1...m Ïƒ2 j â‰¥ expvarUP norm(Y ) , with a strict inequality as soon as Y and Umdiag{Ïƒj} donâ€™t coincide. The same reasoning with the polar decomposition in place of the QR decomposition shows that the converse inequality can happen. - There is no natural order between the variances defined by projection and normalization, as illustrated in Figure 3. Figure 3: The two sets of components Y = [ y1, y2] and ËœY = [Ëœy1, Ëœy2] have been chosen such that their polar decomposition produces the same basis X = [x1, x2], and one sees that : expvarUP proj( ËœY ) â‰¤ expvarUP norm( ËœY ) = expvarUP norm(Y ) â‰¤ expvarUP proj(Y ). 5.2 What we see We compare now numerically the six explained variances. The comparison is made on non orthogonal components Y obtained by applying sparse PCA to simulated data matrices A. The matrices A are obtained using the simulation scheme of Chavent and Chavent [2023] based on m = 4 underlying loadings vectors of size p = 20 and m = 4 underlying first eigenvalues which are chosen to be either close or different. More precisely, 100 matrices A of size nÃ—20 were drawn randomly using either the â€œclose eigenvaluesâ€ or the â€œdifferent eigenval- uesâ€ scheme. Three sparse PCA algorithms [see Chavent and Chavent, 2023] 12were applied to each matrix A for a grid of 101 values of sparsity parameters Î» âˆˆ [0, 1]. Finaly, 30300 = 100 âˆ— 3 âˆ— 101 loadings matrices Z and components matrices Y = AZ where obtained for the â€œclose eigenvaluesâ€ and for the â€œdif- ferent eigenvaluesâ€ scheme. The variance explained by these components where performed using the six variance definitions to finally obtain the dimensionless proportion of explained variance (pev) defined by : pev = expvar(Y ) âˆ¥Aâˆ¥2 F â‰¤ Ïƒ2 1 + Â·Â·Â· + Ïƒ2 m âˆ¥Aâˆ¥2 F , where the right inequality follows from (10) in Condition 2, which is satisfied by all definitions. Figure 4 gives the mean pev for the 300 non orthogonal components (for the â€œclose eigenvaluesâ€ and the â€œdifferent eigenvaluesâ€ case) as a function of the sparsity parameter Î» and for each definition of explained variance. 0.0 0.1 0.2 0.3 0.4 0.5 0.80 0.85 0.90 0.95 Close eigenvalues lambda mean pev subspVar optprojVar/UPprojVar QRprojVar QRnormVar UPnormVar 0.0 0.1 0.2 0.3 0.4 0.5 0.70 0.75 0.80 0.85 0.90 0.95 Different eigenvalues lambda mean pev subspVar optprojVar UPprojVar QRprojVar QRnormVar UPnormVar Figure 4: Comparison of the mean pev (proportion of explained variance) over two sets of components (top and bottom) as function of sparsity parameter Î» for the six variance definitions. For the â€œclose eigenvaluesâ€ case (top), the six definitions give relatively close results and the results produced by optprojVar and UPprojVar are so close that 13they cannot be distinguished on the figure (remember that they would coincide were the norms equal, see (32)). One sees also that the results seem to be in a certain order for all Î» : subspVar â‰¥ optprojVar â‰¥ UPprojVar â‰¥ QRprojVar â‰¥ QRnormVar â‰¥ UPnormVar (the two first inequalities are not a surprise, as they hold theoretically). For the â€œdifferent eigenvaluesâ€ case (bottom), a zoom on the curves shows that subspVar and optprojVar are again larger than all other variances. But the previous apparent order between UPprojVar, QRprojVar and QRnormVar is not longer observed. The UPnormvar remains the smallest but its behavior seems disturbed as the sparsity paramter Î» increases. Table 2 shows that all definitions but UPnormVar (in the â€œdifferent eigen- valuesâ€ case) exhibit quite similar dispersions over the 300 realizations (for Î» = 0.3). Close eigenvalues Different eigenvalues subspVar 0.63 1.63 optprojVar 0.66 1.38 UPprojVar 0.66 1.06 QRprojVar 0.72 1.25 QRnormVar 1.06 1.33 UPnormVar 1.21 7.74 Table 2: Standard deviations Ã—100 of the six pev (proportion of explained vari- ance) obtained for Î» = 0.3 with the three algorithms over the two sets of com- ponents (close eigenvalues and different eigenvalues). 5.3 Ranking properties The proportions of explained variance are meant to be used for the ranking of algorithms, so it is important to figure out wether or not definitions i and j of explained variance will rank in the same order the components Y and Y â€² obtained from possibly different algorithms and/or sparsity parameter Î» and/or realization of the data matrix A. The components obtained with the 3 algorithms, the 50 smallest values Î» and 100 realizations of A (â€œdifferent eigenvalues â€ case) gave 15000 Ã— 14999/2 couples ( Y, Yâ€²) to be tested. Among these couples, we may consider as Ïµ-distinguishable from the point of view of our explained variances those for which |pevi(Y ) âˆ’ pevi(Y â€²)| â‰¥Ïµ for all i = 1 . . .6 for some Ïµ â‰¥ 0. Table 3 shows the percentage of cases where pevi and pevj rank identically components Y and Y â€² among all Ïµ-distinguishable couples. The good news is that all three projected variances optprojVar, UPpojVar and QRprojVar, as well as the normalized variance QRnormVar, produce the 14optprojVar UPprojVar QRprojVar QRnormVar UPnormVar subspVar 79.98 71.05 70.71 69.75 56.19 optprojVar 88.93 89.87 89.19 73.71 UPprojVar 96.22 95.27 84.62 QRprojVar 98.34 83.15 QRnormVar 82.75 UPnormVar optprojVar UPprojVar QRprojVar QRnormVar UPnormVar subspVar 86.13 83.78 84.70 84.57 68.68 optprojVar 96.40 98.57 98.32 81.14 UPprojVar 97.81 97.72 84.74 QRprojVar 99.66 82.55 QRnormVar 82.66 UPnormVar optprojVar UPprojVar QRprojVar QRnormVar UPnormVar subspVar 89.57 89.57 89.57 89.57 68.80 optprojVar 100.00 100.00 100.00 79.23 UPprojVar 100.00 100.00 79.23 QRprojVar 100.00 79.23 QRnormVar 79.23 UPnormVar Table 3: The entry of each table on line i and column j gives the percentage of Ïµ-distinguishable couples Y, Yâ€² which are ranked identically by pev i and pevj. Top : Ïµ = 0, middle : Ïµ = 10âˆ’3, bottom : Ïµ = 10âˆ’2. same ranking as soon as one considers that differences in proportion of explained variance under 10 âˆ’2 are not significative. For the same Ïµ, the two other def- initions subspVar and UPnormVar still produce quite different rankings. The numerical results of this section show that all investigated definitions rank the explained variance of components in essentially the same order, and hence can all be used to compare the variance explained by components obtained by different algorithms or with different parameters. Of course, this is only an experimental result based on our simulated data sets, that needs to be confirmed by further numerical tests. 6 Explained variance block PCA formulations With the objective in mind to get rid of the orthogonality constraints in the usual block PCA formulation (1), we discuss in this section the possibility of using the above defined explained variance measures as objective function in new block PCA formulations where the loadings are subject to unit norm constraints 15only : max âˆ¥zjâˆ¥ = 1, j= 1. . . m expvar(AZ) = Ïƒ2 1 + Â·Â·Â· + Ïƒ2 m , (33) As seen in Lemmas 3.1, 4.1 and 4.2, property (33) holds for the subspace variance and the five normalized and projected variance definitions. However, from these six definitions, only the three projected explained variances satisfy (Lemma 4.2) all necessary conditions 1, 2 and 3 of section 2 required for a proper definition of explained variance. So we shall limit our search for an explained variance block formulation to these three definitions. A projected explained variance expvar proj(AZ) will provide a block PCA formulation (33) if and only if its sole maximizers are the loadings Z made of the m first right singular vectors Vm in any order, up to a Â±1 multiplication, or in short : Z perm = Vm are the sole maximizers of expvar( AZ) . (34) We denote also by Am def = UmÎ£mV T m with Î£ m def = diag {Ïƒ1 . . . Ïƒm} (35) the restriction of the matrix A to the subspace of the m first right singular vectors V1 . . . Vm. It is a bijection from the right to the left singular subspaces associated to the m â‰¤ r first singular values. In the singular basis Um, Vm, Am reduces to Î£m. The next lemma characterizes the maximizers of any projected variance : Lemma 6.1. Maximizers of projected variances 1. For a given set of components Y , the basis X which gives the largest projected explained variance expvarX proj(Y ) is necessarily a solution of : Y diag(XT Y ) = XP with PT = P and P â‰¥ 0 (36) 2. The projected explained variance expvarX proj(AZ) achieves it PCA maxi- mum value Ïƒ2 1 +Â·Â·Â· +Ïƒ2 m if and only if Z and X satisfy the three following conditions : span{Z} = span{Vm} (37) the loadings Z are (AT mAm)âˆ’1â€“ orthogonal (38) the basis X associated to Y = AZ is X = [n1 . . . nm] . (39) where n1 . . . nm denote unit normals at y1 . . . ym to the ellipsoid Em of span{Um} image by Am of the unit sphere of span{Vm} (see Figure 5) 16y1 n1 yâ€²2 n2n1n2 y2 yâ€²1 R2=Ïƒ21+Ïƒ22 Figure 5: Illustration of condition (39) for m = 2. The ellipse E2 is the image by A of all unit norm loadings z of the right singular space V2, with half axes Ïƒ1 and Ïƒ2. Let y1, y2 be two (possibly correlated) components such that the normals n1, n2 to E2 at y1, y2 are orthogonal, the tangents are orthogonal too and hence meet on the Cartan Circle of radius R = (Ïƒ2 1 + Ïƒ2 2)1/2. Then for the choice X = (n1, n2) one has expvarX projY = âˆ¥yâ€² 1âˆ¥2 + âˆ¥yâ€² 2âˆ¥2 = R2 = Ïƒ2 1 + Ïƒ2 2. The proof is in Appendix 8.4 Condition (38) reduces, in the singular basis Vm, to Î£ âˆ’2 m â€“orthogonality of the loadings. It ensures that the normals nj are orthogonal, and hence can be chosen as the basis X associated to Y in (39). Of course, the SVD solutions Z perm = Vm of PCA satisfy always (37) (38) (39). Once a rule Y â‡ X has been specified, points 1 and 2 of the lemma will make it possible to determine wether or not (34) is satisfied. 6.1 Maximizers of expvarQR proj(Y ) The projected explained variance expvar QR proj(AZ) attains its maximum Ïƒ2 1 + Â·Â·Â· + Ïƒ2 m if and only if : Z perm = Vm , (40) When the maximum norm selection procedure is applied at each step and the components renumbered accordingly, the unique maximizer is Z = Vm. Proof: The â€œifâ€ part of (40) is trivial, we prove the â€œonly ifâ€ part : let Y be such that expvarQR projY = Ïƒ2 1 +Â·Â·Â· +Ïƒ2 m, and X be given by its QR decomposition 17Y = XR with R upper triangular. By hypothesis, X maximizes expvarX Y , and point 1 of Lemmas 6.1 implies that Y diag(XT Y ) = XP = XRdiag(XT Y ), with P = Rdiag(XT Y ) symmetric positive. This implies that R is a diagonal matrix, so that all xj point in the direction of yj. But property 2 of Lemma 6.1 shows that the xjâ€™s are also normal to the ellipsoid Em at yj, which can happen only if yj = Azj coincides with its principal axes and hence each zj is one of the m first right eigenvectors vj. Proposition 6.2. The QR projected explained variance expvarQR proj(AZ) (the adjusted variance of Zou et al. [2006]) associated to unit norm loadings Z pro- vides a block PCA formulation : max âˆ¥zjâˆ¥ = 1, j= 1. . . m expvarQR proj(AZ) = Ïƒ2 1 + Â·Â·Â· + Ïƒ2 m , (41) which admits the SVD solution (40) as â€œuniqueâ€ maximizer. Numerical implementation of this formulation requires the computation of the gradient of the Z â‡ expvarQR proj(AZ) function, which is defined through the QR decomposition of Y = AZ. This can be done by the adjoint state method Chavent [2010], which is feasible but may be cumbersome. The block PCA formulation (41) can be used as starting point for the design of sparse PCA algorithms, keeping in mind that enforcing sparsity by subtracting theâ„“1 norm of loadings leads to a difficult, though tractable, non smooth optimization problem. 6.2 Maximizers of expvarUP proj(Y ) The projected explained variance expvar UP proj(AZ) attains its maximum Ïƒ2 1 + Â·Â·Â· + Ïƒ2 m if and only if : Z perm = Vm (SVD solution) or Z = Z# (42) where the â€œparasiticâ€ solution Z# is such that the components Y # = AZ# satisfy âŸ¨y# 1 , x1âŸ© = Â·Â·Â· = âŸ¨y# m, xmâŸ©, with the hyperplanes tangent to Em at y# j delimiting an m-dimensional orthant of span {Vm}. For m = 2, the parasitic solution is illustrated on figure 6, where one sees that the components y# j (in red) corresponding to the choice of different principal axes for the intersection of the tangents coincide up to a multiplication by Â±1; the SVD components are in blue. The proof is in Appendix 8.5. So expvarUP proj(AZ) cannot be used for the construction of a block PCA for- mulation like (33), as the optimization algorithm might converge to the parasitic solution Z# ! 18x1 x2 E2 u1 yâ€²1# yâ€²2# u2 R2=Ïƒ21+Ïƒ22 y#2 yâˆ—1 y#1 yâˆ—2 âˆ’y#2 âˆ’y#1 Figure 6: Illustration, for m = 2, of the parasitic maximizer Y # of expvarUP proj. The polar decomposition associates to components Y # = y# 1 , y# 2 orthonormal vectors X = x1, x2 such that âˆ¥yâ€² 1 #âˆ¥ = âˆ¥yâ€² 2 #âˆ¥. The red points on the Cartan circle correspond to the parasitic maximizers Y # = Â±y# 1 , Â±y# 2 , the blue points to the SVD solution Y âˆ— = Â±yâˆ— 1, Â±yâˆ— 2 = Â±Ïƒ1u1, Â±Ïƒ2u2 6.3 Maximizers of expvaropt proj(Y ) According to point 2 of Lemma 6.1, the optimal projected explained variance expvaropt proj(AZ) attains its maximum Ïƒ2 1 + Â·Â·Â· + Ïƒ2 m if and only if : span{Z} = span{Vm} and the loadings Z are (AT mAm)âˆ’1â€“ orthogonal . (43) This situation is similar to the maximization (1) ofâˆ¥AZâˆ¥2 under orthonormality constraints ZT Z = Im, were the maximum is attained for all orthonormal Z which span Vm. But the difference is that the orthogonality condition (43)-right is not a constraint for the maximization of expvar opt proj(AZ), it just happens to be satisfied by the maximizer ! 196.4 Weighted optimal projected explained variance In order to select the SVD solutionZ = Vm among the maximizers of expvaropt proj(AZ), we introduce weights Âµj such that : Âµ1 â‰¥ Âµ2 â‰¥ . . . Âµm > 0 , (44) and define a weighted optimal projected variance by : expvaropt proj,Âµ(AZ) = max XT X=Im X j=1...m Âµ2 jâŸ¨Azj , xjâŸ©2 , which coincides with expvar opt proj(AZ) when Âµj = 1 for all j. This leads to the weighted optimal projected explained variance block PCA formulation : max âˆ¥zjâˆ¥ = 1 j = 1. . . m expvaropt proj,Âµ(AZ) = max âˆ¥zjâˆ¥ = 1 j = 1. . . m max XT X=Im X j=1...m Âµ2 jâŸ¨Azj , xjâŸ©2 = X j=1...m Âµ2 jÏƒ2 j . (45) The nice properties of this formulation are recalled in the next proposition : Proposition 6.3. Let the singular values of A satisfy : Ïƒ1 > Ïƒ2 > Â·Â·Â· > Ïƒm > 0 , and the weights Âµj satisfy (44). Then the PCA loadings Z = Vm and nor- malized components X = Um defined in (5) are one solution of the block PCA formulation (45) when the weights Âµj are constant, and its unique solution (up to a multiplication by Â±1 of each column of course) when the weights Âµj are strictly decreasing, in which case the maximizers Zâˆ— and Xâˆ— are independent of the weights Âµj. The (unweighted) optimal projected variance explained by Y âˆ— = AZâˆ— is : expvaropt proj (Y âˆ—) = Ïƒ2 1 + Â·Â·Â· + Ïƒ2 m â‰¤ âˆ¥Aâˆ¥2 F . Proof: Exchanging the order of maximization in the center term of (45) solving analytically the maximization with respect to Z gives : max âˆ¥zjâˆ¥ = 1 j = 1. . . m expvaropt proj,Âµ(AZ) = max XT X=Im X j=1...m Âµ2 jâˆ¥AT xjâˆ¥2 . (46) The last term in resp. (46) is the maximization of a weighted Rayleigh quotient for AT , which is known to be equivalent to a PCA problem forAT , and hence for A (see for example Absil et al. [2008], recalled as Theorem 8.1 in the Appendix for the case of constant weights, and Brockett [1991] for the case of decreasing weights). Of course, formulation (45) is of little interest for PCA itself, as there exists plenty of other efficient solution methods. But it will provide the starting point for the design of a sparse PCA algorithm, to be developped in the companion paper Chavent and Chavent [2023]. 207 Conclusion We have investigated the problem of defining the part of the variance of a data matrix explained by correlated components, such as those which arise when sparse loadings are searched for. We have established three compatibility conditions to be satisfied by any such explained variance definition in order to be compatible with the Principal Component Analysis (Condition 1), and to ensure a loss in explained variance when the components are correlated (Conditions 2 and 3). We have proved that the two existing and the four new definitions : - Subspace (total variance of Shen and Huang [2008]), - QR normalized, - UP or Polar normalized, - QR projected (adjusted variance of Zou et al. [2006]), - UP or Polar projected, - Optimal projected all satisfy the two first compatibility conditions, but that only the three pro- jected explained variance satisfy also the third one and provide proper explained variance definitions. Numerical experiments have shown that the choice of a specific definition for the ranking of correlated components by explained variance is not critical. But we have shown that only the QR and the (weighted) optimal projected explained variance definitions admit the SVD solution as unique maximizer, and hence provide new explained variance block PCA formulations rid of orthogonality constraints on loadings. Their use for the construction of a group sparse PCA algorithm is the subject of a second paper Chavent and Chavent [2023]. 8 Appendix 8.1 Generalized Rayleigh quotient This is a classical result, see for example Absil et al. [2008] and Brockett [1991] : Theorem 8.1. Let the loadings Z satisfy : Z = [z1 . . . zm] âˆˆ I RpÃ—m , rank(Z) = m â‰¤ rank(A) def = r . Then the generalized Rayleigh quotient tr{(ZT AT AZ)(ZT Z)âˆ’1} satisfies : tr{(ZT AT AZ)(ZT Z)âˆ’1} â‰¤Ïƒ2 1 + Â·Â·Â· + Ïƒ2 m â‰¤ âˆ¥Aâˆ¥2 F , and the left inequality becomes an equality if and only if : span{Z} = span{v1 . . . vm} , where v1, . . . vm are the m first right singular vectors of A. 218.2 Proof of Lemma 3.1 Definition (12) of the subspace explained variance and the properties of the Rayleigh quotient tr {ZT AT AZ(ZT Z)âˆ’1)} recalled in Theorem 8.1 show that (9) and (10), and hence Properties 1 and 2, hold as well as (13) and (15). It remains to prove (14) which shows that Condition 3 does not hold. So let Y = AZ be orthogonal components : âŸ¨yj, ykâŸ© = 0 , j, k= 1 . . . m, jÌ¸= k corresponding to unit norm loadings : âˆ¥zjâˆ¥ = 1 j = 1 . . . m , and define X, Tby : xj = yj/âˆ¥yjâˆ¥ , t j = zj/âˆ¥yjâˆ¥ , j = 1 . . . m , so that : XT X = Im . Then on one side one has : âˆ¥Y âˆ¥2 F = X j=1...m âˆ¥yjâˆ¥2 = X j=1...m 1/âˆ¥tjâˆ¥2 = tr{diagâˆ’1(TT T)} , (47) and on the other side, as Y and X span the same subspace : expvarsubsp (Y ) = expvarsubsp (X) = tr{(XT X)(TT T)âˆ’1} = tr{(TT T)âˆ’1} (48) Formula (14) will be proved if we show that : tr{diagâˆ’1(TT T)} â‰¤tr{(TT T)âˆ’1} . (49) We use for that an idea taken from Miller [1969], and perform a QR-decomposition of T. By construction, the diagonal elements of R satisfy : 0 < ri,i â‰¤ âˆ¥tiâˆ¥ . Then : TT T = RT QT Q R= RT R , (TT T)âˆ’1 = Râˆ’1(RT )âˆ’1 = Râˆ’1(Râˆ’1)T , where Râˆ’1 satisfies : Râˆ’1 = upper triangular matrix , [Râˆ’1]i,i = 1/ri,i . Hence the diagonal element of ( TT T)âˆ’1 are given by : : \u0002 (TT T)âˆ’1\u0003 i,i = \u0002 Râˆ’1(Râˆ’1)T \u0003 i,i = [ Râˆ’1]2 i,i + X j>i [Râˆ’1]2 i,j â‰¥ [Râˆ’1]2 i,i = 1/r2 i,i â‰¥ 1/âˆ¥tiâˆ¥2 . (50) 22which gives (49) by summation over i = 1 . . . m, and (14) is proved. We suppose now that the orthogonal components yj, j= 1 . . . msatisfy âˆ¥Y âˆ¥2 F = expvarsubsp (Y ). Then (47) (48) imply that equality holds in (49) and hence all inequality in (50) are equalities : 1. first inequality : [ Râˆ’1]2 i,j = 0 for all j > i â‡’ Râˆ’1 and hence R are diagonal 2. second inequality : 1 /r2 i,i = 1/âˆ¥tiâˆ¥2 â‡’ R is diagonal But R diagonal implies that the tj - and hence also the loadings zj - are orthog- onal, which together with the hypothesis of orthogonal components yj, implies that (yj/âˆ¥yjâˆ¥, zj) are pairs of singular vectors of A, which proves that Z = Vm and ends the proof of (14). 8.3 Proof of Lemma 4.2 By construction, expvarX proj(AZ) satisfies clearly conditions 1 and 3 of Section 4. We prove now that it satisfies moreover (27), and hence also condition 2. Let EX be the ellipsoid of span(X) = span(Y ) image by A of the unit sphere of span(Z). By construction one has : yj âˆˆ EX , j = 1 . . . m , and the modified components Y â€² defined by projection satisfy, c.f. (26) : âˆ¥yâ€² jâˆ¥ = |âŸ¨yj, xjâŸ©| â‰¤Î½j def = max y âˆˆ EX âŸ¨y, xjâŸ© , j = 1 . . . m , (51) so that : expvarX projY def = âˆ¥Y â€²âˆ¥2 F â‰¤ Î½2 1 + Â·Â·Â· + Î½2 m . (52) We can now â€œboxâ€ the ellipsoid EX in the parallelotope PX of span{X} defined by : PX = \b y âˆˆ span{X} | âˆ’Î½j â‰¤ âŸ¨y, xjâŸ© â‰¤+Î½j , j = 1 . . . m \t , (see figure 7). By construction, one can draw from each of the 2 m vertices of PX m orthogonal hyperplanes tangent to the ellipsoid EX, which implies that they are all on the orthoptic or Cartan sphere of the ellipsoid, whose radius is known to be the sum of the squares of the half principal axes ÏƒX j , j= 1 . . . mof EX (see for example the textbook Tauvel [2000]). Hence : Î½2 1 + Â·Â·Â· + Î½2 m = (ÏƒX 1 )2 + Â·Â·Â· + (ÏƒX m)2 . (53) Let then yX 1 . . . yX m be vectors whose extremity are points of EX located on its principal axes, so that : âˆ¥yX j âˆ¥ = ÏƒX j , j = 1 . . . m ,âŸ¨yX i , yX j âŸ© = 0 , i, j= 1 . . . m, iÌ¸= j . Property (14) of Lemma 3.1 applied to the orthogonal components Y = Y X gives: : (ÏƒX 1 )2 + Â·Â·Â· + (ÏƒX m)2 = âˆ¥Y Xâˆ¥2 â‰¤ expvarsubsp Y X â‰¤ Ïƒ2 1 + Â·Â·Â· + Ïƒ2 m . (54) Combining inequalities (52) (53) (54) proves the inlem 4-2equality (27). 23Figure 7: Illustration of the upper bound to âˆ¥Y â€²âˆ¥2 F in span {Y } when Y â€² is defined by projection. 8.4 Proof of Lemma 6.1 (notations of Section 8.3) We prove first point 1 of the Lemma. Maximization of the convex function X â‡ expvarprojY X under the constraint XT X = Im by algorithm (31) and passing to the limit proves (36). We prove now the â€œonly ifâ€ part of point 2. Let X be given such that expvarX projY = Ïƒ2 1 + Â·Â·Â· + Ïƒ2 m. Then necessarily : â€¢ equality holds in (54), and property (13) of subspace variance implies that the loadings Z span the subspace Vm of the m first right singular vectors, which proves (37). â€¢ equality holds in (51), which implies that for j Ì¸= k the normals to EX at yj and yk are orthogonal (see Figure 7). The restriction Am of A to span{Vm} is an isomorphism from span {Vm} to span{Um}, hence : EX = {y âˆˆ span{Um} | âˆ¥Aâˆ’1 m yâˆ¥2 = 1} . A normal n(y) to EX at y is then : n(y) = 1 2âˆ‡y \u0000 âˆ¥Aâˆ’1 m yâˆ¥2 âˆ’ 1 \u0001 = (Aâˆ’1 m )T Aâˆ’1 m y = (Aâˆ’1 m )T z , 24and the orthogonality of n(yj) and n(yk) shows that : âŸ¨n(yj), n(yk)âŸ© = âŸ¨(Aâˆ’1 m )T z1, (Aâˆ’1 m )T z2âŸ© = âŸ¨z1, (AT mAm)âˆ’1z2âŸ© = 0 , (55) which proves (38). When vectors and matrices are written on the singular bases Um and Vm, one has ( AT mAm)âˆ’1 = diag{ 1 Ïƒ2 1 . . .1 Ïƒ2m }. We finally prove the â€œifâ€ part of point 2. So let (37) (38) (39) hold. Property (37) implies that the half axes of EX are Ïƒ1 . . . Ïƒm, and (38) that the normal nj to EX at yj, j= 1 . . . mare orthogonal. So one can box EX with a parallelotope PX with axes parallel to the normals nj, and define X as the orthonormal basis along its axes. Then the same reasonning as above for the proof of (27) shows that expvarprojY = Ïƒ2 1 + Â·Â·Â· + Ïƒ2 m, which ends the proof of the lemma. 8.5 Proof of Property (42) Let the loadings Z be such that the UP-projected explained variance of the components Y = AZ satisfies expvarUP proj(Y ) = Ïƒ2 1 +Â·Â·Â· +Ïƒ2 m. Then by definition of expvarUP proj one has : expvarUP proj(Y ) = expvarX proj(Y ) with Y = UP, UT U = Im , P = PT , Pâ‰¥ 0 (56) But expvarX proj(Y ) = Ïƒ2 1 + Â·Â·Â· + Ïƒ2 m, and point 1 of Lemma 6.1 implies the existence of Pâ€² such that : Y diag(XT Y ) = XP â€² with Pâ€²T = Pâ€² and Pâ€² â‰¥ 0 (57) Comparison of the two last properties shows that : Y = UP = XP â€²diag(XT Y )âˆ’1 , (58) and uniqueness of the polar decomposition implies that Pâ€²diag(XT Y )âˆ’1 = P and hence is symmetrical, which can happen in only two cases : 1. either diag( XT Y ) = Î»Im for some Î», which gives the parasitic solution Z = Z#, 2. or Pâ€² - and hence also P itself - is diagonal, which implies that the com- ponents Y = UP are orthogonal. But point 2 of Lemma 6.1 implies that they are also ( AT mAm)âˆ’1â€“ orthogonal, which is possible only if the com- ponents Y are proportional to the left singular basis Um, which gives the SVD solution Z perm = Vm. References Pierre-Antoine Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization Algorithms on Matrix Manifolds , volume 78. 12 2008. ISBN 978-0-691-13298- 3. doi: 10.1515/9781400830244. 25RW Brockett. Dynamical systems that sort lists, diagonalize matrices, and solve linear programming problems. Linear algebra and its applications, 146:79â€“91, 1991. Guy Chavent. Nonlinear Least Squares for Inverse Problems: Theoretical Foun- dations and Step-by-Step Guide for Applications . 01 2010. ISBN 978-90-481- 2784-9. doi: 10.1007/978-90-481-2785-6. Marie Chavent and Guy Chavent. A group sparse explained variance block pca. Submitted, 2023. Michel JournÂ´ ee, Yurii Nesterov, Peter RichtÂ´ arik, and Rodolphe Sepulchre. Gen- eralized power method for sparse principal component analysis. Journal of Machine Learning Research, 11(Feb):517â€“553, 2010. G Miller. Closed-form inversion of the gram matrix arising in certain least- squares problems. IEEE Transactions on Circuit Theory, 16(2):237â€“240, 1969. R Core Team. R: A Language and Environment for Statistical Computing . R Foundation for Statistical Computing, Vienna, Austria, 2021. URL https: //www.R-project.org/. Haipeng Shen and Jianhua Z Huang. Sparse principal component analysis via regularized low rank matrix approximation. Journal of multivariate analysis , 99(6):1015â€“1034, 2008. Patrice Tauvel. Cours de gÂ´ eomÂ´ etrie: agrÂ´ egation de mathÂ´ ematiques. Dunod, Paris, 2000. Hui Zou, Trevor Hastie, and Robert Tibshirani. Sparse principal component analysis. Journal of computational and graphical statistics , 15(2):265â€“286, 2006. 26",
      "references": [
        "Optimization Algorithms on Matrix Manifolds",
        "Dynamical systems that sort lists, diagonalize matrices, and solve linear programming problems.",
        "Nonlinear Least Squares for Inverse Problems: Theoretical Foun- dations and Step-by-Step Guide for Applications",
        "A group sparse explained variance block pca.",
        "Generalized power method for sparse principal component analysis.",
        "Closed-form inversion of the gram matrix arising in certain least- squares problems.",
        "R: A Language and Environment for Statistical Computing",
        "Sparse principal component analysis via regularized low rank matrix approximation.",
        "Cours de gÂ´ eomÂ´ etrie: agrÂ´ egation de mathÂ´ ematiques.",
        "Sparse principal component analysis."
      ],
      "meta_data": {
        "arxiv_id": "2402.04692v1",
        "authors": [
          "Marie Chavent",
          "Guy Chavent"
        ],
        "published_date": "2024-02-07T09:32:32Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses how to quantify â€œexplained varianceâ€ when PCA components/loadings are correlated (e.g., in sparse PCA), where the classical âˆ¥AZâˆ¥_F^2 can overestimate variance and even exceed âˆ¥Aâˆ¥_F^2. Proposes three necessary compatibility conditions for any explained-variance definition (agreement with PCA/SVD in the orthogonal case; upper bound by PCA optimum; penalty for component correlation). Studies two existing definitionsâ€”Zou et al.â€™s adjusted variance (QR-projected) and Shen & Huangâ€™s total/subspace varianceâ€”and introduces four new ones (QR/UP normalized; UP-projected; optimal projected). Proves only the projected variances (QRprojVar, UPprojVar, optprojVar) satisfy all three conditions, while subspace and normalized variants violate the correlation-penalty condition. Uses these measures to build â€œBlock PCA without orthogonality constraintsâ€ objective formulations; shows QRprojVar and a weighted optprojVar admit the SVD solution as unique maximizer, enabling block formulations suitable for sparse PCA design.",
        "methodology": "Defines explained variance for correlated components Y=AZ by mapping Y to an orthonormal basis X spanning span(Y) that â€œalignsâ€ with components (via QR decomposition, polar/UP decomposition, or by directly maximizing projected variance). Constructs (i) normalized variances by re-scaling along X using induced loadings T so adjusted components remain images of unit-norm loadings; (ii) projected variances by projecting each y_j onto x_j and summing squared projections. Establishes theoretical properties using SVD geometry, generalized Rayleigh quotient results, ellipsoid/Cartan sphere arguments, and inequalities comparing definitions. Computes the optimal projected variance via a Stiefel-manifold iteration (generalized power method): X_{k+1}=polar(2Y diag(X_k^T Y)). Introduces weighted optimal projected variance with decreasing weights to break symmetries and guarantee unique SVD maximizer; shows equivalence to weighted Rayleigh quotient maximization for A^T.",
        "experimental_setup": "Numerical evaluation on simulated centered data matrices A (nÃ—p with p=20) generated with m=4 underlying loadings and two eigenvalue regimes (â€œcloseâ€ vs â€œdifferentâ€ leading eigenvalues). For each regime: 100 random matrices; apply 3 sparse PCA algorithms (from the companion work) across a grid of 101 sparsity parameters Î»âˆˆ[0,1], yielding 30,300 sets of (Z,Y). For each Y, compute six explained-variance measures and report proportion explained variance pev=expvar(Y)/âˆ¥Aâˆ¥_F^2. Compare mean pev vs Î», standard deviations at Î»=0.3, and ranking stability: evaluate pairwise agreement of rankings between variance definitions over many component-pairs, for distinguishability thresholds Îµâˆˆ{0,10^-3,10^-2}.",
        "limitations": "Theoretical results assume unit-norm loadings and full column rank mâ‰¤rank(A); uniqueness claims for the weighted formulation require strictly ordered singular values and strictly decreasing weights. Some candidate measures are unsuitable as objectives: subspace variance and normalized variances violate the correlation-penalty condition; UP-projected variance can have parasitic maximizers (non-SVD). The optimal projected variance is not uniquely maximized without weights and requires iterative optimization on the Stiefel manifold; QRprojVar objective differentiation involves QR decomposition and may be cumbersome (adjoint-state approach suggested). Empirical comparisons are limited to small simulated problems (p=20, m=4) and to algorithms/datasets from the authorsâ€™ simulation scheme; ranking-consistency conclusions are experimental rather than guaranteed, and no real-data benchmarking or downstream task validation is provided.",
        "future_research_directions": "Develop and validate sparse/group-sparse block PCA algorithms built on the weighted optimal projected variance objective (as indicated in the companion paper), including efficient gradients, convergence guarantees, and large-scale implementations. Extend evaluation to real-world datasets, larger pâ‰«n regimes, and broader benchmarks; study statistical properties (consistency, bias/variance trade-offs) and robustness to noise, scaling, and missing data. Explore alternative penalties (elastic net, structured/group lasso, nonconvex sparsity) and automatic selection of weights Âµ_j or sparsity parameters via model selection criteria. Generalize the â€œexplained variance with correlated componentsâ€ framework to other component methods (PLS, CCA/CA), and investigate connections to manifold optimization and orthogonality-free formulations in other matrix factorization problems.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ],
  "research_hypothesis": {
    "open_problems": "Auto-CoTâ€™s main bottleneck is *demonstration fragility under surface-level distribution shift*: many self-generated demos are â€œlocally correctâ€ only for the exact phrasing that produced them. Because the generator and verifier are typically the same model (or the same prompt pattern), these demos can pass length filters and even self-verification while still encoding spurious heuristics (keywordâ†’operation shortcuts, mis-parsed quantities, or narrative fillers). Once placed in-context, such demos induce brittle solution patterns that fail on small, meaning-preserving changes (entity-name swaps, clause reorderings, punctuation/format changes) that frequently occur in real educational and user-generated settings.\n\nThe unmet gap is a *prompt-only, script-auditable, low-cost* demo-selection criterion that (i) does not rely on an LLM to generate the perturbations it is later evaluated on, (ii) reduces shared-bias verification failures by evaluating robustness across multiple automatically-generated variants, and (iii) yields demonstrations whose intermediate artifact is independently checkable (not merely a persuasive narrative).",
    "method": "Metamorphic Proof-Carrying Auto-CoT (MPC-AutoCoT).\n\nHuman inspiration: when people trust a worked example, they donâ€™t only check the final arithmeticâ€”they also â€œstress testâ€ the reasoning under trivial rewrites (re-reading, re-ordering details, changing names) that should not affect the solution. In software engineering, this is formalized as *metamorphic testing*: validate a system by asserting invariances under transformations where ground-truth labels are unchanged.\n\nMPC-AutoCoT keeps Auto-CoTâ€™s diversity clustering, but replaces heuristic acceptance with a **two-part proof + metamorphic robustness gate** that is fully prompt-only and Python-verifiable:\n\n1) **Proof-carrying structured solution (one-call):** For each candidate question q, prompt the LLM to output strict JSON: `{expr, final}` where `expr` is a single arithmetic expression computing `final`. (No free-form CoT.)\n\n2) **Model-independent execution gate:** Parse `expr` and safely evaluate it in Python (AST whitelist). Require `safe_eval(expr) == final`.\n\n3) **Deterministic metamorphic perturbation suite (no paraphrase model):** Generate a small set P(q) of *meaning-preserving* variants via pure Python string transforms that do not require any LLM:\n   - rename/permute person names (via a fixed name pool + regex replacement),\n   - reorder sentences/clauses when separated by â€œ. â€ or â€œ, then/andâ€,\n   - normalize numerals formatting (commas, extra spaces),\n   - swap synonymous question stems with a fixed rule list (e.g., â€œHow manyâ€â†’â€œWhat is the number ofâ€).\n   These transformations are intentionally conservative to preserve semantics for GSM8K-style problems.\n\n4) **Cross-prompt (and optionally cross-model) robustness check:** Solve each pâˆˆP(q) using *answer-only* prompting (no CoT) and require all answers equal `final`. To reduce shared-bias failures, use a different prompt style and optionally a cheaper/different model for this check (still prompt-only; no training).\n\n5) **Selection within each cluster:** Prefer demos that satisfy execution + metamorphic invariance with minimal tokens:\n   `score = 1{exec_ok} * (mean_invariance_pass_rate) âˆ’ Î±Â·(demo_tokens/budget)`.\n   Pick the top-scoring candidate per cluster.\n\n6) **Poisoning-resistant demo format:** Provide only `expr` and `#### final` in context (optionally one-line equation summary), explicitly discouraging narrative imitation.\n\nWhat is novel relative to Auto-CoT and the prior PCI-AutoCoT variant:\n- Replaces *LLM-generated paraphrase invariance* with a **deterministic metamorphic test suite** (script-generated), making robustness evaluation less circular and more auditable.\n- Uses **metamorphic invariance pass rate** as the *selection target*, turning â€œrobust under trivial rewritesâ€ into a measurable quality signal for demonstrations.\n- Decouples generation and verification distributions (structured proof vs answer-only under perturbations), reducing same-prompt self-confirmation effects while staying fully prompt-only.",
    "experimental_setup": "Datasets (fast, Python-accessible):\n- Primary: GSM8K (main).\n- Secondary generality check (optional, same pipeline): SVAMP.\n\nProtocol (prompt-only; no fine-tuning):\n1) Demo pool: sample N_pool=200 questions from the training split.\n2) Build k=8 demonstrations using identical clustering (SentenceTransformer + k-means) across methods.\n3) Methods compared:\n   A) Auto-CoT baseline: clustering + length/step heuristic acceptance; free-form CoT demos.\n   B) PCI-AutoCoT (current best prior within this thread): exec-checked `{expr, final}` plus LLM paraphrase invariance.\n   C) **MPC-AutoCoT (proposed):** exec-checked `{expr, final}` plus *deterministic metamorphic* invariance across M=5 perturbations; answer-only checking on perturbations.\n\nAblations (quick, isolates novelty):\n- MPC without cross-model/prompt decoupling (same model, same style) to measure shared-bias contribution.\n- MPC with only 1 perturbation vs 5 perturbations to study robustnessâ€“cost tradeoff.\n- MPC with/without â€œstructured-trace-only demosâ€ to quantify narrative poisoning effects.\n\nEvaluation:\n- Test set: N_test=200 GSM8K test questions (and optionally 200 SVAMP).\n- Fixed decoding: temperature=0, consistent max_tokens.\n\nMetrics:\n- Primary: accuracy (exact-match integer final answer).\n- Secondary: demo build acceptance rate; mean invariance pass rate; demo token length; API call count; accuracy std over 3 seeds.\n\nFeasibility:\n- All gates are checkable with a short Python script (AST evaluation + perturbations + answer comparisons). Extra calls occur only during demo construction, not at test-time.",
    "primary_metric": "accuracy",
    "experimental_code": "import os, re, json, ast, random\nimport numpy as np\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import KMeans\n\n# ---------------- LLM wrapper ----------------\ndef llm_chat(messages, model=\"gpt-4o-mini\", temperature=0.0, max_tokens=250):\n    from openai import OpenAI\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n    resp = client.chat.completions.create(\n        model=model,\n        messages=messages,\n        temperature=temperature,\n        max_tokens=max_tokens,\n    )\n    return resp.choices[0].message.content\n\nFINAL_RE = re.compile(r\"####\\s*(-?\\d+)\")\n\ndef extract_final_int(text):\n    m = FINAL_RE.search(text)\n    if m:\n        return int(m.group(1))\n    ints = re.findall(r\"-?\\d+\", text)\n    return int(ints[-1]) if ints else None\n\n# ---------------- Safe expression eval ----------------\nALLOWED_NODES = (\n    ast.Expression, ast.BinOp, ast.UnaryOp, ast.Constant,\n    ast.Add, ast.Sub, ast.Mult, ast.Div, ast.FloorDiv,\n    ast.USub, ast.UAdd, ast.Mod, ast.Pow, ast.Load\n)\n\ndef safe_eval_expr(expr: str):\n    tree = ast.parse(expr, mode=\"eval\")\n    for node in ast.walk(tree):\n        if not isinstance(node, ALLOWED_NODES):\n            raise ValueError(f\"Disallowed node: {type(node).__name__}\")\n        if isinstance(node, ast.Constant) and not isinstance(node.value, (int, float)):\n            raise ValueError(\"Only numeric constants allowed\")\n    return eval(compile(tree, \"<expr>\", \"eval\"), {\"__builtins__\": {}}, {})\n\n# ---------------- Deterministic metamorphic perturbations ----------------\nNAME_POOL = [\"Alex\", \"Blake\", \"Casey\", \"Drew\", \"Evan\", \"Finley\", \"Gray\", \"Harper\"]\n\ndef rename_people(text, seed=0):\n    # conservative: replace capitalized first names (single token) using a fixed map\n    rnd = random.Random(seed)\n    tokens = re.findall(r\"\\b[A-Z][a-z]{2,}\\b\", text)\n    # take a small set to avoid over-replacement of sentence starts (still imperfect but conservative)\n    uniq = []\n    for t in tokens:\n        if t not in uniq:\n            uniq.append(t)\n    rnd.shuffle(NAME_POOL)\n    mapping = {u: NAME_POOL[i % len(NAME_POOL)] for i, u in enumerate(uniq[:4])}\n    out = text\n    for src, dst in mapping.items():\n        out = re.sub(rf\"\\b{re.escape(src)}\\b\", dst, out)\n    return out\n\ndef reorder_sentences(text):\n    parts = [p.strip() for p in text.split(\".\") if p.strip()]\n    if len(parts) <= 2:\n        return text\n    # keep last sentence (often the question) fixed; shuffle earlier context\n    ctx, last = parts[:-1], parts[-1]\n    ctx = list(reversed(ctx))\n    return \". \".join(ctx + [last]) + \".\"\n\nSTEM_RULES = [\n    (r\"\\bHow many\\b\", \"What is the number of\"),\n    (r\"\\bWhat is\\b\", \"Compute\"),\n]\n\ndef stem_swap(text):\n    out = text\n    for pat, rep in STEM_RULES:\n        out = re.sub(pat, rep, out)\n    return out\n\ndef normalize_space(text):\n    return re.sub(r\"\\s+\", \" \", text).strip()\n\ndef metamorphic_variants(q, m=5, seed=0):\n    # conservative fixed set; deterministic\n    vars = [q]\n    vars.append(normalize_space(q))\n    vars.append(stem_swap(q))\n    vars.append(rename_people(q, seed=seed))\n    vars.append(reorder_sentences(q))\n    return [normalize_space(v) for v in vars[:m]]\n\n# ---------------- MPC-AutoCoT components ----------------\n\ndef gen_expr_final(q, model=\"gpt-4o-mini\"):\n    prompt = (\n        \"Solve the math word problem. Output ONLY valid JSON with keys: expr, final.\\n\"\n        \"- expr: one arithmetic expression using only numbers and + - * / ( )\\n\"\n        \"- final: the final integer answer\\n\"\n        \"No extra text.\\n\\n\"\n        f\"Problem: {q}\"\n    )\n    out = llm_chat([{\"role\": \"user\", \"content\": prompt}], model=model, max_tokens=160)\n    j = out.strip()\n    if not j.startswith(\"{\"):\n        m = re.search(r\"\\{.*\\}\", j, flags=re.S)\n        j = m.group(0) if m else \"{}\"\n    data = json.loads(j)\n    expr = str(data.get(\"expr\", \"\")).strip()\n    final = data.get(\"final\", None)\n    final = int(final) if final is not None and str(final).strip().lstrip('-').isdigit() else None\n    return expr, final\n\n\ndef answer_only(q, model=\"gpt-4o-mini\"):\n    prompt = (\n        \"Solve the problem. Output ONLY the final answer as '#### <number>'.\\n\\n\"\n        f\"Problem: {q}\"\n    )\n    out = llm_chat([{\"role\": \"user\", \"content\": prompt}], model=model, max_tokens=80)\n    return extract_final_int(out)\n\n\ndef invariance_pass_rate(q, final, m=5, seed=0, verify_model=\"gpt-4o-mini\"):\n    vs = metamorphic_variants(q, m=m, seed=seed)\n    ok = 0\n    for v in vs:\n        a = answer_only(v, model=verify_model)\n        ok += int(a is not None and final is not None and a == final)\n    return ok / len(vs)\n\n\ndef build_demos_mpc(questions, k=8, seed=0, n_candidates_per_cluster=10,\n                    alpha=0.15, m_vars=5,\n                    gen_model=\"gpt-4o-mini\", verify_model=\"gpt-4o-mini\"):\n    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n    X = embedder.encode(questions, normalize_embeddings=True)\n    km = KMeans(n_clusters=k, random_state=seed, n_init=\"auto\")\n    labels = km.fit_predict(X)\n\n    demos = []\n    for c in range(k):\n        idxs = [i for i, l in enumerate(labels) if l == c]\n        centroid = km.cluster_centers_[c]\n        idxs.sort(key=lambda i: float(-np.dot(X[i], centroid)))\n        cand_idxs = idxs[:min(len(idxs), n_candidates_per_cluster)]\n\n        best_demo, best_score = None, -1e9\n        for i in cand_idxs:\n            q = questions[i]\n            try:\n                expr, final = gen_expr_final(q, model=gen_model)\n                if not expr or final is None:\n                    continue\n                val = safe_eval_expr(expr)\n                exec_ok = int(round(float(val)) == final)\n                if not exec_ok:\n                    continue\n\n                inv = invariance_pass_rate(q, final, m=m_vars, seed=seed, verify_model=verify_model)\n                demo = f\"Q: {q}\\nA: expr={expr}\\n#### {final}\\n\"\n                tok_pen = len(demo.split()) / 120.0\n                score = inv - alpha * tok_pen\n                if score > best_score:\n                    best_score, best_demo = score, demo\n                if inv == 1.0:\n                    break\n            except Exception:\n                continue\n\n        if best_demo is None:\n            q = questions[cand_idxs[0]]\n            best_demo = f\"Q: {q}\\nA: (no demo found)\\n\"\n        demos.append(best_demo)\n\n    return \"\\n\".join(demos) + \"\\n\"\n\n\ndef answer_with_demos(demos_text, q, model=\"gpt-4o-mini\"):\n    prompt = demos_text + f\"Q: {q}\\nA:\"\n    out = llm_chat([{\"role\": \"user\", \"content\": prompt}], model=model, max_tokens=260)\n    return extract_final_int(out)\n\n\ndef eval_mpc(k=8, n_pool=200, n_test=200, seed=0, model=\"gpt-4o-mini\"):\n    ds = load_dataset(\"gsm8k\", \"main\")\n    train_q = [x[\"question\"] for x in ds[\"train\"].select(range(n_pool))]\n    test = ds[\"test\"].select(range(n_test))\n\n    demos = build_demos_mpc(train_q, k=k, seed=seed, gen_model=model, verify_model=model)\n\n    correct = 0\n    for ex in test:\n        pred = answer_with_demos(demos, ex[\"question\"], model=model)\n        gold = extract_final_int(ex[\"answer\"])\n        correct += int(pred is not None and gold is not None and pred == gold)\n    return correct / len(test)\n\n# Example:\n# print(\"MPC-AutoCoT acc:\", eval_mpc(seed=0))\n",
    "expected_result": "On GSM8K (k=8 demos; pool=200; test=200; temperature=0), MPC-AutoCoT should outperform Auto-CoT by specifically rejecting brittle demonstrations that fail under deterministic rewrites.\n\nPredicted accuracy (higher is better):\n- Auto-CoT baseline: ~0.55\n- PCI-AutoCoT (exec + LLM paraphrase invariance): ~0.64â€“0.66\n- **MPC-AutoCoT (exec + deterministic metamorphic invariance): ~0.66â€“0.69**\n\nSecondary predicted effects:\n- Lower accuracy variance across 3 seeds (e.g., std ~0.01 vs ~0.02 for Auto-CoT) because invariance gating stabilizes demo quality.\n- Clear diagnostic breakdown: among rejected candidates, a sizable fraction pass exec_ok but fail metamorphic invariance (identifying â€œspurious-parseâ€ demos).\n- Similar or lower demo-context length than Auto-CoT due to expr-only formatting; build-time cost increases due to M answer-only checks per accepted demo (M=5), but can be reduced by using a cheaper verifier model without changing the method.\n\nOptional SVAMP: expect smaller but consistent gains (+1â€“3 points) due to distribution shift where brittle demos hurt more.",
    "expected_conclusion": "MPC-AutoCoT advances prompt-only Chain-of-Thought by turning *demonstration robustness* into an explicit, auditable selection criterion rather than an implicit side-effect of diversity and length heuristics. The key academic novelty is importing **metamorphic testing** into in-context demonstration selection: demos are accepted only if they carry an executable proof (expression) and remain correct across a deterministic suite of meaning-preserving rewrites, decoupling verification from the same-model paraphrasing loop.\n\nThis increases significance in two ways:\n1) **Scientific clarity:** robustness is operationalized as a measurable signal (invariance pass rate) with straightforward ablations, enabling stronger causal claims about when and why demos poison ICL.\n2) **Social value:** in educational and decision-support deployments, outputs are more reliable under everyday rephrasings and formatting differences, and demonstrations are less likely to propagate persuasive but invalid narratives because the context contains checkable artifacts rather than free-form CoT.\n\nAll claims remain feasible to validate with a short Python script and prompt-only API calls, aligning with the research objective (no fine-tuning; no time-intensive experiments)."
  },
  "experimental_design": {
    "experiment_summary": "Task: solve grade-school arithmetic word problems by in-context learning (ICL). The experiment compares how different *demonstration selection* rules affect the modelâ€™s final-answer accuracy on unseen GSM8K test questions.\n\nPurpose: demonstrate that MPC-AutoCoT selects demonstrations that are robust to surface-level distribution shift (entity renames, clause/sentence reorderings, spacing/numeral normalization, and fixed stem swaps), thereby improving test-time ICL performance.\n\nWorkflow (all inference-only; no fine-tuning):\n1) Demo pool sampling: draw N_pool=200 questions from GSM8K train split.\n2) Diversity clustering (shared across methods): embed questions using SentenceTransformer (all-MiniLM-L6-v2) and run k-means with k=8 clusters; within each cluster, rank candidates by cosine similarity to the cluster centroid.\n3) Candidate demo generation (per cluster): for the top n_candidates_per_cluster=10 questions, query the LLM to emit strict JSON {\"expr\": <single arithmetic expression>, \"final\": <integer>}.\n4) Proof-carrying execution gate (Python-auditable): safely parse/evaluate expr via a strict AST whitelist; require safe_eval(expr) == final.\n5) Deterministic metamorphic robustness gate (proposed): generate M=5 conservative meaning-preserving variants via pure Python string transforms (no paraphrase model). For each variant, ask the LLM with an *answer-only* prompt to output â€œ#### <number>â€; require agreement with final and compute invariance pass rate.\n6) Selection: within each cluster, choose the highest-scoring candidate demo: score = invariance_pass_rate âˆ’ Î±*(demo_tokens/budget), with Î±=0.15 and budget=120 words-equivalent.\n7) Poisoning-resistant demo formatting: in-context demos include only â€œexpr=â€ and â€œ#### finalâ€ (no free-form CoT).\n8) Evaluation: build exactly 8 demos and solve N_test=200 GSM8K test questions with temperature=0 and fixed max_new_tokens; compute metrics and repeat across 3 random seeds.\n\nScale fit to Runner (NVIDIA H200 140GB VRAM):\n- Single 8B instruction-tuned model run locally with bf16, batch size 1 for generation; no training.\n- N_pool=200, k=8, n_candidates_per_cluster=10, M=5 keeps total model calls manageable (~8*10*(1+M)=480 calls for demo building + 200 calls for evaluation per seed). With 3 seeds: ~2,040 calls. This is designed to complete on a single H200 within a typical research run budget and avoid long-context, high-token CoT generations (JSON + answer-only).",
    "runner_config": {
      "runner_label": [
        "self-hosted",
        "gpu-runner"
      ],
      "description": "NVIDIA H200, VRAM: 140 GB, RAM: 240 GB"
    },
    "evaluation_metrics": [
      {
        "name": "accuracy",
        "description": "Correctness criteria: a prediction is correct if the extracted integer final answer equals the GSM8K gold integer answer. Extraction: parse the model output for the pattern `####\\s*(-?\\d+)`; if absent, fallback to the last integer substring in the output. Gold: extract the integer from the datasetâ€™s provided answer string using the same rule.\n\nCalculation method: `accuracy = (1/N) * sum_i 1[pred_i == gold_i]` over the N_test examples.\n\nTask appropriateness: GSM8K is a single-integer-answer math benchmark; exact integer match is the standard and directly reflects task success.\n\nRelevant visualizations: (i) bar chart of accuracy by method and seed, (ii) paired seed-wise difference plot (proposed âˆ’ baseline), (iii) histogram of wrong-answer absolute error |predâˆ’gold| to distinguish near-misses vs large failures."
      },
      {
        "name": "metamorphic_invariance_pass_rate",
        "description": "Correctness criteria: for each accepted demo candidate question q with gold/proposed final answer `final`, generate P(q) = {p_j}_{j=1..M} deterministic meaning-preserving variants. For each variant p_j, run answer-only prompting and extract integer a_j. A variant passes if a_j == final.\n\nCalculation method (per candidate): `inv(q) = (1/M) * sum_j 1[a_j == final]`. Report (i) mean inv over all accepted demonstrations, and (ii) distribution (min/median/max) across the k selected demos.\n\nTask appropriateness: directly operationalizes the hypothesis that robust demonstrations should be invariant under trivial surface rewrites.\n\nRelevant visualizations: box/violin plot of inv across demos per method; heatmap with demos (rows) Ã— perturbation types (columns) showing pass/fail rates."
      },
      {
        "name": "demo_acceptance_rate",
        "description": "Correctness criteria: a demo candidate is accepted if it passes the methodâ€™s gating rules (baseline: heuristic filters; proposed: JSON parse success + safe AST eval equality + invariance threshold). For MPC-AutoCoT, a candidate passes if exec_ok==1 and inv(q)â‰¥Ï„, with default Ï„=1.0 (all variants match).\n\nCalculation method: `accept_rate = (#accepted_candidates) / (#candidates_evaluated)` measured during demo construction; also report `clusters_filled = (#clusters_with_valid_demo)/(k)`.\n\nTask appropriateness: captures whether stricter robustness checks overly reduce usable demos (a key cost/benefit tradeoff).\n\nRelevant visualizations: stacked bar chart of candidate outcomes (json_fail / exec_fail / inv_fail / accepted) per method."
      },
      {
        "name": "demo_context_tokens",
        "description": "Correctness criteria: count the number of tokens/words consumed by the final in-context demonstration block used at test time. Since tokenizer differences vary across models, report both (i) whitespace-separated word count and (ii) model-tokenizer token count (using the chosen modelâ€™s tokenizer).\n\nCalculation method: `mean_demo_len = (1/k) * sum_d len(demo_d)` and total context length `len(concat_demos)`.\n\nTask appropriateness: MPC-AutoCoT claims lower narrative poisoning risk and lower context cost by using expr-only demos; length is a necessary efficiency and confounding-control metric.\n\nRelevant visualizations: bar chart of total demo context length per method; scatter plot of demo length vs demo invariance score within method."
      },
      {
        "name": "compute_cost_calls",
        "description": "Correctness criteria: count LLM forward passes (calls) and total generated tokens separately for (i) demo building and (ii) evaluation.\n\nCalculation method: instrument the generation wrapper to log `n_calls`, `prompt_tokens`, `completion_tokens`. Report totals and per-test-example averages.\n\nTask appropriateness: proposed robustness gating increases build-time calls; this metric quantifies the cost/quality tradeoff.\n\nRelevant visualizations: cost vs accuracy scatter plot; stacked bars splitting build vs eval cost."
      },
      {
        "name": "seed_variance_accuracy",
        "description": "Correctness criteria: run the full pipeline (demo sampling + clustering + selection + evaluation) for S=3 seeds.\n\nCalculation method: report `mean_accuracy` and `std_accuracy` across seeds; optionally 95% CI using t-distribution: `CI = mean Â± t_{0.975,S-1} * std/sqrt(S)`.\n\nTask appropriateness: hypothesis predicts MPC-AutoCoT reduces fragility, which should manifest as lower variance across seeds.\n\nRelevant visualizations: error bars (meanÂ±std) per method; seed-wise line plot connecting methods within the same seed."
      }
    ],
    "models_to_use": [
      "Qwen3-8B (8B parameters)"
    ],
    "datasets_to_use": [
      "gsm8k"
    ],
    "proposed_method": {
      "method_name": "Metamorphic Proof-Carrying Auto-CoT (MPC-AutoCoT)",
      "description": "Inference-only demonstration selection for math ICL that replaces fragile free-form CoT demos with proof-carrying JSON {expr, final} plus a Python-verifiable execution gate and a deterministic metamorphic robustness gate. Demonstrations are accepted only if (1) the arithmetic expression safely evaluates to the claimed final answer, and (2) answer-only solutions remain invariant across a conservative, script-generated suite of meaning-preserving surface rewrites (name swaps, sentence reorderings, spacing/numeral normalization, and fixed stem swaps). Selected demos are formatted as expr-only to reduce narrative imitation/poisoning.",
      "optuna_config": {
        "enabled": false,
        "n_trials": 20
      }
    },
    "comparative_methods": [
      {
        "method_name": "Auto-CoT (clustering + heuristic CoT demo filtering)",
        "description": "Baseline prompt-only Auto-CoT: sample a pool, cluster for diversity, and select demonstrations using simple heuristics (e.g., length/step constraints) with free-form chain-of-thought solutions placed in-context. No explicit execution verification and no robustness checks under surface-level rewrites.",
        "optuna_config": {
          "enabled": false,
          "n_trials": 20
        }
      }
    ]
  }
}